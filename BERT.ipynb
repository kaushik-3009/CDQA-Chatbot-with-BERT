{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaushik-3009/NLP-project/blob/main/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W_KnfMBQ2Hb"
      },
      "source": [
        "<h1><center> Passage Based Question Answer Model </center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqVRz-PoQ2He"
      },
      "source": [
        "<h3><center> NLP PROJECT REVIEW 2 </center></h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxJs3lCctaWi",
        "outputId": "3d572dd3-8880-4a08-c5c8-daba2da2bdb2",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.9/dist-packages (1.3.1)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.9/dist-packages (from PyDrive) (6.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.9/dist-packages (from PyDrive) (2.70.0)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client>=1.2->PyDrive) (2.11.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.17.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.1.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client>=1.2->PyDrive) (2.16.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client>=1.2->PyDrive) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.9/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.9)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.9/dist-packages (from oauth2client>=4.0.0->PyDrive) (1.15.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.9/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.9/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->PyDrive) (1.58.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.9/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->PyDrive) (2.25.1)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.9/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->PyDrive) (3.19.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.2->PyDrive) (5.3.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->PyDrive) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->PyDrive) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->PyDrive) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->PyDrive) (4.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "moqAv_5-bky9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df4a9ec0-3625-47d1-cbe6-71d519811461"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pandas==1.0.3\n",
            "  Downloading pandas-1.0.3.tar.gz (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ],
      "source": [
        "!pip install --user pandas==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZCu-VFh9xJ8n"
      },
      "outputs": [],
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1vXUB_l_xOAb"
      },
      "outputs": [],
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXhH7VMG2L2a"
      },
      "source": [
        "**Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qVic_jGHW3KB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d89041b-3e1e-45b0-a488-a44dbdd3e80b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "xgSo872S2Q54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fa51c80-4f2a-4859-f8ef-382da866af72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import json\n",
        "from pandas.io.json import json_normalize\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xLLFDob2pU4"
      },
      "source": [
        "**Load the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KiEy7q8_yaJx"
      },
      "outputs": [],
      "source": [
        "train = pd.read_json('/content/drive/MyDrive/Colab Notebooks/Squad/train-v2.0.json')\n",
        "dev = pd.read_json('/content/drive/MyDrive/Colab Notebooks/Squad/dev-v2.0.json')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Af_vg3LniyQJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "a009568a-b68a-445d-f4e3-26c37c32f3f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  version                                               data\n",
              "0    v2.0  {'title': 'Beyoncé', 'paragraphs': [{'qas': [{...\n",
              "1    v2.0  {'title': 'Frédéric_Chopin', 'paragraphs': [{'...\n",
              "2    v2.0  {'title': 'Sino-Tibetan_relations_during_the_M...\n",
              "3    v2.0  {'title': 'IPod', 'paragraphs': [{'qas': [{'qu...\n",
              "4    v2.0  {'title': 'The_Legend_of_Zelda:_Twilight_Princ..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2e83c269-6ffd-4d0e-82cd-07588e5ab0ea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>version</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Beyoncé', 'paragraphs': [{'qas': [{...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Frédéric_Chopin', 'paragraphs': [{'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Sino-Tibetan_relations_during_the_M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'IPod', 'paragraphs': [{'qas': [{'qu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'The_Legend_of_Zelda:_Twilight_Princ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e83c269-6ffd-4d0e-82cd-07588e5ab0ea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2e83c269-6ffd-4d0e-82cd-07588e5ab0ea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2e83c269-6ffd-4d0e-82cd-07588e5ab0ea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LeF3FsZEt5r9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c4a0043-2cad-4820-9955-63c19ce9ed3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of data = (442, 2)\n"
          ]
        }
      ],
      "source": [
        "print(\"Shape of data =\",train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9ejKyiU3ZFAY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "00945b5f-1f52-436b-eca1-06ce1b17a8b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  version                                               data\n",
              "0    v2.0  {'title': 'Normans', 'paragraphs': [{'qas': [{...\n",
              "1    v2.0  {'title': 'Computational_complexity_theory', '...\n",
              "2    v2.0  {'title': 'Southern_California', 'paragraphs':...\n",
              "3    v2.0  {'title': 'Sky_(United_Kingdom)', 'paragraphs'...\n",
              "4    v2.0  {'title': 'Victoria_(Australia)', 'paragraphs'..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-92ac3a89-82cb-475e-8fde-8537006c669d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>version</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Normans', 'paragraphs': [{'qas': [{...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Computational_complexity_theory', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Southern_California', 'paragraphs':...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Sky_(United_Kingdom)', 'paragraphs'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Victoria_(Australia)', 'paragraphs'...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-92ac3a89-82cb-475e-8fde-8537006c669d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-92ac3a89-82cb-475e-8fde-8537006c669d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-92ac3a89-82cb-475e-8fde-8537006c669d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "dev.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cM0UhNHRZFp8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab251f4c-f624-498e-c9c9-d88d3ec0bbda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of dev data = (35, 2)\n"
          ]
        }
      ],
      "source": [
        "print(\"Shape of dev data =\", dev.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieUiy50SZYlD"
      },
      "source": [
        "Put dev file in a dataframe to be readable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KxlcVcxYZddv"
      },
      "outputs": [],
      "source": [
        "#This has multiple answers for same questions unlike traing data\n",
        "def dtodf(dfile, record_path = ['data','paragraphs','qas','answers'],\n",
        "                           verbose = 1):\n",
        "    #dfile: path to the squad json file.\n",
        "    #record_path: path to deepest level in json file\n",
        "    #verbose controls level of detail of progress messages\n",
        "  \n",
        "    if verbose:\n",
        "        print(\"Reading the json file\")    \n",
        "    file = json.loads(open(dfile).read())\n",
        "    if verbose:\n",
        "        print(\"processing...\")\n",
        "    # parsing different level's in the json file\n",
        "    js = pd.io.json.json_normalize(file , record_path )\n",
        "    m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
        "    r = pd.io.json.json_normalize(file,record_path[:-2])\n",
        "    \n",
        "    #combining it into single dataframe\n",
        "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
        "    m['context'] = idx\n",
        "    main = m[['id','question','context','answers']].set_index('id').reset_index()\n",
        "    main['c_id'] = main['context'].factorize()[0]\n",
        "    if verbose:\n",
        "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
        "        print(\"Done\")\n",
        "    return main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "o8Dz1GI2x0Bw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6d9be24-680a-423c-865a-5921b48992cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading the json file\n",
            "processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-c44eec628d0e>:14: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
            "  js = pd.io.json.json_normalize(file , record_path )\n",
            "<ipython-input-12-c44eec628d0e>:15: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
            "  m = pd.io.json.json_normalize(file, record_path[:-1] )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of the dataframe is (11873, 5)\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-c44eec628d0e>:16: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
            "  r = pd.io.json.json_normalize(file,record_path[:-2])\n"
          ]
        }
      ],
      "source": [
        "dfile = '/content/drive/MyDrive/Colab Notebooks/Squad/dev-v2.0.json'\n",
        "record_path = ['data','paragraphs','qas','answers']\n",
        "dev1 = dtodf(dfile=dfile,record_path=record_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6Z1mjT20yMXr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "a5de29e1-a9d4-42f6-bd8b-f3106cdbcc12"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                         id  \\\n",
              "0  56ddde6b9a695914005b9628   \n",
              "1  56ddde6b9a695914005b9629   \n",
              "2  56ddde6b9a695914005b962a   \n",
              "3  56ddde6b9a695914005b962b   \n",
              "4  56ddde6b9a695914005b962c   \n",
              "\n",
              "                                            question  \\\n",
              "0               In what country is Normandy located?   \n",
              "1                 When were the Normans in Normandy?   \n",
              "2      From which countries did the Norse originate?   \n",
              "3                          Who was the Norse leader?   \n",
              "4  What century did the Normans first gain their ...   \n",
              "\n",
              "                                             context  \\\n",
              "0  The Normans (Norman: Nourmands; French: Norman...   \n",
              "1  The Normans (Norman: Nourmands; French: Norman...   \n",
              "2  The Normans (Norman: Nourmands; French: Norman...   \n",
              "3  The Normans (Norman: Nourmands; French: Norman...   \n",
              "4  The Normans (Norman: Nourmands; French: Norman...   \n",
              "\n",
              "                                             answers  c_id  \n",
              "0  [{'text': 'France', 'answer_start': 159}, {'te...     0  \n",
              "1  [{'text': '10th and 11th centuries', 'answer_s...     0  \n",
              "2  [{'text': 'Denmark, Iceland and Norway', 'answ...     0  \n",
              "3  [{'text': 'Rollo', 'answer_start': 308}, {'tex...     0  \n",
              "4  [{'text': '10th century', 'answer_start': 671}...     0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fdfac56b-f484-4e62-b82c-d27e7d112f86\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>answers</th>\n",
              "      <th>c_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56ddde6b9a695914005b9628</td>\n",
              "      <td>In what country is Normandy located?</td>\n",
              "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
              "      <td>[{'text': 'France', 'answer_start': 159}, {'te...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56ddde6b9a695914005b9629</td>\n",
              "      <td>When were the Normans in Normandy?</td>\n",
              "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
              "      <td>[{'text': '10th and 11th centuries', 'answer_s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56ddde6b9a695914005b962a</td>\n",
              "      <td>From which countries did the Norse originate?</td>\n",
              "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
              "      <td>[{'text': 'Denmark, Iceland and Norway', 'answ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56ddde6b9a695914005b962b</td>\n",
              "      <td>Who was the Norse leader?</td>\n",
              "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
              "      <td>[{'text': 'Rollo', 'answer_start': 308}, {'tex...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56ddde6b9a695914005b962c</td>\n",
              "      <td>What century did the Normans first gain their ...</td>\n",
              "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
              "      <td>[{'text': '10th century', 'answer_start': 671}...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fdfac56b-f484-4e62-b82c-d27e7d112f86')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fdfac56b-f484-4e62-b82c-d27e7d112f86 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fdfac56b-f484-4e62-b82c-d27e7d112f86');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "dev1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFHqUVue3CS2"
      },
      "source": [
        "**Put the training json file into a dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "GtEM0XtK-hGw"
      },
      "outputs": [],
      "source": [
        "def ttodf(dfile, record_path = ['data','paragraphs','qas','answers'], verbose = 1):\n",
        "    #dfile: path to the squad json file.\n",
        "    #record_path: path to last level in the json file\n",
        "    if verbose:\n",
        "        print(\"Reading the json file\")    \n",
        "    file = json.loads(open(dfile).read())\n",
        "    if verbose:\n",
        "        print(\"processing...\")\n",
        "    # parsing different levels in the json file\n",
        "    js = pd.json_normalize(file , record_path )\n",
        "    m = pd.json_normalize(file, record_path[:-1] )\n",
        "    r = pd.json_normalize(file,record_path[:-2])\n",
        "    \n",
        "    #combining it into single dataframe\n",
        "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
        "    ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
        "    m['context'] = idx\n",
        "    js['q_idx'] = ndx\n",
        "    main = pd.concat([ m[['id','question','context']].set_index('id'),js.set_index('q_idx')],1,sort=False).reset_index()\n",
        "    main['c_id'] = main['context'].factorize()[0]\n",
        "    if verbose:\n",
        "        print(\"\\nJson has been successfully converted into a dataframe\")\n",
        "    return main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "KOEawOiNLu4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40c47eed-37b6-4c50-bf25-d28c2731cbfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading the json file\n",
            "processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-109-809cd7b0e445>:19: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
            "  main = pd.concat([ m[['id','question','context']].set_index('id'),js.set_index('q_idx')],1,sort=False).reset_index()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Json has been successfully converted into a dataframe\n"
          ]
        }
      ],
      "source": [
        "dfile = '/content/drive/MyDrive/Colab Notebooks/Squad/train-v2.0.json'\n",
        "record_path = ['data','paragraphs','qas','answers']\n",
        "train1 = ttodf(dfile=dfile,record_path=record_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TDrY8Vq4XMR"
      },
      "source": [
        "**Checking Structure of the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HAWgC5nGMpqM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "c2c59a8a-2ebb-483e-be2d-35b965bbcebd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      index  \\\n",
              "0  56be85543aeaaa14008c9063   \n",
              "1  56be85543aeaaa14008c9065   \n",
              "2  56be85543aeaaa14008c9066   \n",
              "3  56bf6b0f3aeaaa14008c9601   \n",
              "4  56bf6b0f3aeaaa14008c9602   \n",
              "\n",
              "                                            question  \\\n",
              "0           When did Beyonce start becoming popular?   \n",
              "1  What areas did Beyonce compete in when she was...   \n",
              "2  When did Beyonce leave Destiny's Child and bec...   \n",
              "3      In what city and state did Beyonce  grow up?    \n",
              "4         In which decade did Beyonce become famous?   \n",
              "\n",
              "                                             context                 text  \\\n",
              "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...    in the late 1990s   \n",
              "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  singing and dancing   \n",
              "2  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...                 2003   \n",
              "3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...       Houston, Texas   \n",
              "4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...           late 1990s   \n",
              "\n",
              "   answer_start  c_id  \n",
              "0         269.0     0  \n",
              "1         207.0     0  \n",
              "2         526.0     0  \n",
              "3         166.0     0  \n",
              "4         276.0     0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0576d011-68db-458d-9558-9a3c12ea521d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>text</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>c_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56be85543aeaaa14008c9063</td>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>in the late 1990s</td>\n",
              "      <td>269.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56be85543aeaaa14008c9065</td>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>singing and dancing</td>\n",
              "      <td>207.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56be85543aeaaa14008c9066</td>\n",
              "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>2003</td>\n",
              "      <td>526.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
              "      <td>In what city and state did Beyonce  grow up?</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>Houston, Texas</td>\n",
              "      <td>166.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
              "      <td>In which decade did Beyonce become famous?</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>late 1990s</td>\n",
              "      <td>276.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0576d011-68db-458d-9558-9a3c12ea521d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0576d011-68db-458d-9558-9a3c12ea521d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0576d011-68db-458d-9558-9a3c12ea521d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "train1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8FK8zamUrBe"
      },
      "source": [
        "\n",
        "**Text Data Pre-processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dWx76Gz4oT7"
      },
      "source": [
        "Data dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "r6GbGR0HT1yn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cefb5f0-bbd5-4d1e-f0af-35426034e463"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of data = (130319, 6)\n"
          ]
        }
      ],
      "source": [
        "print(\"Shape of data =\", train1.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHT3hkap4ua9"
      },
      "source": [
        "Checking for Null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2UYGfG3XUxEH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ece4393-44db-482d-beab-bbb357cfdc6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "index               0\n",
              "question            0\n",
              "context             0\n",
              "text            43498\n",
              "answer_start    43498\n",
              "c_id                0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "train1.isnull().sum()\n",
        "#no null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "tsYQ-IU2Vf9Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11628001-4078-4748-ad1e-636da952a1bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['When did Beyonce start becoming popular?',\n",
              "       'What areas did Beyonce compete in when she was growing up?',\n",
              "       \"When did Beyonce leave Destiny's Child and become a solo singer?\",\n",
              "       ..., 'What is another name for anti-matter?',\n",
              "       'Matter usually does not need to be used in conjunction with what?',\n",
              "       'What field of study has a variety of unusual contexts?'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "#take a look at the text present in the columns\n",
        "train1['question'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "GbBSLHBNbM8S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0f69021-0605-4f46-81ca-0ab0e7c74361"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
              "       'Following the disbandment of Destiny\\'s Child in June 2005, she released her second solo album, B\\'Day (2006), which contained hits \"Déjà Vu\", \"Irreplaceable\", and \"Beautiful Liar\". Beyoncé also ventured into acting, with a Golden Globe-nominated performance in Dreamgirls (2006), and starring roles in The Pink Panther (2006) and Obsessed (2009). Her marriage to rapper Jay Z and portrayal of Etta James in Cadillac Records (2008) influenced her third album, I Am... Sasha Fierce (2008), which saw the birth of her alter-ego Sasha Fierce and earned a record-setting six Grammy Awards in 2010, including Song of the Year for \"Single Ladies (Put a Ring on It)\". Beyoncé took a hiatus from music in 2010 and took over management of her career; her fourth album 4 (2011) was subsequently mellower in tone, exploring 1970s funk, 1980s pop, and 1990s soul. Her critically acclaimed fifth studio album, Beyoncé (2013), was distinguished from previous releases by its experimental production and exploration of darker themes.',\n",
              "       'A self-described \"modern-day feminist\", Beyoncé creates songs that are often characterized by themes of love, relationships, and monogamy, as well as female sexuality and empowerment. On stage, her dynamic, highly choreographed performances have led to critics hailing her as one of the best entertainers in contemporary popular music. Throughout a career spanning 19 years, she has sold over 118 million records as a solo artist, and a further 60 million with Destiny\\'s Child, making her one of the best-selling music artists of all time. She has won 20 Grammy Awards and is the most nominated woman in the award\\'s history. The Recording Industry Association of America recognized her as the Top Certified Artist in America during the 2000s decade. In 2009, Billboard named her the Top Radio Songs Artist of the Decade, the Top Female Artist of the 2000s and their Artist of the Millennium in 2011. Time listed her among the 100 most influential people in the world in 2013 and 2014. Forbes magazine also listed her as the most powerful female musician of 2015.',\n",
              "       ...,\n",
              "       'In the late 19th century with the discovery of the electron, and in the early 20th century, with the discovery of the atomic nucleus, and the birth of particle physics, matter was seen as made up of electrons, protons and neutrons interacting to form atoms. Today, we know that even protons and neutrons are not indivisible, they can be divided into quarks, while electrons are part of a particle family called leptons. Both quarks and leptons are elementary particles, and are currently seen as being the fundamental constituents of matter.',\n",
              "       'These quarks and leptons interact through four fundamental forces: gravity, electromagnetism, weak interactions, and strong interactions. The Standard Model of particle physics is currently the best explanation for all of physics, but despite decades of efforts, gravity cannot yet be accounted for at the quantum level; it is only described by classical physics (see quantum gravity and graviton). Interactions between quarks and leptons are the result of an exchange of force-carrying particles (such as photons) between quarks and leptons. The force-carrying particles are not themselves building blocks. As one consequence, mass and energy (which cannot be created or destroyed) cannot always be related to matter (which can be created out of non-matter particles such as photons, or even out of pure energy, such as kinetic energy). Force carriers are usually not considered matter: the carriers of the electric force (photons) possess energy (see Planck relation) and the carriers of the weak force (W and Z bosons) are massive, but neither are considered matter either. However, while these particles are not considered matter, they do contribute to the total mass of atoms, subatomic particles, and all systems that contain them.',\n",
              "       'The term \"matter\" is used throughout physics in a bewildering variety of contexts: for example, one refers to \"condensed matter physics\", \"elementary matter\", \"partonic\" matter, \"dark\" matter, \"anti\"-matter, \"strange\" matter, and \"nuclear\" matter. In discussions of matter and antimatter, normal matter has been referred to by Alfvén as koinomatter (Gk. common matter). It is fair to say that in physics, there is no broad consensus as to a general definition of matter, and the term \"matter\" usually is used in conjunction with a specifying modifier.'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "train1['context'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Va3Otgakv0on",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3243db5a-1c6f-4011-f39c-7dd00c4e3448"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['in the late 1990s', 'singing and dancing', '2003', ..., 'Oregon',\n",
              "       'Minsk', 'Kathmandu Metropolitan City'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "train1['text'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8lEzBYq0EmK"
      },
      "source": [
        "**Cleaning Text Data for word embeddings**\n",
        "\n",
        "When cleaning the text, we will perform the following task\n",
        "\n",
        "1.   Convert text to lowercase\n",
        "2.   Remove punctuations\n",
        "3.   Remove extra space\n",
        "4.   Stop words removal\n",
        "5.   Stemming\n",
        "6.   Lemmatization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHf4kNYrvZqa"
      },
      "source": [
        "1. Convert text to lowercase (because in NLP casing matters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "7aQmu9anbpAF"
      },
      "outputs": [],
      "source": [
        "train1['clean_question']=train1['question'].apply(lambda x: x.lower() if isinstance(x, str) else str(x))\n",
        "train1['clean_context']=train1['context'].apply(lambda x: x.lower() if isinstance(x, str) else str(x))\n",
        "train1['clean_text']=train1['text'].apply(lambda x: x.lower() if isinstance(x, str) else str(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVwOl5KzvOCv"
      },
      "source": [
        "2. Removing punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "CJ5K3RpcbpmX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "640cbc36-8208-408e-c239-96c25ad66cfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-118-3e5e0fdaaa50>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  train1['clean_question']=train1['clean_question'].str.replace('[^\\w\\s]','')\n",
            "<ipython-input-118-3e5e0fdaaa50>:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  train1['clean_context']=train1['clean_context'].str.replace('[^\\w\\s]','')\n",
            "<ipython-input-118-3e5e0fdaaa50>:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  train1['clean_text']=train1['clean_text'].str.replace('[^\\w\\s]','')\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "train1['clean_question']=train1['clean_question'].str.replace('[^\\w\\s]','')\n",
        "train1['clean_context']=train1['clean_context'].str.replace('[^\\w\\s]','')\n",
        "train1['clean_text']=train1['clean_text'].str.replace('[^\\w\\s]','')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtlKNpYWvrWs"
      },
      "source": [
        "3. Removing extra space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "F4xJgv3BexEu"
      },
      "outputs": [],
      "source": [
        "train1['clean_question']=train1['clean_question'].apply(lambda x: re.sub(' +',' ',x) )\n",
        "train1['clean_context']=train1['clean_context'].apply(lambda x: re.sub(' +',' ',x))\n",
        "train1['clean_text']=train1['clean_text'].apply(lambda x: re.sub(' +',' ',x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3cHv1l_gA9k"
      },
      "source": [
        "Checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "oCFHZtabfxbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49642e69-db95-4b7a-e971-245dfa6723a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['when did beyonce start becoming popular',\n",
              "       'what areas did beyonce compete in when she was growing up',\n",
              "       'when did beyonce leave destinys child and become a solo singer',\n",
              "       ..., 'what is another name for antimatter',\n",
              "       'matter usually does not need to be used in conjunction with what',\n",
              "       'what field of study has a variety of unusual contexts'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ],
      "source": [
        "train1['clean_question'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "BxwakbwBf5tN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3a9442c-6463-4576-c095-f3c813b85d58"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['beyoncé giselle knowlescarter biːˈjɒnseɪ beeyonsay born september 4 1981 is an american singer songwriter record producer and actress born and raised in houston texas she performed in various singing and dancing competitions as a child and rose to fame in the late 1990s as lead singer of rb girlgroup destinys child managed by her father mathew knowles the group became one of the worlds bestselling girl groups of all time their hiatus saw the release of beyoncés debut album dangerously in love 2003 which established her as a solo artist worldwide earned five grammy awards and featured the billboard hot 100 numberone singles crazy in love and baby boy',\n",
              "       'following the disbandment of destinys child in june 2005 she released her second solo album bday 2006 which contained hits déjà vu irreplaceable and beautiful liar beyoncé also ventured into acting with a golden globenominated performance in dreamgirls 2006 and starring roles in the pink panther 2006 and obsessed 2009 her marriage to rapper jay z and portrayal of etta james in cadillac records 2008 influenced her third album i am sasha fierce 2008 which saw the birth of her alterego sasha fierce and earned a recordsetting six grammy awards in 2010 including song of the year for single ladies put a ring on it beyoncé took a hiatus from music in 2010 and took over management of her career her fourth album 4 2011 was subsequently mellower in tone exploring 1970s funk 1980s pop and 1990s soul her critically acclaimed fifth studio album beyoncé 2013 was distinguished from previous releases by its experimental production and exploration of darker themes',\n",
              "       'a selfdescribed modernday feminist beyoncé creates songs that are often characterized by themes of love relationships and monogamy as well as female sexuality and empowerment on stage her dynamic highly choreographed performances have led to critics hailing her as one of the best entertainers in contemporary popular music throughout a career spanning 19 years she has sold over 118 million records as a solo artist and a further 60 million with destinys child making her one of the bestselling music artists of all time she has won 20 grammy awards and is the most nominated woman in the awards history the recording industry association of america recognized her as the top certified artist in america during the 2000s decade in 2009 billboard named her the top radio songs artist of the decade the top female artist of the 2000s and their artist of the millennium in 2011 time listed her among the 100 most influential people in the world in 2013 and 2014 forbes magazine also listed her as the most powerful female musician of 2015',\n",
              "       ...,\n",
              "       'in the late 19th century with the discovery of the electron and in the early 20th century with the discovery of the atomic nucleus and the birth of particle physics matter was seen as made up of electrons protons and neutrons interacting to form atoms today we know that even protons and neutrons are not indivisible they can be divided into quarks while electrons are part of a particle family called leptons both quarks and leptons are elementary particles and are currently seen as being the fundamental constituents of matter',\n",
              "       'these quarks and leptons interact through four fundamental forces gravity electromagnetism weak interactions and strong interactions the standard model of particle physics is currently the best explanation for all of physics but despite decades of efforts gravity cannot yet be accounted for at the quantum level it is only described by classical physics see quantum gravity and graviton interactions between quarks and leptons are the result of an exchange of forcecarrying particles such as photons between quarks and leptons the forcecarrying particles are not themselves building blocks as one consequence mass and energy which cannot be created or destroyed cannot always be related to matter which can be created out of nonmatter particles such as photons or even out of pure energy such as kinetic energy force carriers are usually not considered matter the carriers of the electric force photons possess energy see planck relation and the carriers of the weak force w and z bosons are massive but neither are considered matter either however while these particles are not considered matter they do contribute to the total mass of atoms subatomic particles and all systems that contain them',\n",
              "       'the term matter is used throughout physics in a bewildering variety of contexts for example one refers to condensed matter physics elementary matter partonic matter dark matter antimatter strange matter and nuclear matter in discussions of matter and antimatter normal matter has been referred to by alfvén as koinomatter gk common matter it is fair to say that in physics there is no broad consensus as to a general definition of matter and the term matter usually is used in conjunction with a specifying modifier'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ],
      "source": [
        "train1['clean_context'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "Ud2hFLp0f-FD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d16cb717-22fd-4f7c-eff3-ecd891471db3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['in the late 1990s', 'singing and dancing', '2003', ..., 'oregon',\n",
              "       'minsk', 'kathmandu metropolitan city'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ],
      "source": [
        "train1['clean_text'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Removing stop words"
      ],
      "metadata": {
        "id": "HXmaZb3oBlKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords \n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "def remove_stopwords(data):\n",
        "  output_array=[]\n",
        "  for sentence in train1['clean_text']:\n",
        "    temp_list=[]\n",
        "    for word in sentence.split():\n",
        "        if word.lower() not in stopwords:\n",
        "            temp_list.append(word)\n",
        "        output_array.append(' '.join(temp_list))\n",
        "  return output_array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0S1-m5E-4PL",
        "outputId": "632f517e-2bba-4955-86a8-77d1362fa3ab"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Stemming"
      ],
      "metadata": {
        "id": "rYPMWNP6BuKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowball = SnowballStemmer(language='english')\n",
        "\n",
        "def stemmer(df):\n",
        "  output_array=[]\n",
        "  for sentence in df:\n",
        "    temp_list=[]\n",
        "    for word in sentence.split():\n",
        "          temp_list.append(snowball.stem(word))\n",
        "    output_array.append(' '.join(temp_list))\n",
        "  output_array1 = pd.Series(output_array)\n",
        "  return output_array1\n",
        "data = train1['clean_text']\n",
        "print(data.unique())\n",
        "data = stemmer(data)\n",
        "print(data.unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdC6pj4EB6RJ",
        "outputId": "5a1a1895-0086-45a7-9588-c24efa2a0e4c"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['in the late 1990s' 'singing and dancing' '2003' ... 'oregon' 'minsk'\n",
            " 'kathmandu metropolitan city']\n",
            "['in the late 1990s' 'sing and danc' '2003' ... 'oregon' 'minsk'\n",
            " 'kathmandu metropolitan citi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Lemmatization"
      ],
      "metadata": {
        "id": "_BZXzj4-BuXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "\n",
        "def lemmatize_words(df):\n",
        "  output_array=[]\n",
        "  for sentence in df:\n",
        "    temp_list=[]\n",
        "    for word in sentence.split():\n",
        "          temp_list.append(lemmatizer.lemmatize(word))\n",
        "    output_array.append(' '.join(temp_list))\n",
        "  output_array1 = pd.Series(output_array)\n",
        "  return output_array1\n",
        "\n",
        "data = train1['clean_text']\n",
        "\n",
        "print(data.unique())\n",
        "data = lemmatize_words(data)\n",
        "print(data.unique())\n",
        "print(\"\\nExample of lemmatization\")\n",
        "sentence = \"The striped bats are hanging on their feet for best\"\n",
        "\n",
        "# Tokenize: Split the sentence into words\n",
        "word_list = nltk.word_tokenize(sentence)\n",
        "print(word_list)\n",
        "#> ['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n",
        "\n",
        "# Lemmatize list of words and join\n",
        "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
        "print(lemmatized_output)\n",
        "#> The striped bat are hanging on their foot for best\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOzlNdnrB689",
        "outputId": "32ae5890-95d2-4b26-846e-87359da7779d"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['in the late 1990s' 'singing and dancing' '2003' ... 'oregon' 'minsk'\n",
            " 'kathmandu metropolitan city']\n",
            "['in the late 1990s' 'singing and dancing' '2003' ... 'oregon' 'minsk'\n",
            " 'kathmandu metropolitan city']\n",
            "\n",
            "Example of lemmatization\n",
            "['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n",
            "The striped bat are hanging on their foot for best\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D6us36025Mc"
      },
      "source": [
        "Statistically checking the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "N9YM53owLTg9"
      },
      "outputs": [],
      "source": [
        "#dropping columns\n",
        "train1 = train1.drop(train1.columns[[0, 1, 2, 4, 5]], axis=1) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "EyApZxkHOkr1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "e75f0d78-dc0b-465a-a6e5-f114f8f970e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  text                                     clean_question  \\\n",
              "0    in the late 1990s            when did beyonce start becoming popular   \n",
              "1  singing and dancing  what areas did beyonce compete in when she was...   \n",
              "2                 2003  when did beyonce leave destinys child and beco...   \n",
              "3       Houston, Texas        in what city and state did beyonce grow up    \n",
              "4           late 1990s          in which decade did beyonce become famous   \n",
              "\n",
              "                                       clean_context           clean_text  \n",
              "0  beyoncé giselle knowlescarter biːˈjɒnseɪ beeyo...    in the late 1990s  \n",
              "1  beyoncé giselle knowlescarter biːˈjɒnseɪ beeyo...  singing and dancing  \n",
              "2  beyoncé giselle knowlescarter biːˈjɒnseɪ beeyo...                 2003  \n",
              "3  beyoncé giselle knowlescarter biːˈjɒnseɪ beeyo...        houston texas  \n",
              "4  beyoncé giselle knowlescarter biːˈjɒnseɪ beeyo...           late 1990s  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-49579e41-0707-46b2-b85e-dbc46ce5cdc9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>clean_question</th>\n",
              "      <th>clean_context</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>in the late 1990s</td>\n",
              "      <td>when did beyonce start becoming popular</td>\n",
              "      <td>beyoncé giselle knowlescarter biːˈjɒnseɪ beeyo...</td>\n",
              "      <td>in the late 1990s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>singing and dancing</td>\n",
              "      <td>what areas did beyonce compete in when she was...</td>\n",
              "      <td>beyoncé giselle knowlescarter biːˈjɒnseɪ beeyo...</td>\n",
              "      <td>singing and dancing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2003</td>\n",
              "      <td>when did beyonce leave destinys child and beco...</td>\n",
              "      <td>beyoncé giselle knowlescarter biːˈjɒnseɪ beeyo...</td>\n",
              "      <td>2003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Houston, Texas</td>\n",
              "      <td>in what city and state did beyonce grow up</td>\n",
              "      <td>beyoncé giselle knowlescarter biːˈjɒnseɪ beeyo...</td>\n",
              "      <td>houston texas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>late 1990s</td>\n",
              "      <td>in which decade did beyonce become famous</td>\n",
              "      <td>beyoncé giselle knowlescarter biːˈjɒnseɪ beeyo...</td>\n",
              "      <td>late 1990s</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-49579e41-0707-46b2-b85e-dbc46ce5cdc9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-49579e41-0707-46b2-b85e-dbc46ce5cdc9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-49579e41-0707-46b2-b85e-dbc46ce5cdc9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "train1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53OYYlIWsFJN"
      },
      "source": [
        "Transfer Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "wtHiCTZSEUwP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e0bb6bf817394d7a8a5bb789ed8238d4",
            "b3be670194ed49949a11a9eaf1a49c9e",
            "5ad0208f33904ed18d087e6a3752b5d6",
            "21abe1facd614c27a18110fe18e772b7",
            "9087d7f5df354912bd6abbdd06daace1",
            "3df73aca057045308693f99093d5b288",
            "8a440f05040c43b98480638fc2701cd4",
            "6f5851cfd20c4437b8d7c4e755195b67",
            "6a00f9a9b83a49e29cc411b10be9a9ef",
            "ddba09d5cad04f2e9b783ca5ae17156f",
            "43a1112936604087baf7f00b8cc96007",
            "7748e936d10446fd8c949559270dcc30",
            "f4671acf233342d99565528bce958d36",
            "0195a29e7cd549eca1a37636a779f722",
            "635b4a176f57458ba86171073529045c",
            "917c9a0e69314039abb81df163ea1ad6",
            "282766e0b4364d79bb04f1bd09d98d82",
            "a348b98d62cc482fb1c42220af423958",
            "852e8d4f9a4940d7999a6c85812f2615",
            "587d0d7788194da4ae7346de594afa17",
            "8de2548d5f90445e8cf571dc2f1ac5dd",
            "5a608247c5c44f61ad70e99c75810914",
            "02260e2c73bb4940b7f4f2363138f9ea",
            "f9e73a36174b46f4875e2118fc8282ac",
            "3e83c058b27943dd8c4a16b62d4059d3",
            "ed4a9180f159442ca0b2b9d005439b7b",
            "2897359a25b34d98ae953c3b31a14edf",
            "6fc33e971cc64a66a7a08a61e1e49dc3",
            "af8ce5b495fb493b9f81d83a378c672d",
            "42fff9765bea4ababac7d3af2af83e2b",
            "83f10ef5f55d48f9b7cfd742790fc99f",
            "6aadc785a9ed4561b9019071fdf5e071",
            "2acd9dc35c71468798ef36501d017337",
            "d292ffd50c444ad687d651a065795c09",
            "021eb80e4a814c2b8a7eda3daa7a1efe",
            "8df2c1ec2ecb422f99be7b783e770437",
            "df908dd56bfc465581b54d0c20e48d2d",
            "53ed3f6483e34e2f80bcf3fb4922880c",
            "62f57cdd09f245a295b22659a2817664",
            "1a15958fa202446bb0cd5220b344e580",
            "093dce8ad0a345f49c94e0211b0c64f5",
            "fdcac9c7c13e41cba39d2fe6ce4372ea",
            "801933e2696d4cfdae0a48de176201dd",
            "30293d4b4fc34fe1bf708b1f91ea896f"
          ]
        },
        "outputId": "cd104aae-fd7f-41e7-b966-50dc07a9ac44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.11-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.16.0-py2.py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.8)\n",
            "Collecting appdirs>=1.4.3\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (57.4.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=2e5eb65d0dfff1d2a45e6d00519226abd1884ecef0a577f47fc429936b7a40ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, appdirs, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.16.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.13.11\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.1-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.1 tokenizers-0.13.2 transformers-4.26.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from seqeval) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.9/dist-packages (from seqeval) (1.2.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16179 sha256=64b69adee2d07ff0357acaae358881cc39a8271095bd6d43d0e69df00efd6da9\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/a5/92/2c80d1928733611c2747a9820e1324a6835524d9411510c142\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardx\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 KB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from tensorboardx) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorboardx) (23.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorboardx) (3.19.6)\n",
            "Installing collected packages: tensorboardx\n",
            "Successfully installed tensorboardx-2.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting simpletransformers\n",
            "  Downloading simpletransformers-0.63.9-py3-none-any.whl (250 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.5/250.5 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (2.25.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (1.10.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (2.11.2)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (1.2.2)\n",
            "Requirement already satisfied: transformers>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (4.26.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (2022.6.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (1.2.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wandb>=0.10.32 in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (0.13.11)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (1.22.4)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.19.0-py2.py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers>=4.6.0->simpletransformers) (3.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.6.0->simpletransformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.6.0->simpletransformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.6.0->simpletransformers) (0.13.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (1.3.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (1.4.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (57.4.0)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (3.1.31)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (4.5.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (0.1.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (8.1.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (5.4.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (3.19.6)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (1.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->simpletransformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->simpletransformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->simpletransformers) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->simpletransformers) (2022.12.7)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets->simpletransformers) (2023.3.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets->simpletransformers) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas->simpletransformers) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from pandas->simpletransformers) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->simpletransformers) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->simpletransformers) (1.2.0)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (5.3.0)\n",
            "Collecting pympler>=0.9\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 KB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting blinker>=1.0.0\n",
            "  Downloading blinker-1.5-py2.py3-none-any.whl (12 kB)\n",
            "Collecting watchdog\n",
            "  Downloading watchdog-2.3.1-py3-none-manylinux2014_x86_64.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (0.10.2)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting validators>=0.2\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (8.4.0)\n",
            "Collecting semver\n",
            "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (6.0.0)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (4.2.2)\n",
            "Collecting rich>=10.11.0\n",
            "  Downloading rich-13.3.2-py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.7/238.7 KB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (6.2)\n",
            "Requirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (4.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (2.2.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (0.4.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (1.51.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (2.16.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (3.4.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (0.38.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (1.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (3.1.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.9/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.12.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.10.32->simpletransformers) (1.15.0)\n",
            "Collecting charset-normalizer<4.0,>=2.0\n",
            "  Downloading charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->simpletransformers) (22.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.10)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=1.4->streamlit->simpletransformers) (3.15.0)\n",
            "Collecting pygments<3.0.0,>=2.13.0\n",
            "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markdown-it-py<3.0.0,>=2.2.0\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.9/dist-packages (from tzlocal>=1.1->streamlit->simpletransformers) (0.1.0.post0)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from validators>=0.2->streamlit->simpletransformers) (4.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (2.1.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers) (0.19.3)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (3.2.2)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.9/dist-packages (from pytz-deprecation-shim->tzlocal>=1.1->streamlit->simpletransformers) (2022.7)\n",
            "Building wheels for collected packages: validators\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19581 sha256=6e21ee62eb89e8410a3775e0f7a63afb54e699b6010431dcc250d9b2866e1d10\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/f0/a8/1094fca7a7e5d0d12ff56e0c64675d72aa5cc81a5fc200e849\n",
            "Successfully built validators\n",
            "Installing collected packages: sentencepiece, xxhash, watchdog, validators, semver, pympler, pygments, multidict, mdurl, frozenlist, dill, charset-normalizer, blinker, async-timeout, yarl, responses, pydeck, multiprocess, markdown-it-py, aiosignal, rich, aiohttp, streamlit, datasets, simpletransformers\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.6.1\n",
            "    Uninstalling Pygments-2.6.1:\n",
            "      Successfully uninstalled Pygments-2.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 blinker-1.5 charset-normalizer-3.1.0 datasets-2.10.1 dill-0.3.6 frozenlist-1.3.3 markdown-it-py-2.2.0 mdurl-0.1.2 multidict-6.0.4 multiprocess-0.70.14 pydeck-0.8.0 pygments-2.14.0 pympler-1.0.1 responses-0.18.0 rich-13.3.2 semver-2.13.0 sentencepiece-0.1.97 simpletransformers-0.63.9 streamlit-1.19.0 validators-0.20.0 watchdog-2.3.1 xxhash-3.2.0 yarl-1.8.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0bb6bf817394d7a8a5bb789ed8238d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7748e936d10446fd8c949559270dcc30"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02260e2c73bb4940b7f4f2363138f9ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d292ffd50c444ad687d651a065795c09"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#using pre-trained transformer models using the simple transformers library from the huggingface transformers https://github.com/huggingface/transformers \n",
        "!pip install wandb  #monitor training in realtime\n",
        "!pip install transformers  #make computer understand how humans understand language\n",
        "!pip install seqeval  #used for sequence labelling evaluations,, evaluates NLP \n",
        "!pip install tensorboardx #used for visualizing matrix through histograms or plots\n",
        "!pip install simpletransformers #used to quickly evaluate transformer model, uses SIMPLE one line code to train model\n",
        "\n",
        "from simpletransformers.question_answering import QuestionAnsweringModel\n",
        "\n",
        "train_args = {\n",
        "    'learning_rate': 3e-5,\n",
        "    'num_train_epochs': 2,  #no of training iterations\n",
        "    'max_seq_length': 384, #max length of answer\n",
        "    'doc_stride': 128,  #used to split input text into smaller seq, length of each seq=128\n",
        "    'overwrite_output_dir': True, \n",
        "    'reprocess_input_data': False,\n",
        "    'train_batch_size': 2, \n",
        "    'gradient_accumulation_steps': 8, #before model is implemented\n",
        "}\n",
        "\n",
        "model = QuestionAnsweringModel('bert', 'bert-base-cased', args=train_args)\n",
        "#bert is model type and bert-base-cased is the model name\n",
        "#specifiying pretrained model preserves casing of input text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "OnxjCMUtvEj0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4aec375-bd1a-4f7a-a1e2-da86f16e513f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "#converting the json files to list of dictionary which is the suitable format to perform QA in simple transformers\n",
        "train = [item for topic in train['data'] for item in topic['paragraphs'] ]\n",
        "\n",
        "type(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "8dQhRDFiA7qn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccce6d81-f7bb-426a-f440-252edf9a0919"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19035"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "len(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Gl59LIYfBxwA"
      },
      "outputs": [],
      "source": [
        "#getting a random sample for training\n",
        "import random\n",
        "random.seed(3)\n",
        "randomtrain=random.sample(train, 5000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Pyz4sncbEHtM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cd5eaff-0fee-4dc9-ef5f-3571f5597806"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "len(randomtrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "hzhbZx3XIQKL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cb3428b-d436-47fa-c1b8-0c192c781968"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14035"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "remain = [ele for ele in train if ele not in randomtrain]\n",
        "len(remain) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "EufebeKp68H3"
      },
      "outputs": [],
      "source": [
        "#getting a random sample for testing\n",
        "random.seed(6)\n",
        "randomtest=random.sample(remain, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "b-nGiVE0Eb4o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166,
          "referenced_widgets": [
            "f66e41bd35584a70914b5ad72ec03e30",
            "7c7fda5a5fa6477e93684a51f6e6c1b4",
            "9c8a5f4a384c4c9ca8616b96ba81e1d5",
            "1209338024d7450c97ffb91dd87bb1ce",
            "b959c58ab43f48debe5c899313c47bc8",
            "21a3155086c3454fb01497cf8d1cca51",
            "97fc1475c69a49f28d2ac269b2b9fcc8",
            "4c616a40287d4df19ac0ad0bfdfd2416",
            "8d5b1d418ec345fda161038c61f24ea2",
            "3c212a83a4a34c38993046642b1d523a",
            "95714e5fdb0f40b899bfe4b997b8300f",
            "fe4e8342b7bc4b22858ad25381f69d88",
            "122994e4fe90493e9b3520e60d250acf",
            "6aacab780d0c48809ccccc0ef13ed2a0",
            "7acad35807c7492d82edca70c387c1e1",
            "f1a511ce016845fd88b8e229b0f697e2",
            "31b5096936084c64b644b5e1ca584cbd",
            "a63f6b60b27342ca88dfab89868b8e09",
            "63f073ebad8e4003b3de52371a8c4201",
            "869f35e2aad6414893f86dba710c23eb",
            "b4d4f3eda1ac4e579d10de772ad982f9",
            "20b8ce32c84f478bae5753b923fe092d",
            "816a05098cc5427f8e011ae0b40a018a",
            "f07bb5e2fbc44eeeb7671f421fa5bdab",
            "9f7821f3fff046d9bdb19e0672f57f85",
            "f749c126f5f84fe8b316448f5d3dd29e",
            "b7a2fcab45e34aa3a4c99c879873ed67",
            "d0b3e64d278246a8802eadb3295e9aa5",
            "3d6f3f1d8e7a4e5d81e1b5cfbaa07967",
            "dbd8c432e5f24f28b0063fc3f7e27609",
            "6d007641e9a94d97ab6a21c39ce5c3f5",
            "cfc9667963a74f06b39f12f81d582295",
            "0e6795e29d73493caa0fcb8da399e85e"
          ]
        },
        "outputId": "cbdcab34-0a01-4d2c-d2ce-975926432c90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "convert squad examples to features: 100%|██████████| 34389/34389 [04:04<00:00, 140.74it/s]\n",
            "add example index and unique id: 100%|██████████| 34389/34389 [00:00<00:00, 846580.94it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f66e41bd35584a70914b5ad72ec03e30"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running Epoch 0 of 2:   0%|          | 0/17470 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe4e8342b7bc4b22858ad25381f69d88"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running Epoch 1 of 2:   0%|          | 0/17470 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "816a05098cc5427f8e011ae0b40a018a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4366, 1.376146515984083)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "import sklearn\n",
        "model.train_model(randomtrain, acc=sklearn.metrics.accuracy_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "gszl62A4f2eR"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "pickle.dump(model, open('model.pkl', 'wb'))\n",
        "#into binary format (serializing data)\n",
        "\n",
        "! cp model.pkl \"/content/drive/MyDrive/Colab Notebooks/Squad\"\n",
        "model = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/Squad/model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErmI6eue-16c"
      },
      "source": [
        "Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Cirsrc0m-0LX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140,
          "referenced_widgets": [
            "6af222981aad4fcc922c4ed19a143af4",
            "797efa893ae941d9a5c25dcd14005a05",
            "4c943fa930cc4bfc942cbe49be696e9e",
            "7e2d0d2d4d944790858407b9e416b361",
            "7458e9c47545461e93c9f885561ea521",
            "526776e3471d472aba940f24e76a658e",
            "b887db09095d42b0b5c78d9a0c11973f",
            "745b437fc6cd481f9a917c87ff666f4d",
            "8ad75817597f46d09945391b10570d52",
            "af742c8180bb4ecda5a607a5b9909ba1",
            "5f12786dd82e46679fc57f47d8583b5e"
          ]
        },
        "outputId": "fc48918b-1f8b-4188-a70f-8c28f2b86723"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "convert squad examples to features: 100%|██████████| 6857/6857 [00:46<00:00, 146.82it/s]\n",
            "add example index and unique id: 100%|██████████| 6857/6857 [00:00<00:00, 565780.94it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running Evaluation:   0%|          | 0/864 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6af222981aad4fcc922c4ed19a143af4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'correct': 4000, 'similar': 2431, 'incorrect': 426, 'acc': 0.5833454863642993, 'eval_loss': -7.502771448206018}\n",
            "{'correct_text': {'5731e624e17f3d1400422519': '1936', '5731e624e17f3d140042251a': 'Talaat Harb', '5731e624e17f3d140042251c': 'three quarters', '572805f84b864d190016425e': '150', '572805f84b864d190016425f': 'The Computer and the Brain', '57326c5ae99e3014001e6795': '1974', '57326c5ae99e3014001e6797': 'New York Times', '57326c5ae99e3014001e6798': 'Edwin Pagan', '56de6a2e4396321400ee28ae': '1938', '56de6a2e4396321400ee28af': 'RCA', '5a83215fe60761001a2eb425': '', '5a83215fe60761001a2eb426': '', '5726dad6708984140094d3ac': 'two and a half months', '5726dad6708984140094d3ad': 'two and a half months', '5ad28656d7d075001a4298dc': '', '5ad28656d7d075001a4298dd': '', '5ad28656d7d075001a4298df': '', '5ad28656d7d075001a4298e0': '', '57309103069b531400832191': 'air traffic control', '5a4e914a755ab9001a10f4fb': '', '5a4e914a755ab9001a10f4fc': '', '5a4e914a755ab9001a10f4fd': '', '5a4e914a755ab9001a10f4fe': '', '5709b308ed30961900e84431': 'Civil War', '5709b308ed30961900e84432': 'War of 1812', '5a8cdd89fd22b3001a8d8f6b': '', '56d1ef6ae7d4791d00902599': 'a Buddha', '56d1ef6ae7d4791d0090259a': 'Samsara', '56be932e3aeaaa14008c90f9': '541,000', '56be932e3aeaaa14008c90fa': 'Déjà Vu', '56bf940da10cfb140055118a': 'Jay Z', '56bf940da10cfb140055118b': 'top five', '56d4bc642ccc5a1400d83190': \"B'Day\", '56d4bc642ccc5a1400d83191': '541,000', '56d4bc642ccc5a1400d83192': 'Jay Z', '572b8eb4111d821400f38f06': 'linguist and historian', '572b8eb4111d821400f38f07': '1809', '572b8eb4111d821400f38f0a': 'Modern scholars', '5a7a175117ab25001a8a0316': '', '5a7a175117ab25001a8a0317': '', '5a7a175117ab25001a8a0318': '', '5a7a175117ab25001a8a0319': '', '5a7a175117ab25001a8a031a': '', '57281dc22ca10214002d9e28': 'Nazi Germany', '57281dc22ca10214002d9e29': 'London', '57281dc22ca10214002d9e2b': '1942', '5a36b2f895360f001af1b301': '', '5a36b2f895360f001af1b302': '', '5a36b2f895360f001af1b303': '', '5730487aa23a5019007fd071': '38.8%', '5730487aa23a5019007fd072': 'Themba Dlamini', '5730487aa23a5019007fd073': '80% coverage or greater', '5730487aa23a5019007fd074': '70% to 80%', '5730487aa23a5019007fd075': '18%', '5a56991a6349e2001acdce75': '', '5733e6a54776f41900661472': '1992–93', '5733e6a54776f41900661473': '22', '5733e6a54776f41900661474': 'Brian Deane', '5733e6a54776f41900661476': 'Manchester United', '5ad0c457645df0001a2d026e': '', '5ad0c457645df0001a2d026f': '', '5ad0c457645df0001a2d0270': '', '5725f36689a1e219009ac0f0': 'Herbert Chapman', '5725f36689a1e219009ac0f1': 'pillar box red', '5acd0cc307355d001abf3256': '', '5acd0cc307355d001abf3259': '', '5acd0cc307355d001abf325a': '', '570e6d560dc6ce190020504f': '1940', '570e6d560dc6ce1900205052': '14 January 1958', '5a271a77c93d92001a4003eb': '', '5a271a77c93d92001a4003ec': '', '5a271a77c93d92001a4003ed': '', '5a271a77c93d92001a4003ee': '', '5a271a77c93d92001a4003ef': '', '56cf306baab44d1400b88de7': 'National Park Service', '56cf306baab44d1400b88de8': \"Grant's Tomb\", '56cf306baab44d1400b88de9': 'Greenwich Village', '56cfe6d2234ae51400d9c045': 'National Park Service', '56cfe6d2234ae51400d9c048': 'Stonewall Inn', '56cfe6d2234ae51400d9c049': \"Grant's Tomb\", '570af6876b8089140040f647': 'mobile', '5a1f38073de3f40018b26540': '', '5a1f38073de3f40018b26541': '', '5a1f38073de3f40018b26543': '', '5a1f38073de3f40018b26544': '', '5727d383ff5b5019007d962c': 'sunspots', '5727d383ff5b5019007d962d': 'eclipses', '5727d383ff5b5019007d962f': '635', '5727d383ff5b5019007d9630': '112', '573198280fdd8d15006c63c9': 'May 2005', '573198280fdd8d15006c63cb': '$3.5 million', '573198280fdd8d15006c63cc': 'December 15, 2005', '573198280fdd8d15006c63cd': '14 billion', '5acd677a07355d001abf40d6': '', '5acd677a07355d001abf40d7': '', '5acd677a07355d001abf40d9': '', '5acd677a07355d001abf40da': '', '5ace125632bba1001ae49a39': '', '5ace125632bba1001ae49a3a': '', '5ace125632bba1001ae49a3b': '', '5ace125632bba1001ae49a3c': '', '5ace125632bba1001ae49a3d': '', '56f9313f9b226e1400dd1284': 'three', '56f9313f9b226e1400dd1287': '13th Street', '572fcc11947a6a140053ccd4': 'Borrelia burgdorferi', '572fcc11947a6a140053ccd5': 'single linear chromosome', '5726c029f1498d1400e8ea32': 'Brian May and Roger Taylor', '5726c029f1498d1400e8ea34': 'Modena, Italy', '5726c029f1498d1400e8ea35': 'Robbie Williams', '5727dad64b864d1900163e9d': 'to minimize physical incompatibilities in connectors from different vendors', '572800942ca10214002d9b15': 'when the device is first connected', '572e82aacb0c0d14000f120d': 'steam power', '572e82aacb0c0d14000f120e': 'moving goods in bulk in mines and factories', '572e82aacb0c0d14000f120f': 'Burton and Hormer', '572e82aacb0c0d14000f1210': 'London', '56df20e5c65bf219000b3f79': 'Messiah', '56df20e5c65bf219000b3f7b': 'Nazareth', '56df20e5c65bf219000b3f7c': 'northern Israel', '5ad2d8e3d7d075001a42a457': '', '56f89cb39e9bad19000a01c9': 'George C. Williams', '56f89cb39e9bad19000a01cb': 'Richard Dawkins', '5728b45f3acd2414000dfd15': 'Marathas', '5728b45f3acd2414000dfd17': 'British and Marathas', '5727e6f8ff5b5019007d97f9': 'ethnic Tibetans', '5727e6f8ff5b5019007d97fa': 'India', '572f9a2ba23a5019007fc7cc': 'fining', '572f9a2ba23a5019007fc7cd': 'pig iron', '5731c7ade17f3d14004223d7': '31 October 1517', '5731c7ade17f3d14004223d9': 'Virgin Mary', '5731c7ade17f3d14004223db': 'the Church and the papacy', '572f9e5504bcaa1900d76aeb': 'morphologies', '572f9e5504bcaa1900d76aec': 'one-tenth the size', '572f9e5504bcaa1900d76aed': '0.7 mm', '572f9e5504bcaa1900d76aef': 'not well-studied', '570a661f6d058f1900182e0c': 'Robert C. Solomon', '570a661f6d058f1900182e0e': 'Dutch', '5ad27982d7d075001a4295b0': '', '5ad27982d7d075001a4295b2': '', '5ad27982d7d075001a4295b3': '', '572b8f5d111d821400f38f10': 'Czech', '572b8f5d111d821400f38f11': 'since 2004', '572b8f5d111d821400f38f13': 'Jonathan van Parys', '5a7a185f17ab25001a8a0321': '', '5a7a185f17ab25001a8a0322': '', '5a7a185f17ab25001a8a0323': '', '5a7a185f17ab25001a8a0324': '', '5731b71e0fdd8d15006c6485': 'Rhode Island', '5731b71e0fdd8d15006c6486': '1644', '5731b71e0fdd8d15006c6487': 'Thomas Jefferson', '5ad13aec645df0001a2d12ee': '', '5ad13aec645df0001a2d12ef': '', '5ad13aec645df0001a2d12f0': '', '5ad13aec645df0001a2d12f1': '', '5728c1482ca10214002da71a': '1848', '5728c1482ca10214002da71b': '1860', '5728c1482ca10214002da71c': 'Georges-Eugène Haussmann', '572eb077c246551400ce4518': 'Voyager 2', '572eb077c246551400ce4519': '25 August 1989', '5acee61232bba1001ae4b8f5': '', '5acee61232bba1001ae4b8f6': '', '5acee61232bba1001ae4b8f7': '', '5acee61232bba1001ae4b8f8': '', '5acee61232bba1001ae4b8f9': '', '56df6c5a56340a1900b29af6': '26.2%', '56df6c5a56340a1900b29af7': '78.3 years', '5733f55e4776f419006615ab': 'five waters', '5733f55e4776f419006615ae': '91.379.615', '5733f55e4776f419006615af': 'Lahore', '5a68b42b8476ee001a58a76a': '', '5a68b42b8476ee001a58a76b': '', '5a68b42b8476ee001a58a76e': '', '57274eb3f1498d1400e8f60b': '2000', '571adfb39499d21900609b71': 'limited to the Septuagint', '571adfb39499d21900609b73': 'Septuagint Greek', '5ace995332bba1001ae4ac34': '', '5ace995332bba1001ae4ac35': '', '5ace995332bba1001ae4ac36': '', '5aceb60032bba1001ae4b11a': '', '5aceb60032bba1001ae4b11c': '', '5acd198007355d001abf34ca': '', '5acd198007355d001abf34cb': '', '5acd198007355d001abf34cc': '', '5acd198007355d001abf34cd': '', '5acd198007355d001abf34ce': '', '5a0cdf07f5590b0018dab5fe': '', '5731f389e99e3014001e6414': 'limited', '5731f389e99e3014001e6415': 'animal sacrifice', '5731f389e99e3014001e6417': 'Bona Dea', '5731f389e99e3014001e6418': 'motherhood', '57264dedf1498d1400e8db8d': '1883', '57264dedf1498d1400e8db8e': '1886', '57264dedf1498d1400e8db8f': '1893', '5a157229a54d420018529435': '', '5a157229a54d420018529437': '', '5a157229a54d420018529438': '', '5a157229a54d420018529439': '', '5731e0ad0fdd8d15006c65eb': 'constitution of the United States', '5731e0ad0fdd8d15006c65ed': 'John F. Kennedy', '5731e0ad0fdd8d15006c65ee': 'God', '5ad14c5d645df0001a2d164d': '', '5ad14c5d645df0001a2d164e': '', '5ad14c5d645df0001a2d1650': '', '57094dde9928a81400471514': 'Best Places for Business and Careers', '57094dde9928a81400471515': '92', '57094dde9928a81400471516': 'Forty', '57094dde9928a81400471517': 'Twenty-five', '57094dde9928a81400471518': '13', '5709c252200fba14003682b1': '2006', '5709c252200fba14003682b3': '92', '5709c252200fba14003682b4': 'Forty', '5709c252200fba14003682b5': '13', '5ad420c6604f3c001a400715': '', '5ad420c6604f3c001a400716': '', '5ad420c6604f3c001a400718': '', '5ad420c6604f3c001a400719': '', '57273aa2dd62a815002e99c3': 'Charles I of Hungary', '57273aa2dd62a815002e99c4': '1325', '57273aa2dd62a815002e99c6': '1348', '5ad022b677cf76001a686b5e': '', '5ad022b677cf76001a686b60': '', '5ad022b677cf76001a686b61': '', '5ad022b677cf76001a686b62': '', '572f9e8204bcaa1900d76af5': 'Timber', '572f9e8204bcaa1900d76af9': 'Timber', '56def0acc65bf219000b3e3b': 'conflict', '5ad2ccebd7d075001a42a2ac': '', '5ad2ccebd7d075001a42a2ad': '', '5ad2ccebd7d075001a42a2ae': '', '5ad2ccebd7d075001a42a2af': '', '57301bfca23a5019007fcd82': 'subtherapeutic antibiotic treatment', '57328cf2b3a91d1900202e34': 'Early life', '5a65bfbec2b11c001a425d2f': '', '5a65bfbec2b11c001a425d30': '', '5a65bfbec2b11c001a425d31': '', '5a65bfbec2b11c001a425d32': '', '5a65bfbec2b11c001a425d33': '', '56f8ee329e9bad19000a0718': '7 to 13', '56f8ee329e9bad19000a071a': '53.5%', '572eaae1dfa6aa1500f8d287': '1370', '572eaae1dfa6aa1500f8d28a': 'dots and chapter separators', '5ad218cfd7d075001a428406': '', '5ad218cfd7d075001a428407': '', '5ad218cfd7d075001a428408': '', '5ad218cfd7d075001a428409': '', '57299fb33f37b31900478517': 'divides the mountain system into two unequal portions', '5ace1dc132bba1001ae49b20': '', '5ace1dc132bba1001ae49b21': '', '5ace1dc132bba1001ae49b23': '', '57318f33a5e9cc1400cdc07f': 'Calçada Portuguesa', '57318f33a5e9cc1400cdc080': 'two-tone stone mosaic paving', '57318f33a5e9cc1400cdc082': 'Lisbon', '570e63fe0dc6ce1900205003': '1956', '570e63fe0dc6ce1900205004': 'Europe and the United States', '570e63fe0dc6ce1900205005': '2006, 2008 and 2010', '570e63fe0dc6ce1900205006': 'Olympic Stand', '570e63fe0dc6ce1900205007': 'Melbourne', '56df56288bc80c19004e4ac9': 'May 2015', '56d3847159d6e414001465fd': 'Taylor Hicks', '56d3847159d6e414001465fe': 'Alabama', '56db3ebee7c41114004b4f99': 'Alabama', '5acfc10177cf76001a685cd0': '', '5acfc10177cf76001a685cd1': '', '5acfc10177cf76001a685cd3': '', '5acfc10177cf76001a685cd4': '', '5733f37ed058e614000b664f': 'three', '5733f37ed058e614000b6650': 'Navy, Army and Air Force', '5733f37ed058e614000b6652': '7,500', '5726069c38643c19005acf62': 'Greeks', '57270870dd62a815002e9823': 'degradation of the dielectric', '5acf5e9177cf76001a684c88': '', '5acf5e9177cf76001a684c89': '', '5acf5e9177cf76001a684c8a': '', '5732a8a6328d981900601fe9': 'ice cap', '5732a8a6328d981900601fea': 'the Alps', '5732a8a6328d981900601feb': 'Oligocene', '5732a8a6328d981900601fec': 'Antarctica', '5a4ebc82af0d07001ae8cc15': '', '5a4ebc82af0d07001ae8cc16': '', '5a4ebc82af0d07001ae8cc17': '', '5a4ebc82af0d07001ae8cc18': '', '56f8b9549e9bad19000a03b6': 'a union of genomic sequences encoding a coherent set of potentially overlapping functional products', '56f8b9549e9bad19000a03b8': 'their functional products (proteins or RNA)', '56f8b9549e9bad19000a03b9': 'regulatory elements', '572841842ca10214002da1af': 'Armed Forces Special Weapons Project', '572841842ca10214002da1b0': 'General Advisory Committee', '572a1ef73f37b319004786ff': 'Jacques Cauvin and Oliver Aurenche', '572a1ef73f37b31900478700': 'social, economic and cultural characteristics', '5a7d103570df9f001a874f4d': '', '5a7d103570df9f001a874f4e': '', '5a7d103570df9f001a874f4f': '', '5a7d103570df9f001a874f50': '', '5a7d103570df9f001a874f51': '', '572ecec0cb0c0d14000f15b4': 'Wang Mang', '572ecec0cb0c0d14000f15b5': 'Gradual silt buildup', '572ecec0cb0c0d14000f15b6': 'Han engineers', '572ecec0cb0c0d14000f15b8': 'massive floods', '5731ce3ab9d445190005e575': '1524–25', '5731ce3ab9d445190005e577': 'between 25% and 40%', '5731ce3ab9d445190005e579': 'the Bavarian, Thuringian and Swabian principalities', '572774e65951b619008f8a5b': 'Carnivals', '572774e65951b619008f8a5d': 'Blancs-Moussis', '572774e65951b619008f8a5e': 'Laetare Sunday', '572774e65951b619008f8a5f': 'Carnival Parade of Maaseik', '56cf54baaab44d1400b89010': '1963', '56cf54baaab44d1400b89011': '21', '56d39d2359d6e41400146821': '1963', '56d39d2359d6e41400146822': '21', '572a2c791d04691400779812': 'Little Mosque on the Prairie and The Border', '5a54e92d134fea001a0e1773': '', '5a54e92d134fea001a0e1775': '', '57261f2fec44d21400f3d92b': 'Zeuxis', '57261f2fec44d21400f3d92c': 'larger', '57261f2fec44d21400f3d92d': 'Seleucus', '57261f2fec44d21400f3d92e': 'basileion', '5706ef6c9e06ca38007e9226': '8th', '5706ef6c9e06ca38007e9227': 'uncial', '5706ef6c9e06ca38007e9228': '835', '5706ef6c9e06ca38007e9229': 'Uspenski Gospels', '5728e7254b864d1900165060': 'inductivist', '5728e7254b864d1900165061': 'Imre Lakatos', '5728e7254b864d1900165063': \"Einstein's theory\", '5ad27cb0d7d075001a429696': '', '5ad27cb0d7d075001a429697': '', '5ad27cb0d7d075001a429698': '', '5ad27cb0d7d075001a429699': '', '5ad27cb0d7d075001a42969a': '', '5a79d0b117ab25001a8a00c6': '', '5a79d0b117ab25001a8a00c7': '', '5a79d0b117ab25001a8a00c8': '', '5a79d0b117ab25001a8a00ca': '', '570d61a5b3d812140066d7a5': 'Salvados', '570d61a5b3d812140066d7a8': 'El Mundo', '570d61a5b3d812140066d7a9': 'March 2012', '5725d45b89a1e219009abf62': 'Chess', '5725d45b89a1e219009abf63': 'Beersheba', '5725d45b89a1e219009abf64': 'Boris Gelfand', '57282a944b864d1900164638': 'body cavities', '57282a944b864d1900164639': 'septa', '57282a944b864d190016463b': 'modified epitheliomuscular cells', '5ace853932bba1001ae4a953': '', '5ace853932bba1001ae4a956': '', '56e087957aa994140058e5c2': 'triplet state', '56e087957aa994140058e5c4': '25%', '56e087957aa994140058e5c5': '75%', '56df81eb5ca0a614008f9bbd': 'Tavistock', '56df81eb5ca0a614008f9bbf': '1596', '56df81eb5ca0a614008f9bc0': 'dysentery', '56df81eb5ca0a614008f9bc1': 'Sir Joshua Reynolds', '5727a6233acd2414000de8d6': 'Gateway Community College', '5727a6233acd2414000de8d8': 'Yale University', '5727a6233acd2414000de8d9': '2012', '572a37a3af94a219006aa8be': 'Southern Connecticut State University', '572a37a3af94a219006aa8bf': 'Albertus Magnus College', '572a37a3af94a219006aa8c0': 'Long Wharf district', '572a37a3af94a219006aa8c1': 'Fall 2012', '572ea0bedfa6aa1500f8d21d': 'high school grades', '572ea0bedfa6aa1500f8d21e': 'high-school diploma', '572ea0bedfa6aa1500f8d220': '25%', '56f957d89e9bad19000a0853': 'Morningside Avenue', '56f957d89e9bad19000a0855': 'six', '56f957d89e9bad19000a0856': 'Mount Morris', '56f957d89e9bad19000a0857': 'Lenox Avenue', '5729234a1d046914007790a5': 'hypotheses', '5729234a1d046914007790a6': 'continental', '5729234a1d046914007790a8': 'racial groupings', '572b8b54111d821400f38efc': 'widespread national pride', '572b8b54111d821400f38eff': 'the return of the language to high culture', '5a7a161417ab25001a8a030d': '', '5a7a161417ab25001a8a030e': '', '5a7a161417ab25001a8a030f': '', '5a7a161417ab25001a8a0310': '', '570a85944103511400d59800': 'Ælfric of Eynsham', '570a85944103511400d59801': 'Bishop Æthelwold of Winchester', '570a85944103511400d59802': '10th', '570a85944103511400d59803': 'Late West Saxon', '5a678d67f038b7001ab0c2b2': '', '5a678d67f038b7001ab0c2b3': '', '5a678d67f038b7001ab0c2b4': '', '570ddc210dc6ce1900204cce': 'lose weight', '572fde5e04bcaa1900d76dfe': 'full-time or part-time', '572fde5e04bcaa1900d76dff': 'Mahendra R. Gupta', '572fde5e04bcaa1900d76e00': '40–60%', '5ace2b5c32bba1001ae49c7a': '', '56f8d8959e9bad19000a05e0': '2012', '5726c9c25951b619008f7e20': '1950', '5726e581f1498d1400e8ef28': 'Hoog Catharijne', '5a580e33770dc0001aeeffa1': '', '5a580e33770dc0001aeeffa2': '', '56fc3f0e00a8df190040382a': 'phonological theory', '56fc3f0e00a8df190040382b': 'synchronic and diachronic accounts', '5a8209af31013a001a335127': '', '5a8209af31013a001a335128': '', '5a8209af31013a001a335129': '', '5a8209af31013a001a33512a': '', '5a8209af31013a001a33512b': '', '56e14acbcd28a01900c6774c': 'April 1927', '5ad13864645df0001a2d122c': '', '5ad13864645df0001a2d122e': '', '5728d51a4b864d1900164f06': 'The Estonian Academy of Arts', '5728d51a4b864d1900164f07': 'Viljandi Culture Academy of University of Tartu', '5728d51a4b864d1900164f09': '245', '56fa85dd8f12f31900630170': 'December 2009', '56fa85dd8f12f31900630171': 'DVB-T2', '56fa85dd8f12f31900630172': 'Digital TV Group', '5ad3bee3604f3c001a3fef4c': '', '5ad3bee3604f3c001a3fef4d': '', '5732bcead6dcfa19001e8a99': 'batons, tear gas, riot control agents, rubber bullets, riot shields, water cannons and electroshock weapons', '5732bcead6dcfa19001e8a9a': 'The use of firearms or deadly force', '5732bcead6dcfa19001e8a9c': 'Brazil', '5acece2e32bba1001ae4b4ac': '', '5acece2e32bba1001ae4b4af': '', '572ee3a8c246551400ce4782': \"coup d'état\", '5727903ddd62a815002ea084': 'high cost', '5727903ddd62a815002ea086': '1925', '5727903ddd62a815002ea087': '$20', '5727903ddd62a815002ea088': 'cabinetry', '56d13543e7d4791d00902004': '2010', '56f81f0ea6d7ea1400e173d7': 'Europe', '56f81f0ea6d7ea1400e173d9': 'over tens of millions of years', '56f81f0ea6d7ea1400e173da': 'Mont Blanc', '57313c12a5e9cc1400cdbd6b': 'Alphanumeric', '57313c12a5e9cc1400cdbd6d': 'Starburst', '57313c12a5e9cc1400cdbd6e': '5x7', '5ad194ca645df0001a2d2066': '', '5ad194ca645df0001a2d2067': '', '5ad194ca645df0001a2d2068': '', '5ad194ca645df0001a2d2069': '', '5ad194ca645df0001a2d206a': '', '56f744beaef2371900625a79': 'Baroque era', '56ceb9c4aab44d1400b8892b': '2,300', '56ceb9c4aab44d1400b8892d': '9,000', '56ceb9c4aab44d1400b8892e': '3,000 to 5,000', '56ceb9c4aab44d1400b8892f': '10,000', '56d5307f2593cc1400307abb': '2,300', '56d5307f2593cc1400307abd': '3,000 to 5,000', '56d5307f2593cc1400307abe': '10,000', '572a69cefed8de19000d5bfd': 'promoting better eating practices', '5a0e3462d7c850001886454a': '', '5a0e3462d7c850001886454c': '', '5a0e3462d7c850001886454d': '', '57110273b654c5140001fab1': 'alphabetical', '57110273b654c5140001fab3': 'the number of educated consumers who could afford such texts began to multiply', '57110273b654c5140001fab4': '63', '57110273b654c5140001fab5': '148', '572817474b864d1900164458': 'eclecticist', '572817474b864d1900164459': 'World War II', '572817474b864d190016445a': 'Palais du Rhin', '572817474b864d190016445b': 'Höhere Mädchenschule', '5acd6c4a07355d001abf4182': '', '5acd6c4a07355d001abf4183': '', '5acd6c4a07355d001abf4185': '', '5acd6c4a07355d001abf4186': '', '572f820a04bcaa1900d76a38': 'Fort Duquesne and Fort Frontenac', '572f820a04bcaa1900d76a39': 'the Acadian population', '5728f64eaf94a219006a9e7b': 'samurai', '5728f64eaf94a219006a9e7c': 'monochrome ink', '5728f64eaf94a219006a9e7e': 'Zen monks', '5728e8b43acd2414000e01ab': 'Malcolm Turnbull', '5728e8b43acd2414000e01ac': 'Julie Bishop', '5728e8b43acd2414000e01ad': 'Tony Abbott', '5728e8b43acd2414000e01ae': 'the Abbott Government', '5a6a14595ce1a5001a9696db': '', '5a6a14595ce1a5001a9696dc': '', '5a6a14595ce1a5001a9696dd': '', '5a6a14595ce1a5001a9696de': '', '570d2b46b3d812140066d4db': 'the \"National Army\"', '570d2b46b3d812140066d4dc': 'end of World War I', '5acecfac32bba1001ae4b4f9': '', '5acecfac32bba1001ae4b4fc': '', '5acecfac32bba1001ae4b4fd': '', '57261b6dec44d21400f3d8f5': 'undocumented', '5a0cfe3af5590b0018dab6fa': '', '5a0cfe3af5590b0018dab6fb': '', '5a0cfe3af5590b0018dab6fd': '', '5a0cfe3af5590b0018dab6fe': '', '57268816dd62a815002e885e': '1976', '5727ba9f4b864d1900163ba2': 'Nimbarka', '5727ba9f4b864d1900163ba4': 'three', '5727ba9f4b864d1900163ba5': 'Brahman, soul, and matter', '5727ba9f4b864d1900163ba6': 'Krishna', '5a5e608c5bc9f4001a75af87': '', '5a5e608c5bc9f4001a75af89': '', '5a5e608c5bc9f4001a75af8a': '', '5a5e608c5bc9f4001a75af8b': '', '572629d9ec44d21400f3db2c': '1717', '572629d9ec44d21400f3db2d': 'English traders', '5726572af1498d1400e8dc7c': 'Bengal', '5726572af1498d1400e8dc7d': 'waived customs duties', '5726572af1498d1400e8dc7f': 'The Dutch', '5726572af1498d1400e8dc80': 'Anglo-Dutch Wars', '5a8456db7cf838001a46a75e': '', '5a8456db7cf838001a46a761': '', '570cbdcfb3d812140066d263': 'biological mother', '570cbdcfb3d812140066d264': 'Emanuel Swedenborg', '570cbdcfb3d812140066d265': 'Nontrinitarians', '570cbdcfb3d812140066d266': 'Marian', '5ad186c5645df0001a2d1e96': '', '56e0fd33231d4119001ac54e': 'three', '5acd345807355d001abf3946': '', '5acd345807355d001abf3947': '', '5acd345807355d001abf3949': '', '5acd345807355d001abf394a': '', '572ea349cb0c0d14000f13b8': 'conceptual art', '572ea349cb0c0d14000f13ba': '2006', '57271234f1498d1400e8f31e': 'Pakistan', '57271234f1498d1400e8f320': 'Ayurveda', '56d97d8ddc89441400fdb4e4': 'spirit of Olympics', '56d97d8ddc89441400fdb4e5': 'sports and politics', '56db76fde7c41114004b515a': 'run for spirit of Olympics', '56db76fde7c41114004b515b': 'sports and politics', '56f7e518aef2371900625c50': 'opole', '57268e54dd62a815002e8966': '1967', '57268e54dd62a815002e8967': '1971', '57268e54dd62a815002e8968': '1976', '57268e54dd62a815002e8969': 'Denis Healey', '5ad13aca645df0001a2d12d4': '', '5ad13aca645df0001a2d12d5': '', '5ad13aca645df0001a2d12d7': '', '5ad13aca645df0001a2d12d8': '', '572ff0a2947a6a140053ce36': 'Tehran', '572ff0a2947a6a140053ce39': '19', '572ff0a2947a6a140053ce3a': 'through its large reserves of fossil fuels', '572690d45951b619008f76d1': 'Avenida de los Insurgentes', '56df226bc65bf219000b3f8d': '1986', '56df226bc65bf219000b3f8e': '$125 million', '56df226bc65bf219000b3f90': 'Stephen Swid, Martin Bandier, and Charles Koppelman', '5ace33db32bba1001ae49df7': '', '5ace33db32bba1001ae49df8': '', '5ace33db32bba1001ae49df9': '', '5acf76ad77cf76001a684e79': '', '5acf76ad77cf76001a684e7a': '', '5acf76ad77cf76001a684e7b': '', '5acf76ad77cf76001a684e7c': '', '5707070890286e26004fc805': 'Jornado', '5707070890286e26004fc806': 'Rio Grande', '5707070890286e26004fc807': 'hunting, gathering, and farming', '5707070890286e26004fc808': 'Tepehuan', '5707070890286e26004fc809': 'rock', '5731d56fe17f3d1400422474': 'Los Angeles', '5731d56fe17f3d1400422475': 'urban', '56df785856340a1900b29bee': '1823', '56df785856340a1900b29bef': 'September 2011', '56df785856340a1900b29bf0': 'nine', '5728b2813acd2414000dfcf8': '19th century', '5728b2813acd2414000dfcfa': 'Madeira Island', '5ad22fced7d075001a42869b': '', '5ad22fced7d075001a42869c': '', '5ad22fced7d075001a42869d': '', '5ad22fced7d075001a42869e': '', '572ec004c246551400ce45fa': 'Sweden', '5727721add62a815002e9d05': '1444', '5727721add62a815002e9d07': 'count John Hunyadi', '5727721add62a815002e9d08': 'Pope Pius II', '5ad02eec77cf76001a686d5f': '', '572782b6dd62a815002e9f30': 'charter', '572782b6dd62a815002e9f32': 'instant-runoff', '5ace451232bba1001ae4a143': '', '5ace451232bba1001ae4a144': '', '5ace451232bba1001ae4a145': '', '5ace451232bba1001ae4a146': '', '57313b16e6313a140071cd51': 'Christianity', '5a81ff2b31013a001a335085': '', '5a81ff2b31013a001a335087': '', '5a81ff2b31013a001a335088': '', '5726d3475951b619008f7f25': 'Wake Island', '57318a1305b4da19006bd25e': 'tile', '5acf9e8d77cf76001a68557c': '', '5acf9e8d77cf76001a68557d': '', '572816302ca10214002d9d98': 'Ukrainian Independence Day', '572816302ca10214002d9d99': 'religious service', '5726a792dd62a815002e8c1a': 'February 1985', '5726a792dd62a815002e8c1b': 'Vision Quest', '5726a792dd62a815002e8c1c': 'March 1985', '5726fd0f5951b619008f8423': '25 September', '5726fd0f5951b619008f8425': 'Sir John Burgoyne', '5726fd0f5951b619008f8426': 'Raglan and St Arnaud', '570d3ebefed7b91900d45d8e': 'Mediterranean western coast', '570d3ebefed7b91900d45d8f': 'second', '570d3ebefed7b91900d45d90': 'oranges', '572f02fbcb0c0d14000f1709': 'Thailand', '572f02fbcb0c0d14000f170a': 'colonial powers', '5732488d0fdd8d15006c68eb': '15th Infantry', '5732488d0fdd8d15006c68ef': 'Louisiana Maneuvers', '57266708708984140094c4e6': 'Karstadt', '57266708708984140094c4e8': 'GALERIA Kaufhof', '5ad0b1c4645df0001a2d007a': '', '5ad0b1c4645df0001a2d007b': '', '5ad0b1c4645df0001a2d007c': '', '5ad0b1c4645df0001a2d007d': '', '5ad0b1c4645df0001a2d007e': '', '56dedfc3c65bf219000b3d9e': '2012', '56dedfc3c65bf219000b3d9f': 'chairman', '56f8cc9a9b226e1400dd102f': 'Farming', '56f8cc9a9b226e1400dd1031': 'because of the steep and rocky topography of the Alps', '56f8cc9a9b226e1400dd1032': 'mid-June', '572ff633a23a5019007fcbbc': 'New York Times', '572ff633a23a5019007fcbbd': 'Professor Ehsan Yarshater', '572ff633a23a5019007fcbbe': 'Iran', '573192cde6313a140071d0c2': '1989', '573192cde6313a140071d0c3': 'Indiana Jones and the Last Crusade', '573192cde6313a140071d0c4': 'Sean Connery', '573192cde6313a140071d0c5': 'Tim Burton', '573192cde6313a140071d0c6': 'a daredevil pilot who extinguishes forest fires', '5ad4c0485b96ef001a109f31': '', '5ad4c0485b96ef001a109f32': '', '570a6f236d058f1900182e56': 'Paul Ekman', '570a6f236d058f1900182e57': 'six', '570a6f236d058f1900182e58': 'surprise', '5ad243b2d7d075001a428a1b': '', '5ad243b2d7d075001a428a1c': '', '5726a88e5951b619008f7950': 'The resurgent republican movement', '5a21dc2f8a6e4f001aa08fae': '', '5a21dc2f8a6e4f001aa08fb1': '', '5a21dc2f8a6e4f001aa08fb2': '', '5acf8ff377cf76001a685276': '', '5acf8ff377cf76001a685277': '', '5acf8ff377cf76001a685278': '', '572f78f5b2c2fd1400568162': 'enterprise database management', '5a8c840bfd22b3001a8d89de': '', '5a8c840bfd22b3001a8d89df': '', '5a8c840bfd22b3001a8d89e0': '', '5a8c840bfd22b3001a8d89e1': '', '57313c97e6313a140071cd6c': 'Qianlong', '57313c97e6313a140071cd6d': '53', '56f754a3a6d7ea1400e171bf': '2010', '56f754a3a6d7ea1400e171c0': 'Peter Underhill', '5ad4c6d55b96ef001a10a01a': '', '5ad4c6d55b96ef001a10a01b': '', '5ad4c6d55b96ef001a10a01c': '', '56cecf68aab44d1400b88ab8': 'tofu-dregs schoolhouses', '56d64a821c85041400947076': 'over 7,000', '56d64a821c85041400947077': 'tofu-dregs schoolhouses', '56d64a821c85041400947079': 'legal replacements', '5727c9722ca10214002d9634': 'Thunderbolt', '56e0f68f7aa994140058e833': 'Valentina Tereshkova', '56e0f68f7aa994140058e834': 'Vostok 6', '570a88724103511400d59818': '$1.1 billion', '570a88724103511400d59819': '24,000', '570a88724103511400d5981a': '12,500', '570a88724103511400d5981c': '80.5%', '5ad42096604f3c001a400701': '', '5ad42096604f3c001a400703': '', '57282c833acd2414000df63f': 'Eunicidae and Phyllodocidae', '5ace874132bba1001ae4a980': '', '5ace874132bba1001ae4a981': '', '5ace874132bba1001ae4a982': '', '573238630fdd8d15006c685f': 'pontifex maximus', '573238630fdd8d15006c6861': 'Theodosius I', '573238630fdd8d15006c6862': 'Sacred fire', '573238630fdd8d15006c6863': 'East and West', '572618d0ec44d21400f3d8c5': 'Al-Biruni', '572618d0ec44d21400f3d8c6': '150–100 BC', '56f7fd15a6d7ea1400e1735f': 'Varius', '56f7fd15a6d7ea1400e17360': 'Servius and Donatus', '56f7fd15a6d7ea1400e17361': 'inferences made from his poetry and allegorizing', '56f7fd15a6d7ea1400e17362': 'problematic', '5a7e3aec70df9f001a8755c5': '', '5a7e3aec70df9f001a8755c6': '', '5a7e3aec70df9f001a8755c7': '', '5a7e3aec70df9f001a8755c8': '', '5a7e3aec70df9f001a8755c9': '', '5709720ded30961900e84165': 'Thompson Seedless', '5709720ded30961900e84166': 'Niagara grapes', '5727aa682ca10214002d9339': '548,000', '5a6fe7e38abb0b001a676037': '', '5a6fe7e38abb0b001a676038': '', '5a6fe7e38abb0b001a676039': '', '5a6fe7e38abb0b001a67603b': '', '57269b6b708984140094cb75': 'impure', '57269b6b708984140094cb78': 'wrought iron', '5a2052ef54a786001a36b303': '', '5a2052ef54a786001a36b304': '', '57105362b654c5140001f8cb': 'France', '57105362b654c5140001f8cc': 'Isaac Newton', '57105362b654c5140001f8cd': 'France', '572818a9ff5b5019007d9d20': '25,000', '572818a9ff5b5019007d9d21': 'one-hour', '572818a9ff5b5019007d9d22': 'eight local factories and institutions', '572818a9ff5b5019007d9d23': '1944', '57096228ed30961900e8403e': 'handicrafts', '57096228ed30961900e8403f': 'woolen and pashmina shawls', '57096228ed30961900e84040': 'increased', '57096228ed30961900e84041': 'aesthetic and tasteful handicrafts', '5a362317788daf001a5f8746': '', '5a362317788daf001a5f8747': '', '5a362317788daf001a5f8749': '', '57265630708984140094c2d1': 'Lawrence Lessig', '5a15b101a54d4200185294ad': '', '5a15b101a54d4200185294ae': '', '5a15b101a54d4200185294af': '', '5a15b101a54d4200185294b0': '', '572675a3dd62a815002e85b2': '10,000,000', '572675a3dd62a815002e85b6': 'Informal Empire', '5728f26aaf94a219006a9e27': 'Romanticism', '5728f26aaf94a219006a9e28': 'Montmartre', '5728f26aaf94a219006a9e29': 'Montmartre and Montparnasse', '5728f26aaf94a219006a9e2a': '1905 and 1907', '5ad28a70d7d075001a4299b8': '', '5ad28a70d7d075001a4299b9': '', '5ad28a70d7d075001a4299bb': '', '5ad28a70d7d075001a4299bc': '', '5ad211e7d7d075001a4282de': '', '5ad211e7d7d075001a4282df': '', '5ad211e7d7d075001a4282e1': '', '5726e2bf708984140094d4c3': 'Muslim', '5726e2bf708984140094d4c4': 'rational philosophy', '5726e2bf708984140094d4c6': '19th century', '5ace618b32bba1001ae4a4cd': '', '5ace618b32bba1001ae4a4ce': '', '5ace618b32bba1001ae4a4d0': '', '5ace618b32bba1001ae4a4d1': '', '5acea21632bba1001ae4ae1a': '', '5acea21632bba1001ae4ae1b': '', '5acea21632bba1001ae4ae1d': '', '572ac9cbbe1ee31400cb8255': '$750 million', '572ac9cbbe1ee31400cb8258': '2002', '572947026aef051400154c4c': 'John Rolfe', '5ad40858604f3c001a3ffee1': '', '5ad40858604f3c001a3ffee2': '', '5ad40858604f3c001a3ffee4': '', '5ad40858604f3c001a3ffee5': '', '5726fec6dd62a815002e973c': 'where the accumulation of snow and ice exceeds ablation', '5726fec6dd62a815002e973e': 'armchair-shaped', '5a358b22788daf001a5f8624': '', '5a358b22788daf001a5f8626': '', '5a358b22788daf001a5f8627': '', '5ad50a975b96ef001a10aa91': '', '5ad50a975b96ef001a10aa92': '', '5ad50a975b96ef001a10aa93': '', '5ad50a975b96ef001a10aa94': '', '5ad690ae191832001aa7b21f': '', '5ad690ae191832001aa7b220': '', '572f8a4904bcaa1900d76a6a': 'thalamus', '572f8a4904bcaa1900d76a6b': 'spinal cord fibers', '572f8a4904bcaa1900d76a6c': 'the insular cortex', '5acd38ac07355d001abf3984': '', '572688895951b619008f7601': 'Jack Brickhouse', '572688895951b619008f7602': 'Hey Hey!', '572745d8f1498d1400e8f590': 'Washington D.C.', '572745d8f1498d1400e8f591': 'help to open doors for historically excluded groups in workplace settings and higher education', '572745d8f1498d1400e8f592': 'actively seek to promote an inclusive workplace', '5ad4171c604f3c001a400377': '', '5ad4171c604f3c001a400378': '', '5ad4171c604f3c001a400379': '', '5ad4171c604f3c001a40037a': '', '570d2d61b3d812140066d4ed': '2002', '570d2d61b3d812140066d4ef': '2008', '570d2d61b3d812140066d4f1': 'May 2010', '59d291ec2763a600182840d1': '', '59d291ec2763a600182840d3': '', '572a54e07a1753140016aeb3': 'August 31, 2011', '5a5535e0134fea001a0e19d7': '', '5a5535e0134fea001a0e19d8': '', '5a5535e0134fea001a0e19d9': '', '5a5535e0134fea001a0e19da': '', '5731dd32e17f3d14004224ba': 'pervasive', '5731dd32e17f3d14004224bb': 'most Protestants (and most Jews)', '5731dd32e17f3d14004224bd': 'strict separationism', '5ad14b17645df0001a2d15e0': '', '5ad14b17645df0001a2d15e1': '', '5ad14b17645df0001a2d15e2': '', '5ad14b17645df0001a2d15e3': '', '5ad14b17645df0001a2d15e4': '', '5726bbc15951b619008f7c55': 'Wake County Public School System', '5726bbc15951b619008f7c56': 'innovative efforts to maintain a socially, economically and racial balanced system', '5726bbc15951b619008f7c57': 'three', '5726bbc15951b619008f7c58': 'International Baccalaureate', '5acd648707355d001abf407c': '', '5acd648707355d001abf407d': '', '5acd648707355d001abf407e': '', '5acd648707355d001abf407f': '', '5acd648707355d001abf4080': '', '571008d5a58dae1900cd67eb': 'such that individuals in the same category show the same balance between the heterosexual and homosexual elements in their histories', '571008d5a58dae1900cd67ec': \"relation of heterosexuality to homosexuality in one's history\", '5a86079eb4e223001a8e73cb': '', '5a86079eb4e223001a8e73cc': '', '5a86079eb4e223001a8e73cd': '', '572b65e4be1ee31400cb8359': \"Guam rail (or ko'ko' bird in Chamorro) and the Guam flycatcher\", '5ace640e32bba1001ae4a553': '', '56e8daed0b45c0140094cd18': 'Henry Yevele', '56e8daed0b45c0140094cd1b': \"The Confessor's shrine\", '5ad3f42d604f3c001a3ff92c': '', '5ad3f42d604f3c001a3ff92d': '', '5ad3f42d604f3c001a3ff92f': '', '5728ee664b864d19001650b5': '40,000', '5728ee664b864d19001650b6': '900', '5728ee664b864d19001650b7': 'northern Kyūshū', '5728ee664b864d19001650b8': '10,000', '572670f9dd62a815002e84fa': 'Don Baylor', '572670f9dd62a815002e84fb': 'Mack Newton', '572670f9dd62a815002e84fc': 'Preston Wilson', '5728b1163acd2414000dfcd1': 'Andy Irvine', '5728b1163acd2414000dfcd2': 'John Sheahan and the late Barney McKenna', '5728b1163acd2414000dfcd5': 'Rory Gallagher', '5ad22b4bd7d075001a4285ec': '', '5ad22b4bd7d075001a4285ed': '', '5ad22b4bd7d075001a4285ee': '', '5ad22b4bd7d075001a4285ef': '', '570a6e2f4103511400d596f8': 'Michael C. Graham', '570a6e2f4103511400d596f9': 'Psychotherapist', '570a6e2f4103511400d596fc': 'Moods', '5ad24351d7d075001a4289e0': '', '5ad24351d7d075001a4289e2': '', '5ad24351d7d075001a4289e3': '', '570f40f65ab6b81900390eb3': 'Melatonin', '570f40f65ab6b81900390eb6': 'hormone', '5a223ceb819328001af38a47': '', '5a223ceb819328001af38a48': '', '5a223cec819328001af38a49': '', '5a223cec819328001af38a4a': '', '572a630e7a1753140016aefb': 'George H. W. Bush', '572a630e7a1753140016aefc': 'Freiburg, Germany', '572a630e7a1753140016aefe': 'American Economic Review', '5726a4465951b619008f78bf': '2007', '5726a4465951b619008f78c0': '21', '5726a4465951b619008f78c1': '11', '5726a4465951b619008f78c2': 'Pachuca', '5726f3eef1498d1400e8f0c6': 'Sumer', '5726f3eef1498d1400e8f0c8': 'numerical data', '5726f3eef1498d1400e8f0c9': '18th century BC', '5726f3eef1498d1400e8f0ca': 'Plimpton 322', '572fa91e04bcaa1900d76b69': 'light-gathering membrane', '5731e810e17f3d1400422535': 'indigenous, Mediterranean, African and Western', '5731e810e17f3d1400422536': 'Hathor', '5731e810e17f3d1400422538': 'Amr Diab and Mohamed Mounir', '56e7b14c37bdd419002c4371': '2014', '570e58ee0dc6ce1900204f75': 'William Pitt', '570e58ee0dc6ce1900204f76': \"Charles D'Ebro and Richard Speight\", '570e58ee0dc6ce1900204f77': 'Eureka Tower', '570e58ee0dc6ce1900204f78': '2006', '570e58ee0dc6ce1900204f79': 'second largest', '5727e243ff5b5019007d977a': 'New Orleans Hornets', '5727e243ff5b5019007d977b': \"Oklahoma City's Ford Center\", '5727e243ff5b5019007d977c': 'Hurricane Katrina', '5727e243ff5b5019007d977e': 'Professional Basketball Club LLC', '56cff6f3234ae51400d9c191': 'Karol Szymanowski', '56cff6f3234ae51400d9c192': 'national modes and idioms', '56cff6f3234ae51400d9c193': 'Nikolai Zverev', '56d3a9282ccc5a1400d82dc8': 'Karol Szymanowski', '56d3a9282ccc5a1400d82dc9': 'Alexander Scriabin', '56d3a9282ccc5a1400d82dca': 'Nikolai Zverev', '57296ab01d046914007793e7': 'W. Klement', '57296ab01d046914007793eb': 'Caltech', '5a67151af038b7001ab0c1c7': '', '5a67151af038b7001ab0c1c8': '', '5a67151af038b7001ab0c1c9': '', '5726994b708984140094cb4b': 'Paul Gauguin and Paul Cézanne', '5726994b708984140094cb4c': 'The Demoiselles', '5acfc92477cf76001a685f86': '', '5acfc92477cf76001a685f87': '', '5acfc92477cf76001a685f89': '', '56dcf8689a695914005b94a0': 'Sassou', '56dcf8689a695914005b94a2': 'very low', '5ad00a9d77cf76001a6867e0': '', '5ad00a9d77cf76001a6867e1': '', '5ad00a9d77cf76001a6867e4': '', '56f78981aef2371900625ba9': '1874', '56f78981aef2371900625bad': '1986', '56f952ba9b226e1400dd1315': '1919', '56f952ba9b226e1400dd1316': '1986', '56cfebbd234ae51400d9c0c9': 'Solar heating, cooling and ventilation technologies', '5728bf3a2ca10214002da6d2': 'Île de la Cité', '5728bf3a2ca10214002da6d3': 'Maurice de Sully', '5728bf3a2ca10214002da6d4': 'The Left Bank', '5728bf3a2ca10214002da6d5': 'Louis VII', '5a864143b4e223001a8e750a': '', '5a864143b4e223001a8e750b': '', '5a864143b4e223001a8e750c': '', '5a864143b4e223001a8e750d': '', '572808cd2ca10214002d9c1e': '3 June 2013', '572808cd2ca10214002d9c1f': 'Pope Francis', '572808cd2ca10214002d9c20': 'Bergamo', '572808cd2ca10214002d9c21': '5 July 2013', '5a61447ae9e1cc001a33d030': '', '5a61447ae9e1cc001a33d031': '', '5a61447ae9e1cc001a33d032': '', '56dfbe7c231d4119001abd70': 'medium-to-large businesses, or other ISPs', '56dfbe7c231d4119001abd73': 'synchronous optical networking', '5a10d9ce06e79900185c341f': '', '572e8519c246551400ce42b0': 'Sir William Armstrong', '572e8519c246551400ce42b2': 'they provided a much greater force', '572e8519c246551400ce42b4': 'Counterweights and balances', '572fa79aa23a5019007fc839': '1992, 2000, 2004, and 2016', '572fa79aa23a5019007fc83a': 'Washington University Athletic Complex', '572fa79aa23a5019007fc83b': 'scheduling difficulties between the candidates', '572fa79aa23a5019007fc83d': 'Republican Sarah Palin and Democrat Joe Biden', '5ace1b9532bba1001ae49adc': '', '5ace1b9532bba1001ae49add': '', '5ace1b9532bba1001ae49ade': '', '5725eaf7271a42140099d319': 'radiant luminous efficacy', '5725eaf7271a42140099d31a': '683 lm/W', '5725eaf7271a42140099d31c': 'the ratio of the visible light flux emitted (the luminous flux) to the total power radiated over all wavelengths', '5725eaf7271a42140099d31d': 'source luminous efficacy', '5ad18d4e645df0001a2d1f08': '', '5ad18d4e645df0001a2d1f09': '', '5ad18d4e645df0001a2d1f0a': '', '5ad18d4e645df0001a2d1f0b': '', '5ad18d4e645df0001a2d1f0c': '', '5a0cc44cf5590b0018dab54c': '', '5a0cc44cf5590b0018dab54d': '', '5a0cc44cf5590b0018dab54e': '', '5a0cc44cf5590b0018dab54f': '', '5a0cc44cf5590b0018dab550': '', '572734eb5951b619008f86b7': '1922–23', '572734eb5951b619008f86b8': 'Empire Stadium', '5a8c802dfd22b3001a8d8955': '', '5a8c802dfd22b3001a8d8956': '', '5a8c802dfd22b3001a8d8957': '', '5a8c802dfd22b3001a8d8958': '', '5735c0d8e853931400426b4b': 'three', '572830622ca10214002da032': 'Charles Darwin', '572830622ca10214002da033': 'The Formation of Vegetable Mould through the Action of Worms', '572830622ca10214002da035': 'oxygen and water can penetrate it', '5ace8a7232bba1001ae4a9d9': '', '5ace8a7232bba1001ae4a9da': '', '5ace8a7232bba1001ae4a9dc': '', '5ace8a7232bba1001ae4a9dd': '', '56f8d4209b226e1400dd10ad': 'France', '56f8d4209b226e1400dd10ae': 'Switzerland', '570ac16f4103511400d5998e': 'a catapult or JATO rocket', '570ac16f4103511400d5998f': 'would eliminate one or more helicopter landing areas', '570ac16f4103511400d59990': 'with a minimal armament and fuel load', '5acd84c307355d001abf453d': '', '572901d03f37b31900477f74': '1960s', '572901d03f37b31900477f75': 'Hector Guimard', '572901d03f37b31900477f76': '1913', '56cdd83862d2951400fa68e1': 'SPECTRE', '56cdd83862d2951400fa68e2': 'Eon Productions', '56cf39c4aab44d1400b88ebc': 'Danjaq', '5ad22870d7d075001a428558': '', '5ad22870d7d075001a428559': '', '5ad22870d7d075001a42855a': '', '57267c12dd62a815002e86c2': 'William Gladstone', '57267c12dd62a815002e86c4': '1916', '5731edafe17f3d1400422559': 'Amsterdam', '5731edafe17f3d140042255a': 'John Smyth', '5731edafe17f3d140042255b': 'General Baptists', '5731edafe17f3d140042255c': 'Roger Williams', '5731edafe17f3d140042255d': 'Baptist missionaries', '57321a39e99e3014001e651d': 'Pouākai', '57321a39e99e3014001e651e': 'National Audubon Society', '57321a39e99e3014001e651f': 'nightingales', '5731f0ffe99e3014001e63ec': 'devotio', '5731f0ffe99e3014001e63ee': 'Roman gladiator', '57324b39b9d445190005e9d4': 'a skewer of marinated roasted meat and vegetables', '5a53f1c6bdaabd001a386815': '', '5a53f1c6bdaabd001a386816': '', '5727f9a14b864d190016410f': '33 1⁄3 rpm', '572a31481d04691400779830': 'seven', '5a110ff906e79900185c3516': '', '5a110ff906e79900185c3517': '', '5a110ff906e79900185c3518': '', '56e4731e8c00841900fbaf95': 'string courses or rustication', '5acf9cb577cf76001a6854dc': '', '5acf9cb577cf76001a6854dd': '', '5acf9cb577cf76001a6854de': '', '572714d2708984140094d975': 'Cancer', '572714d2708984140094d976': 'International Agency for Research on Cancer', '572714d2708984140094d978': 'tobacco', '572714d2708984140094d979': 'Western', '5705e3dd75f01819005e76fa': 'George V', '5705e3dd75f01819005e76fb': '12 December 1911', '5705e3dd75f01819005e76fc': 'King George V and Queen Mary', '5705e3dd75f01819005e76fd': 'Edwin Lutyens', '5705e3dd75f01819005e76fe': '10 February 1931', '5725ee0a38643c19005acea9': '6,300 °C', '5725ee0a38643c19005aceaa': '52 lumens per watt', '5725ee0a38643c19005acead': '52 lumens per watt', '5ad18e11645df0001a2d1f12': '', '5ad18e11645df0001a2d1f13': '', '5ad18e11645df0001a2d1f14': '', '5ad18e11645df0001a2d1f15': '', '5ad18e11645df0001a2d1f16': '', '5a0cc174f5590b0018dab543': '', '56e6f6e0de9d371400068101': 'Christian AC', '56e6f6e0de9d371400068102': 'Radio & Records', '56e6f6e0de9d371400068103': 'hot AC', '5731c5ba0fdd8d15006c651d': '2005', '5731c5ba0fdd8d15006c651e': 'two', '5731c5ba0fdd8d15006c651f': 'Geoff Zanelli', '5731c5ba0fdd8d15006c6520': '2010', '5731c5ba0fdd8d15006c6521': '$250 million', '5ad4ccbb5b96ef001a10a106': '', '5ad4ccbb5b96ef001a10a107': '', '5ad4ccbb5b96ef001a10a108': '', '5ad4ccbb5b96ef001a10a109': '', '5ad4ccbb5b96ef001a10a10a': '', '57293d583f37b3190047816d': '1,360 GW', '57293d583f37b3190047816e': '19 percent', '57293d583f37b3190047816f': '22 percent', '5ad1248d645df0001a2d0f1e': '', '5ad1248d645df0001a2d0f1f': '', '5ad1248d645df0001a2d0f20': '', '5706aab252bb891400689b3a': 'Detroit techno music', '5706aab252bb891400689b3b': 'the exclusive association of particular tracks with particular clubs and DJs', '5729a3d56aef051400155074': 'delicacies', '5729a3d56aef051400155075': 'cicadas', '5729a3d56aef051400155076': 'high', '5729a3d56aef051400155077': 'entomophagy', '5ad1171b645df0001a2d0d28': '', '5ad1171b645df0001a2d0d29': '', '5ad1171b645df0001a2d0d2a': '', '5ad1171b645df0001a2d0d2b': '', '5ad1171b645df0001a2d0d2c': '', '56e6d988de9d371400068085': 'urban', '56e6d988de9d371400068087': 'physical record', '56d2124ce7d4791d0090263d': 'the yogis', '56d2124ce7d4791d0090263e': 'mindfulness and clear awareness', '5729667c3f37b3190047833d': 'Art Nouveau', '5729667c3f37b3190047833e': 'Teatro Massimo', '5a3ea39b5a76c5001a3a83f1': '', '5a3ea39b5a76c5001a3a83f2': '', '5a3ea39b5a76c5001a3a83f4': '', '572e7aa8dfa6aa1500f8d00d': 'Canadian', '572e7aa8dfa6aa1500f8d00e': '10', '572e7aa8dfa6aa1500f8d010': 'three', '572e7aa8dfa6aa1500f8d011': '1 yard', '5a0dcbf06e16420018587b3e': '', '5a0f39cfdecec900184754eb': '', '5a0f39cfdecec900184754ec': '', '5a0f39cfdecec900184754ef': '', '572f39d804bcaa1900d7679d': 'distribution of seeds', '572f39d804bcaa1900d7679e': 'Fruit', '572f39d804bcaa1900d767a1': 'flowering', '5a3ad7933ff257001ab842b3': '', '5a3ad7933ff257001ab842b5': '', '5a3ad7933ff257001ab842b6': '', '5a3ad7933ff257001ab842b7': '', '56df84d756340a1900b29cd2': 'Sarah Fuller', '56d3805859d6e41400146581': 'Randy Jackson', '56d3805859d6e41400146582': 'one', '56db2e2de7c41114004b4eeb': 'Randy Jackson', '56db2e2de7c41114004b4eec': 'Mariah Carey and Nicki Minaj', '572a3fa3af94a219006aa90d': 'University of Vienna', '572a3fa3af94a219006aa90e': \"Constantin von Monakow's Institute of Brain Anatomy\", '572a3fa3af94a219006aa90f': 'The Sensory Order', '572a3fa3af94a219006aa910': 'the Geistkreis', '572a3fa3af94a219006aa911': 'Herbert Furth', '5726c1825951b619008f7d51': 'marine organisms', '572931113f37b319004780cc': 'the floor of the Atlantic, and the Mid-Atlantic Ridge', '5ad3e9c6604f3c001a3ff6a3': '', '5ad3e9c6604f3c001a3ff6a5': '', '57279543f1498d1400e8fcc6': 'three', '57279543f1498d1400e8fcc9': 'a constitutional initiative and a referendum', '57279543f1498d1400e8fcca': 'Direct democracy and federalism', '5730187d947a6a140053d0d3': 'Catalina Foothills', '573428154776f419006619b5': 'Catalina Foothills', '573428154776f419006619b8': 'La Encantada', '573428154776f419006619b9': 'Hacienda Del Sol, Westin La Paloma Resort, Loews Ventana Canyon Resort and Canyon Ranch Resort', '56dde98d9a695914005b96aa': 'French kings', '5ad2e74a604f3c001a3fd95d': '', '5ad2e74a604f3c001a3fd95e': '', '5ad2e74a604f3c001a3fd95f': '', '5ad2e74a604f3c001a3fd960': '', '572a18b96aef051400155266': 'a growing papyrus sprout', '572a18b96aef051400155267': 'Osiris', '572a18b96aef051400155268': 'to protect them from evil', '5a74e30342eae6001a389b0e': '', '5a74e30342eae6001a389b0f': '', '56d0875b234ae51400d9c348': 'convert solar light to heat', '56d0875b234ae51400d9c34b': 'Europe', '572eb9a703f98919007569a5': 'Abdus Salam', '572eb9a703f98919007569a6': '67:3-4', '5ad2292fd7d075001a42857d': '', '5ad2292fd7d075001a42857e': '', '5727c25f2ca10214002d9594': 'politicians, judges and academics', '5a3b0aab3ff257001ab843b7': '', '5a3b0aab3ff257001ab843b8': '', '5a3b0aab3ff257001ab843b9': '', '5a3b0aab3ff257001ab843ba': '', '5a3b0aab3ff257001ab843bb': '', '56e10245e3433e1400422a96': '2 November 2006', '56e10245e3433e1400422a97': '10 meters', '5acd35ea07355d001abf3964': '', '5acd35ea07355d001abf3965': '', '5acd35ea07355d001abf3966': '', '5acd35ea07355d001abf3967': '', '5acd35ea07355d001abf3968': '', '5728d9e23acd2414000e002f': 'March 10, 2004', '5728d9e23acd2414000e0030': 'September 2, 2004', '5728d9e23acd2414000e0031': 'Dick Cheney', '5728d9e23acd2414000e0033': 'Conservative Party', '5732a7f71d5d2e14009ff87b': 'Eocene', '5732a7f71d5d2e14009ff87c': 'huge lakes', '5732a7f71d5d2e14009ff87d': 'Tethys Sea', '5732a7f71d5d2e14009ff87e': 'Tethys Sea', '5a4ebb46af0d07001ae8cc0b': '', '5a4ebb46af0d07001ae8cc0d': '', '5a4ebb46af0d07001ae8cc0e': '', '56de94c84396321400ee2a41': 'Conan the Republican', '56e8648f37bdd419002c44ea': 'River Aare', '5728ddb1ff5b5019007da884': 'Anti-communism', '5a6a5f32a9e0c9001a4e9db9': '', '572b85aef75d5e190021fe21': 'phytates', '5acfcdfc77cf76001a6860cc': '', '5acfcdfc77cf76001a6860ce': '', '5acfcdfc77cf76001a6860cf': '', '570a5afd6d058f1900182d96': 'Richard Lazarus', '570a5afd6d058f1900182d97': 'judgments', '5ad24963d7d075001a428b9e': '', '5ad24963d7d075001a428b9f': '', '5ad24963d7d075001a428ba0': '', '5733a3cbd058e614000b5f3f': 'The College of Arts and Letters', '5733a3cbd058e614000b5f40': '1842', '5733a3cbd058e614000b5f41': '1849', '5733a3cbd058e614000b5f42': 'Saint Louis University', '5733a3cbd058e614000b5f43': '33', '572acbfe111d821400f38d7e': 'Toruń', '572acbfe111d821400f38d80': '1976', '572acbfe111d821400f38d82': 'ten', '5726ba315951b619008f7c03': 'tin', '5726ba315951b619008f7c05': 'tin', '5726ba315951b619008f7c06': 'alloyed with other metals', '5a20a62e8a6e4f001aa08e02': '', '5a20a62e8a6e4f001aa08e03': '', '5a20a62e8a6e4f001aa08e04': '', '5a20a62e8a6e4f001aa08e05': '', '57277d20f1498d1400e8f990': 'manufacturing', '57277d20f1498d1400e8f992': '56%', '573040dd947a6a140053d33d': 'British war economy', '573040dd947a6a140053d33e': 'British coastal centres and shipping at sea west of Ireland', '56cee43eaab44d1400b88c06': 'Manhattan', '56cee43eaab44d1400b88c07': 'Alexander Hamilton', '56cee43eaab44d1400b88c08': '1827', '56cee43eaab44d1400b88c09': '16,000', '56cfbb5f234ae51400d9bf2b': '1799', '56cfbb5f234ae51400d9bf2c': '1827', '57315327e6313a140071ce21': 'Alexandria', '57315327e6313a140071ce23': 'Egyptian', '5729e5501d0469140077965d': 'kinetic', '5acd166907355d001abf3418': '', '5acd166907355d001abf3419': '', '5acd166907355d001abf341b': '', '5acd166907355d001abf341c': '', '56e1688fe3433e1400422ebc': 'Marty Walsh', '5732696fe17f3d140042295f': 'refusal of the Russians to permit any sort of inspections', '5732696fe17f3d1400422960': 'London', '5732696fe17f3d1400422962': 'Austria', '5732696fe17f3d1400422963': 'Open Skies', '56f81fe6aef2371900625df5': 'The altitude and size of the range', '56f81fe6aef2371900625df6': 'ibex', '56f81fe6aef2371900625df7': 'Edelweiss', '5acf5b6977cf76001a684c1c': '', '5ad418a2604f3c001a400415': '', '5ad418a2604f3c001a400416': '', '5ad418a2604f3c001a400417': '', '5ad418a2604f3c001a400418': '', '573149e7e6313a140071cdcf': 'Pulse oximeters', '5ad1981f645df0001a2d20e0': '', '5ad1981f645df0001a2d20e1': '', '5a70dbb38abb0b001a6761c5': '', '5a70dbb38abb0b001a6761c6': '', '5a70dbb38abb0b001a6761c7': '', '5a70dbb38abb0b001a6761c8': '', '5a70dbb38abb0b001a6761c9': '', '572781a5f1498d1400e8fa1c': 'council-manager', '572781a5f1498d1400e8fa1d': '11', '572781a5f1498d1400e8fa1e': 'two-year', '5ace433e32bba1001ae4a116': '', '5ace433e32bba1001ae4a117': '', '5727d0914b864d1900163dc0': 'humid subtropical', '5727d0914b864d1900163dc1': 'Tornado Alley', '570b4c3c6b8089140040f861': 'Islamic Extremist', '570b4c3c6b8089140040f864': 'Arkansas and Texas', '5ad1787c645df0001a2d1d70': '', '5ad1787c645df0001a2d1d73': '', '5726ec71708984140094d635': 'December 2011', '5726ec71708984140094d636': 'Kim Jong Un', '5726ec71708984140094d637': 'Kim Jong Un', '57303157b2c2fd1400568a37': 'Maximum power transfer', '57303157b2c2fd1400568a38': '50 ohms', '57303157b2c2fd1400568a3a': 'standing wave ratio', '572810aa3acd2414000df393': 'Sodor and Man Diocesan Synod', '572810aa3acd2414000df395': 'Order in Council', '572810aa3acd2414000df396': 'lieutenant governor', '5acfb9fb77cf76001a685ab2': '', '5acfb9fb77cf76001a685ab5': '', '56ddc1d966d3e219004dacd4': 'Sun Open Storage', '56ddc1d966d3e219004dacd5': \"Sun Microsystems' California campus\", '5a6b0974a9e0c9001a4e9e88': '', '5a6b0974a9e0c9001a4e9e8b': '', '5a82062531013a001a335101': '', '5a82062531013a001a335103': '', '5a82062531013a001a335104': '', '5a82062531013a001a335105': '', '5731f24bb9d445190005e6d3': 'Publius Claudius Pulcher', '570e70c30b85d914000d7eff': 'Melbourne', '570e70c30b85d914000d7f00': 'Victoria', '5725c61c38643c19005acc9b': 'Energy', '5725c61c38643c19005acc9c': 'telecommunications', '570c9048b3d812140066d227': '196', '570c9048b3d812140066d228': 'La Liga', '570c9048b3d812140066d229': 'June 1950', '570c9048b3d812140066d22a': '1954', '570c9048b3d812140066d22b': 'Archbishop of Barcelona Gregorio Modrego', '572600a9ec44d21400f3d7f7': 'Athens', '572600a9ec44d21400f3d7f8': 'Library of Alexandria', '572600a9ec44d21400f3d7f9': 'Pergamon', '572600a9ec44d21400f3d7fa': 'Rhodes', '572600a9ec44d21400f3d7fb': 'Seleucia', '56d4647c2ccc5a1400d83130': '2007', '56d4647c2ccc5a1400d83131': 'Chicago', '56d4647c2ccc5a1400d83134': 'August', '56de9a164396321400ee2a45': '55.4%', '56de9a164396321400ee2a47': '1.3 million', '56de9a164396321400ee2a48': '31%', '56de9a164396321400ee2a49': 'John G. Downey', '57279d0aff5b5019007d9109': 'Treaty of Versailles', '5a6fdd4c8abb0b001a675fd4': '', '5a6fdd4c8abb0b001a675fd5': '', '5a6fdd4c8abb0b001a675fd6': '', '570b421bec8fbc190045b925': '1860', '570b421bec8fbc190045b927': 'South Carolina', '570b421bec8fbc190045b928': 'April 12, 1861', '5ad17172645df0001a2d1bc8': '', '5ad17172645df0001a2d1bc9': '', '5ad17172645df0001a2d1bca': '', '570ba2b3ec8fbc190045baa1': 'protostars', '5a07f8453fc87400182070cb': '', '5a07f8453fc87400182070cd': '', '5a07f8453fc87400182070ce': '', '5a07f8453fc87400182070cf': '', '570b2dd76b8089140040f7d5': '1750', '570b2dd76b8089140040f7d6': 'Gregorian calendar', '570b2dd76b8089140040f7d8': 'Calendar Act of 1750', '5a37143595360f001af1b3eb': '', '5a37143595360f001af1b3ec': '', '5a37143595360f001af1b3ed': '', '5a37143595360f001af1b3ee': '', '5726b597f1498d1400e8e84f': 'belles-lettres', '5a7a34d717ab25001a8a03a3': '', '5a7a34d717ab25001a8a03a6': '', '5a7cfda9e8bc7e001a9e2105': '', '5a7cfda9e8bc7e001a9e2106': '', '5a7cfda9e8bc7e001a9e2107': '', '5a7d25d970df9f001a874fe2': '', '57268e59708984140094c9f5': 'Homo erectus', '5726f6d3708984140094d731': '$250 million', '5726f6d3708984140094d732': '$70 million', '5726f6d3708984140094d733': 'Richard Gilder', '5ad3ece8604f3c001a3ff78b': '', '5ad3ece8604f3c001a3ff78c': '', '5ad3ece8604f3c001a3ff78d': '', '5ad3ece8604f3c001a3ff78f': '', '57262d20271a42140099d703': 'carbon', '5ad19bb3645df0001a2d2112': '', '5ad19bb3645df0001a2d2113': '', '5ad19bb3645df0001a2d2114': '', '5ad19bb3645df0001a2d2115': '', '5ad19bb3645df0001a2d2116': '', '56d09354234ae51400d9c3ab': 'increasing availability, economy, and utility of coal and petroleum', '572efc3503f9891900756b13': 'two', '572efc3503f9891900756b17': 'Darwin', '5a3acb0f3ff257001ab8428c': '', '5a3acb0f3ff257001ab8428d': '', '5a3acb0f3ff257001ab8428f': '', '56dedc703277331400b4d775': '1992', '570888bf9928a814004714da': 'blue', '570888bf9928a814004714dc': 'grey', '570888bf9928a814004714dd': 'navy blue', '570888bf9928a814004714de': 'home matches', '59fb256bee36d60018400d47': '', '59fb256bee36d60018400d48': '', '59fb256bee36d60018400d4a': '', '57324714b9d445190005e98f': 'executive officer', '57324714b9d445190005e990': 'Panama Canal Zone', '57324714b9d445190005e991': 'On War', '57324714b9d445190005e992': '1925–26', '57324714b9d445190005e993': '245', '572931841d04691400779145': 'uncertain', '572931841d04691400779147': '20,000', '572931841d04691400779149': 'undescribed', '572a016b3f37b3190047863d': 'Energy transfer', '572a016b3f37b3190047863e': 'heat', '5acd5d9e07355d001abf3f00': '', '5acd5d9e07355d001abf3f01': '', '5acd5d9e07355d001abf3f02': '', '5706be1e0eeca41400aa0de7': '$26,969', '5706be1e0eeca41400aa0de8': '$31,997', '5706be1e0eeca41400aa0de9': '$15,402', '5706be1e0eeca41400aa0dea': '19.1%', '5706be1e0eeca41400aa0deb': '23.6%', '5727bb132ca10214002d94f4': 'Santa Hermandad', '5727bb132ca10214002d94f5': 'Real Audiencia del Reino de Galicia', '5727bb132ca10214002d94f6': '10%', '5727bb2a3acd2414000deacf': 'sardines, wood, and some cattle and wine', '572ffe9ea23a5019007fcc1f': 'irrigation and farming', '572ffe9ea23a5019007fcc20': 'almonds and citrus fruit', '572ffe9ea23a5019007fcc21': 'Arab merchants', '572ffe9ea23a5019007fcc22': '16th century', '572ffe9ea23a5019007fcc23': 'Hormuz', '5a5447e0134fea001a0e16ec': '', '5a5447e0134fea001a0e16ee': '', '56cf68594df3c31400b0d73d': 'Graduation', '56cf68594df3c31400b0d73e': '50 Cent', '56cf68594df3c31400b0d73f': '957,000', '56cf68594df3c31400b0d740': 'Daft Punk', '56d1091117492d1400aab7bc': 'Graduation', '56d1091117492d1400aab7bd': '50 Cent', '56d1091117492d1400aab7bf': 'Daft Punk', '57303b58947a6a140053d2e0': 'Mediterranean Sea', '57303b58947a6a140053d2e2': 'Caesar', '57303b58947a6a140053d2e3': 'to meet several new demands', '56f865baaef237190062603f': 'Victorian era', '56f865baaef2371900626040': '1835', '56f865baaef2371900626041': 'October', '56f865baaef2371900626042': '1840', '56f865baaef2371900626043': 'The Gateway to the Empire', '5719c6a94faf5e1900b8a7eb': 'New York', '5725f7a389a1e219009ac114': '15 September 1940', '5725f7a389a1e219009ac115': 'the Palace', '5725f7a389a1e219009ac117': \"King's Messenger\", '5725f7a389a1e219009ac118': '2005', '5726245b89a1e219009ac2f2': '2005', '5a7a513117ab25001a8a04ec': '', '5a7a513117ab25001a8a04ee': '', '56d09a0e234ae51400d9c3c2': '2060', '56d8e18ddc89441400fdb38a': 'Reporters Without Borders', '56db1ba8e7c41114004b4d3a': 'Reporters Without Borders', '57305ed58ab72b1400f9c4aa': 'Proto-Indo-European pantheon', '57305ed58ab72b1400f9c4ac': 'each household', '5a43023e4a4859001aac73e5': '', '5a43023e4a4859001aac73e6': '', '5a43023e4a4859001aac73e8': '', '5a43023e4a4859001aac73e9': '', '57304e618ab72b1400f9c414': 'halves', '57304e618ab72b1400f9c416': '1891', '5a79e4bf17ab25001a8a0182': '', '5a79e4bf17ab25001a8a0183': '', '5a79e4bf17ab25001a8a0184': '', '5a79e4bf17ab25001a8a0185': '', '5a79e4bf17ab25001a8a0186': '', '56f9ebe18f12f3190062fff9': \"International Telecommunication Union's radio telecommunications sector\", '56f9ebe18f12f3190062fffa': 'Digital Video Broadcasting', '56f9ebe18f12f3190062fffd': 'ETSI', '5ad3b2f6604f3c001a3fed2f': '', '5ad3b2f6604f3c001a3fed30': '', '572ea993cb0c0d14000f1418': \"Sana'a, Yemen\", '5ad2183dd7d075001a4283f4': '', '5ad2183dd7d075001a4283f5': '', '5ad2183dd7d075001a4283f7': '', '5ad2183dd7d075001a4283f8': '', '571a8f6a4faf5e1900b8aa7a': '40%', '571a8f6a4faf5e1900b8aa7c': 'founder lineages', '571a8f6a4faf5e1900b8aa7d': 'Hebrew/Levantine', '56de2fa0cffd8e1900b4b63e': 'July 24, 2014', '5ad0ccaa645df0001a2d03ac': '', '5ad0ccaa645df0001a2d03ad': '', '5ad0ccaa645df0001a2d03ae': '', '56e7a21637bdd419002c42a3': 'Jeffrey Vinik', '56e7a21637bdd419002c42a4': 'Hurricane Sandy', '572fe399a23a5019007fcae1': 'antistatic bags', '572fe399a23a5019007fcae2': 'earthed', '572fe399a23a5019007fcae5': 'MCMs', '5ace846a32bba1001ae4a941': '', '5ace846a32bba1001ae4a943': '', '5ace846a32bba1001ae4a944': '', '57318a0ae6313a140071d06c': 'UN Human Rights Council', '57318a0ae6313a140071d06d': 'March', '57318a0ae6313a140071d06e': 'hundreds', '57318a0ae6313a140071d06f': 'United Arab Emirates', '56e039947aa994140058e3d5': 'manhua', '5acf95a777cf76001a685380': '', '5acf95a777cf76001a685381': '', '5acf95a777cf76001a685382': '', '5acf95a777cf76001a685383': '', '5acf95a777cf76001a685384': '', '572644761125e71900ae191e': 'Corsica', '572644761125e71900ae191f': '1793', '572644761125e71900ae1920': '26', '572644761125e71900ae1921': 'the Austrians and their Italian allies', '572644761125e71900ae1922': '1798', '572817584b864d1900164463': 'Inner London and Outer London', '572817584b864d1900164465': 'North and South', '572817584b864d1900164466': 'London commuter belt', '56e14c63e3433e1400422d74': 'Tourism', '56e14c63e3433e1400422d75': '21.2 million', '56e14c63e3433e1400422d76': '$8.3 billion', '56e14c63e3433e1400422d77': '2014', '5727b1f42ca10214002d941c': '34.7%', '5727b1f42ca10214002d941d': 'one fifth', '572fce07b2c2fd140056848b': 'Sebastián Vizcaíno', '572fce07b2c2fd140056848c': 'San Salvador', '572fce07b2c2fd140056848e': 'Friar Antonio de la Ascensión', '5ad4a7fc5b96ef001a109d14': '', '5ad4a7fc5b96ef001a109d15': '', '5ad4a7fc5b96ef001a109d16': '', '5ad4a7fc5b96ef001a109d17': '', '5ad4a7fc5b96ef001a109d18': '', '5727b2e53acd2414000dea15': 'reactivation', '5727b2e53acd2414000dea16': 'HIV', '5727b2e53acd2414000dea17': '8%', '5a871df11d3cee001a6a10e3': '', '5a871df11d3cee001a6a10e5': '', '572a2d163f37b3190047876d': '1,996,626 people', '572a2d163f37b3190047876e': 'Protestants', '572a2d163f37b3190047876f': 'Masurians, Kursenieki and Prussian Lithuanians', '5a3bf619cc5d22001a521c5a': '', '5a3bf619cc5d22001a521c5c': '', '5a3bf619cc5d22001a521c5d': '', '5a3bf619cc5d22001a521c5e': '', '56e11d89e3433e1400422c22': '16', '56e11d89e3433e1400422c23': '14', '5acd4eab07355d001abf3cac': '', '5acd4eab07355d001abf3cad': '', '5acd4eab07355d001abf3cae': '', '5acd4eab07355d001abf3caf': '', '5acd4eab07355d001abf3cb0': '', '5722d357f6b826140030fc66': '50', '5722d357f6b826140030fc69': 'his parentage', '5723e1e80dadf01500fa1f6d': '1887', '5723e1e80dadf01500fa1f6f': 'Sir Henry', '5724e8960ba9f01400d97bbd': '1887', '5724e8960ba9f01400d97bbe': 'Hindustani', '5724e8960ba9f01400d97bc1': '20 June', '572635ccec44d21400f3dc3f': '1887', '572635ccec44d21400f3dc41': 'Abdul Karim', '5ad17cf2645df0001a2d1e12': '', '5ad17cf2645df0001a2d1e14': '', '5ad17cf2645df0001a2d1e15': '', '57277778708984140094de56': 'Bible', '57277778708984140094de57': 'Mesopotamia and Egypt', '5a8c775dfd22b3001a8d8857': '', '5a8c775dfd22b3001a8d8858': '', '5a8c775dfd22b3001a8d8859': '', '5a8c775dfd22b3001a8d885a': '', '56ce6629aab44d1400b8874d': '2050', '56d00697234ae51400d9c297': '2050', '56f7f695aef2371900625cfc': 'the highest members of the nobility', '56f7f695aef2371900625cfd': 'Russian Empire', '572802332ca10214002d9b51': 'slip-cueing, beatmatching, and scratching', '572f7b31b2c2fd140056817e': 'North America', '572f7b31b2c2fd140056817f': 'Europe', '572f7b31b2c2fd1400568180': 'The Jazz Age', '5ad3c98e604f3c001a3ff0a1': '', '5ad3c98e604f3c001a3ff0a3': '', '56f9608b9b226e1400dd13df': '18', '56f9608b9b226e1400dd13e0': 'every four years', '56f9608b9b226e1400dd13e1': 'Nitijela', '56f9608b9b226e1400dd13e2': '1979', '570bd2ec6b8089140040fa6a': 'handshaking', '570bd2ec6b8089140040fa6b': 'Control-R (DC2) and Control-T (DC4)', '5a6513c9c2b11c001a425be1': '', '5a6513c9c2b11c001a425be2': '', '5a6513c9c2b11c001a425be4': '', '5a6513c9c2b11c001a425be5': '', '573039c004bcaa1900d773c7': 'cavalry troops', '5ad3d84a604f3c001a3ff377': '', '572785c8dd62a815002e9f6c': '2.55 million', '572842a0ff5b5019007da030': 'Zakaria Mohieddin', '572842a0ff5b5019007da033': 'diabetes', '5734465d879d6814001ca465': 'paleolithic hunting-gathering', '5734465d879d6814001ca466': 'the Hadza of Tanzania', '5735e8736c16ec1900b92888': 'European Age of Discovery', '5ace562032bba1001ae4a2ff': '', '5ace562032bba1001ae4a300': '', '5ace562032bba1001ae4a301': '', '5ace562032bba1001ae4a302': '', '5ace562032bba1001ae4a303': '', '5726275589a1e219009ac3ec': 'George Gaylord Simpson', '5726275589a1e219009ac3ed': '20th', '5726275589a1e219009ac3ee': 'cladistics', '5a39eb432f14dd001ac72643': '', '5a39eb432f14dd001ac72644': '', '5a39eb432f14dd001ac72645': '', '5a39eb432f14dd001ac72646': '', '56fc975cb53dbe1900755133': 'prosody', '5a82188631013a001a3351e6': '', '5a82188631013a001a3351e8': '', '5a82188631013a001a3351e9': '', '56fb91df8ddada1400cd64f8': 'customary law', '56fb91df8ddada1400cd64fa': 'Lithuania', '572eb63603f989190075697d': 'six', '5a848a6d7cf838001a46a8ec': '', '5a848a6d7cf838001a46a8ee': '', '5728cc17ff5b5019007da6e2': 'Bal Gangadhar Tilak', '572f7d6f04bcaa1900d76a19': '26.6 °C', '572f7d6f04bcaa1900d76a1b': 'March–June', '572f7d6f04bcaa1900d76a1d': 'May', '56e0ccb3231d4119001ac3b6': 'September 2008', '56e0ccb3231d4119001ac3b7': 'Internet Explorer', '5a4d38437a6c4c001a2bbc5c': '', '5a4d38437a6c4c001a2bbc5d': '', '5a4d38437a6c4c001a2bbc60': '', '5726e8eb708984140094d589': 'Aposematism', '5a6bdaf94eec6b001a80a5e8': '', '5a6bdaf94eec6b001a80a5ea': '', '5a6bdaf94eec6b001a80a5eb': '', '5a6bdaf94eec6b001a80a5ec': '', '572f5d5eb2c2fd1400568081': '1930s', '572f5d5eb2c2fd1400568082': 'Adolf Hitler', '56e8dca40b45c0140094cd21': '1535', '56e8dca40b45c0140094cd22': '£1,310,000 to £1,530,000', '56e8dca40b45c0140094cd23': 'Glastonbury Abbey', '5ad3f4c7604f3c001a3ff95a': '', '5ad3f4c7604f3c001a3ff95c': '', '5ad3f4c7604f3c001a3ff95d': '', '5ad496b5ba00c4001a268cb8': '', '5ad496b5ba00c4001a268cb9': '', '5ad496b5ba00c4001a268cba': '', '5ad496b5ba00c4001a268cbb': '', '5ad496b5ba00c4001a268cbc': '', '570e25b30dc6ce1900204dfc': 'pasta', '570e25b30dc6ce1900204dfd': 'honey', '5ad0d8bf645df0001a2d06a6': '', '5ad0d8bf645df0001a2d06a7': '', '5ad0d8bf645df0001a2d06a8': '', '5ad0d8bf645df0001a2d06a9': '', '5ad0d8bf645df0001a2d06aa': '', '570624f252bb8914006898f6': 'ASPEC', '570624f252bb8914006898f8': 'Layer 2', '570624f252bb8914006898f9': 'MP3', '5727ff0c2ca10214002d9ae4': 'Central Asian Turks', '5727ff0c2ca10214002d9ae5': 'late 14th century', '5727ff0c2ca10214002d9ae6': 'Sikhism', '572820842ca10214002d9e7f': '6 September 2012', '5a7a7cfa21c2de001afe9c7e': '', '5a7a7cfa21c2de001afe9c7f': '', '5a7a7cfa21c2de001afe9c80': '', '5a7a7cfa21c2de001afe9c81': '', '5a7a7cfa21c2de001afe9c82': '', '56cfe7f4234ae51400d9c05f': '16 November 1848', '56cfe7f4234ae51400d9c060': 'Guildhall', '5731225ca5e9cc1400cdbc78': '13,500 years ago', '5731bfaae17f3d140042238e': 'controversy over the Eucharist', '5731bfaae17f3d1400422390': 'bread and wine', '5731bfaae17f3d1400422391': 'Christ', '5727ff7c2ca10214002d9b00': '30 November 1934', '5727ff7c2ca10214002d9b01': 'Mesembria, Bulgaria', '5727ff7c2ca10214002d9b04': 'Pope Pius XI', '5a60fc3ae9e1cc001a33ce00': '', '5726909f5951b619008f76c4': '53%', '5726909f5951b619008f76c5': '2013', '56f89a0c9e9bad19000a01af': '31 BC', '56f89a0c9e9bad19000a01b1': 'Turnus', '5a7e576e70df9f001a87575d': '', '5a7e576e70df9f001a87575e': '', '5a7e576e70df9f001a87575f': '', '5a7e576e70df9f001a875760': '', '5a7e576e70df9f001a875761': '', '57305b0b069b5314008320a8': 'reproducing the original order of sememes', '57305b0b069b5314008320ab': 'syntactic requirements', '5a7e138a70df9f001a87547f': '', '5a7e138a70df9f001a875480': '', '5a7e138a70df9f001a875481': '', '5a7e138a70df9f001a875482': '', '5ad3586b604f3c001a3fde25': '', '5ad3586b604f3c001a3fde26': '', '5ad3586b604f3c001a3fde27': '', '5ad3586b604f3c001a3fde28': '', '5ad3586b604f3c001a3fde29': '', '5a6bb3444eec6b001a80a4e8': '', '5a6bb3444eec6b001a80a4ea': '', '5a6bb3444eec6b001a80a4eb': '', '5a6bb3444eec6b001a80a4ec': '', '5726f9d3f1498d1400e8f192': 'the Soviet Union', '5726f9d3f1498d1400e8f194': 'atomic warfare', '572792f3f1498d1400e8fc8c': 'December 1994', '572792f3f1498d1400e8fc8f': '2007', '572792f3f1498d1400e8fc90': 'Steve Earle', '5a826083e60761001a2eb21f': '', '5a826083e60761001a2eb220': '', '5a826083e60761001a2eb221': '', '5acf5f2877cf76001a684ca4': '', '5acf5f2877cf76001a684ca5': '', '5acf5f2877cf76001a684ca8': '', '56ce4100aab44d1400b88612': 'Tümen Khan', '56ce4100aab44d1400b88613': 'the great-grandson of Altan Khan', '56ce4100aab44d1400b88614': 'the 5th Dalai Lama', '57268ac8dd62a815002e88d9': '$390 billion', '57268ac8dd62a815002e88dc': '2020', '5734009a4776f41900661691': 'Punjabi', '5734009a4776f41900661693': 'Punjabis', '5734009a4776f41900661694': '89%', '5734009a4776f41900661695': 'south Punjab', '5a68fabf8476ee001a58a964': '', '5a68fabf8476ee001a58a966': '', '5a68fabf8476ee001a58a967': '', '5a68fabf8476ee001a58a968': '', '5727d9b5ff5b5019007d96d6': 'a mystical theologian', '5727d9b5ff5b5019007d96d7': 'Mysticism', '5727d9b5ff5b5019007d96d8': 'German mysticism', '5ad25af2d7d075001a428e5c': '', '5ad25af2d7d075001a428e5d': '', '5ad25af2d7d075001a428e5e': '', '5ad25af2d7d075001a428e5f': '', '5ad25af2d7d075001a428e60': '', '5728c7763acd2414000dfe35': 'cattle', '5728c7763acd2414000dfe36': '6000 BCE', '5728c7763acd2414000dfe37': 'Metal objects', '5728c7763acd2414000dfe38': 'due west', '5a611ac3e9e1cc001a33cf1d': '', '5a611ac3e9e1cc001a33cf1e': '', '5a611ac3e9e1cc001a33cf1f': '', '57344599acc1501500babd66': 'hunting', '57344599acc1501500babd67': 'dance and animal sacrifice', '5735e8246c16ec1900b92883': 'stories and myths', '5735e8246c16ec1900b92884': 'hunting hypothesis', '5ace521a32bba1001ae4a29f': '', '5ace521a32bba1001ae4a2a0': '', '5ace521a32bba1001ae4a2a1': '', '572b7afb34ae481900deae3d': 'apeirotheism', '572b7afb34ae481900deae3e': 'pluralistic', '5a7ca6d9e8bc7e001a9e1f47': '', '5a7ca6d9e8bc7e001a9e1f48': '', '5a7ca6d9e8bc7e001a9e1f4a': '', '5a7ca6d9e8bc7e001a9e1f4b': '', '56e1586ee3433e1400422de9': 'Gramophone', '56e1586ee3433e1400422dea': 'Symphony Hall', '56e1586ee3433e1400422deb': 'west of Back Bay', '56e1586ee3433e1400422dec': 'south of Boston Common', '572fa925947a6a140053cb1e': 'Tollywood', '572fa925947a6a140053cb1f': 'second largest', '5706a52352bb891400689b12': 'New Delhi', '5706a52352bb891400689b14': 'New Delhi', '572a34b83f37b319004787a9': 'sickle blades and grinding stones', '5a7d3e5c70df9f001a87503b': '', '5a7d3e5c70df9f001a87503c': '', '5727c7ad3acd2414000dec37': '$9 billion', '5727c7ad3acd2414000dec39': '$5.2 billion', '5727c7ad3acd2414000dec3a': '13', '5731c971e17f3d14004223e1': 'Huguenots', '5731c971e17f3d14004223e5': 'August 1572', '57329efbcc179a14009dab7c': '15,000', '57329efbcc179a14009dab7d': '1942', '57329efbcc179a14009dab7e': '19 February', '57329efbcc179a14009dab7f': 'Darwin', '57264e34f1498d1400e8db97': 'Tungsten', '5ad1acf3645df0001a2d21b4': '', '5ad1acf3645df0001a2d21b5': '', '5ad1acf3645df0001a2d21b6': '', '5ad1acf3645df0001a2d21b7': '', '570acf964103511400d59a26': 'Ichthyosaurs', '570acf964103511400d59a27': 'Mosasaurs', '570acf964103511400d59a28': 'Iguanodon', '5a2f4b02a83784001a7d26d9': '', '5a2f4b02a83784001a7d26db': '', '5a2f4b02a83784001a7d26dc': '', '5a2f4b02a83784001a7d26dd': '', '572c99202babe914003c299c': 'Republican and the Democratic', '572c99202babe914003c299e': 'yeoman', '572c99202babe914003c299f': 'tobacco and cotton', '56f7d6d8aef2371900625c2b': 'obscure', '56f7d6d8aef2371900625c2c': 'odwieczna', '56f7d6d8aef2371900625c2e': 'Alexander the Great', '56e788ab37bdd419002c40d3': 'February 25, 2010', '56e788ab37bdd419002c40d4': 'over 30 years', '5a7b6b7321c2de001afe9ffa': '', '5a7b6b7321c2de001afe9ffc': '', '5a7b6b7321c2de001afe9ffd': '', '5ad3fd89604f3c001a3ffbed': '', '5ad3fd89604f3c001a3ffbee': '', '57304d34069b531400832023': '150 years', '57304d34069b531400832024': 'K-8 parochial schools', '57304e4b069b531400832037': '150 years', '57304e4b069b531400832038': 'Bishop England High School', '57304e4b069b53140083203a': 'Ashley Hall', '5ad42525604f3c001a4008d6': '', '5ad42525604f3c001a4008d8': '', '57293b691d0469140077918d': '$244 billion', '5ad123aa645df0001a2d0ee8': '', '5ad123aa645df0001a2d0eea': '', '5ad123aa645df0001a2d0eeb': '', '5ad123aa645df0001a2d0eec': '', '56e1223ecd28a01900c67631': 'Boston Brahmins', '56e1223ecd28a01900c67632': 'Boston Brahmins', '56e1223ecd28a01900c67634': 'artistic patronage', '57266080f1498d1400e8ddac': 'the European continent', '57266080f1498d1400e8ddad': '12.7 million', '57266080f1498d1400e8ddae': 'United Kingdom', '57266080f1498d1400e8ddaf': 'Central Macedonia', '57266080f1498d1400e8ddb0': '6.5 million', '56de24b24396321400ee25fe': 'Thomas Hobbes', '56de24b24396321400ee25ff': 'Montesquieu', '56de24b24396321400ee2600': 'the framers of the United States Constitution', '56de33fc4396321400ee2695': 'Thomas Hobbes', '56de33fc4396321400ee2696': 'Montesquieu', '5ad37aaa604f3c001a3fe3ce': '', '5ad37aaa604f3c001a3fe3d0': '', '56cd7d3262d2951400fa6634': 'lithium-ion', '56d2706859d6e41400145fdc': 'Chinese', '56d2706859d6e41400145fdd': 'Lokakṣema', '56d2706859d6e41400145fde': 'Prajñāpāramitā', '56d2706859d6e41400145fdf': '1st', '5726b75add62a815002e8dd1': 'controlling members or a board of directors', '5726b75add62a815002e8dd3': 'to meet legal requirements for establishing a contract between the executive and the organization', '5a45597219a820001a1eda2c': '', '5a45597219a820001a1eda2d': '', '5a45597219a820001a1eda2e': '', '573056f1069b531400832085': 'China Central Television', '573056f1069b531400832086': 'Sina Weibo', '573056f1069b531400832088': 'Fudan University', '5ad4a204ba00c4001a268e88': '', '5ad4a204ba00c4001a268e89': '', '5ad4a204ba00c4001a268e8a': '', '5ad4a204ba00c4001a268e8b': '', '5ad4a204ba00c4001a268e8c': '', '5acfc11177cf76001a685cda': '', '5acfc11177cf76001a685cdb': '', '5acfc11177cf76001a685cdc': '', '56e02cb57aa994140058e2fb': 'Portuguese India carracks', '56e02cb57aa994140058e2fc': 'the Dutch', '56e02cb57aa994140058e2fd': 'The Portuguese and Spanish', '572a7a17be1ee31400cb802a': 'Port of Miami', '572a7a17be1ee31400cb802b': '2003', '572a7a17be1ee31400cb802c': 'Brickell Avenue', '572a7a17be1ee31400cb802d': 'South America', '5ad3903d604f3c001a3fe64c': '', '5ad3903d604f3c001a3fe64d': '', '572fe936b2c2fd14005685bf': 'resonance principle', '5727b0563acd2414000de9cb': 'Tang dynasty', '5727b0563acd2414000de9cc': 'Du Fu', '5727b0563acd2414000de9cd': 'armies', '5727b0563acd2414000de9ce': '755-763', '5a512d7dce860b001aa3fbed': '', '5a512d7dce860b001aa3fbee': '', '5a512d7dce860b001aa3fbef': '', '5a512d7dce860b001aa3fbf1': '', '5a68e9e48476ee001a58a880': '', '5a68e9e48476ee001a58a881': '', '57317177a5e9cc1400cdbf6d': 'Senussi', '57317177a5e9cc1400cdbf6f': 'none', '57317177a5e9cc1400cdbf70': 'execution', '57264c62dd62a815002e80d3': 'Rochester, England', '5728b6bbff5b5019007da53f': 'near-equal length', '5728b6bbff5b5019007da541': 'July', '56d0fe7217492d1400aab705': 'mid-1990s', '56d45f882ccc5a1400d830ef': 'mid-1990s', '56d45f882ccc5a1400d830f0': 'Deric \"D-Dot\" Angelettie', '56d45f882ccc5a1400d830f2': 'Hustle Period', '56d45f882ccc5a1400d830f3': '1999', '5acfc9ba77cf76001a685fb6': '', '5acfc9ba77cf76001a685fb8': '', '5728171b4b864d1900164452': '128', '5728171b4b864d1900164453': 'conjuncts', '5acd579107355d001abf3dec': '', '5acd579107355d001abf3ded': '', '5acd579107355d001abf3def': '', '5acd579107355d001abf3df0': '', '56fdee67761e401900d28c59': 'machine language', '57288d3b3acd2414000dfae7': 'Magnetic Lasso', '5acea46832bba1001ae4aead': '', '5acea46832bba1001ae4aeae': '', '5acea46832bba1001ae4aeb0': '', '5acea46832bba1001ae4aeb1': '', '56fdfa03761e401900d28c83': 'registers', '5731320605b4da19006bce88': 'the Wise', '5731320605b4da19006bce89': 'Vladimir the Great', '5731320605b4da19006bce8a': '1019', '5ad0124077cf76001a6868ae': '', '5ad0124077cf76001a6868af': '', '5ad0124077cf76001a6868b1': '', '5ad0124077cf76001a6868b2': '', '5727fb4eff5b5019007d99e2': 'communist', '5727fb4eff5b5019007d99e3': 'nine', '5a770f312d6d7f001a4a9f1d': '', '5a770f312d6d7f001a4a9f1e': '', '5a770f312d6d7f001a4a9f1f': '', '5a770f312d6d7f001a4a9f20': '', '5a770f312d6d7f001a4a9f21': '', '570c5037b3d812140066d0bd': 'Isabella of Angoulême', '570c5037b3d812140066d0be': 'downright mean', '56dee346c65bf219000b3ddf': 'functionalism, anomalous monism, identity theory', '5acd14cb07355d001abf33c0': '', '5acd14cb07355d001abf33c1': '', '5acd14cb07355d001abf33c2': '', '5acd14cb07355d001abf33c3': '', '5acd14cb07355d001abf33c4': '', '5727011b708984140094d845': 'Lord Raglan', '5727011b708984140094d846': 'The local commanders', '56e4cfd839bdeb14003479de': 'New Urbanism, Metaphoric architecture and New Classical Architecture', '5acfb2be77cf76001a68595c': '', '5acfb2be77cf76001a68595d': '', '5acfb2be77cf76001a68595e': '', '5acfb2be77cf76001a68595f': '', '57301a30b2c2fd140056886d': 'thousands', '57301a30b2c2fd140056886e': 'Santa Catalina Mountains', '57301a30b2c2fd140056886f': 'Oro Valley', '57301a30b2c2fd1400568870': 'Marana', '572823804b864d190016454c': \"St. John's\", '572823804b864d190016454d': 'Signal Hill', '572823804b864d190016454e': '1897', '572823804b864d1900164550': 'Guglielmo Marconi', '5a6280f6f8d794001af1c071': '', '5a6280f6f8d794001af1c072': '', '5a6280f6f8d794001af1c073': '', '56dd260266d3e219004dac12': 'Glorious Revolution', '56dd260266d3e219004dac13': 'Bill of Rights', '56dd260266d3e219004dac14': 'House of Commons', '5acfa94777cf76001a685794': '', '5acfa94777cf76001a685796': '', '56f72fe03d8e2e1400e37402': 'complexity', '572b6f49111d821400f38e9c': 'subjectivists', '572b6f49111d821400f38e9e': '1954', '572b6f49111d821400f38e9f': 'Foster', '5a7c7516e8bc7e001a9e1e23': '', '5a7c7516e8bc7e001a9e1e24': '', '5a7c7516e8bc7e001a9e1e26': '', '5a7c7516e8bc7e001a9e1e27': '', '57268c78708984140094c9bd': 'Max Clifford', '57268c78708984140094c9bf': 'Unwrapped', '56e0b2127aa994140058e6ad': 'Hydrogen', '56e0b2127aa994140058e6ae': 'deuterium and tritium', '56e0b2127aa994140058e6af': 'D and T', '5a10dee906e79900185c343c': '', '5a10dee906e79900185c343d': '', '5a10dee906e79900185c343e': '', '572ed3f503f9891900756a68': 'Sahih al-Bukhari', '572ed3f503f9891900756a69': 'Syriac', '572ed3f503f9891900756a6a': 'Mary', '5ad23431d7d075001a4287c4': '', '5ad23431d7d075001a4287c5': '', '5ad23431d7d075001a4287c6': '', '5ad23431d7d075001a4287c7': '', '5ad23431d7d075001a4287c8': '', '56f739203d8e2e1400e3749a': 'international law', '56f739203d8e2e1400e3749b': 'North Korea and the United States', '56f739203d8e2e1400e3749c': 'security guarantees and nuclear proliferation', '56d632371c85041400946fe0': 'less severe', '56d632371c85041400946fe1': '12.9', '56d632371c85041400946fe2': '60.7', '56d9e7e4dc89441400fdb902': 'Colorado', '56d9e7e4dc89441400fdb903': '60.7', '572f068ccb0c0d14000f1719': 'Safety Code for Elevators and Escalators', '572f068ccb0c0d14000f171a': 'CAN/CSA B44 Safety Standard', '572f068ccb0c0d14000f171b': 'ASME A17.2 Standard', '56ce6382aab44d1400b88732': '1872', '56d0007f234ae51400d9c243': 'Solar distillation', '56d0007f234ae51400d9c244': '16th-century Arab alchemists', '56d0007f234ae51400d9c245': '1872', '56d0007f234ae51400d9c246': '22,700 L (5,000 imp gal; 6,000 US gal) per day', '5acda46b07355d001abf48a3': '', '5acda46b07355d001abf48a4': '', '5acda46b07355d001abf48a5': '', '5acda46b07355d001abf48a6': '', '5733b2e14776f41900661085': 'Archaeology', '5ad2d6a8d7d075001a42a421': '', '5ad2d6a8d7d075001a42a422': '', '57318ae9e6313a140071d07e': 'head of state security', '57318ae9e6313a140071d080': 'Istanbul', '57318ae9e6313a140071d081': 'Saif al-Islam', '57318ae9e6313a140071d082': 'Abdullah Senussi', '572fb5b0b2c2fd1400568395': 'endospores', '572fb5b0b2c2fd1400568398': 'Dipicolinic acid', '57278133dd62a815002e9ee8': 'late 19th century', '572a7b02111d821400f38b54': 'Miami', '572a7b02111d821400f38b55': '2001', '5ad39216604f3c001a3fe68f': '', '5ad39216604f3c001a3fe690': '', '5ad39216604f3c001a3fe691': '', '5ad39216604f3c001a3fe692': '', '572f8622947a6a140053ca18': '1941', '572f8622947a6a140053ca1a': 'Adolf Hitler', '570c27686b8089140040fb9a': '1950s and 1960s', '570c27686b8089140040fb9c': 'Dr. T.R.M. Howard', '570c27686b8089140040fb9e': 'COINTELPRO', '5ad37d89604f3c001a3fe417': '', '5ad37d89604f3c001a3fe41a': '', '5ad37d89604f3c001a3fe41b': '', '57096fa9ed30961900e84120': 'any behavior of one animal that affects the current or future behavior of another animal', '57096fa9ed30961900e84121': 'zoo semiotics', '57096fa9ed30961900e84124': 'vibrational communication', '59fc23cca9fb160018f10da5': '', '5726dde0f1498d1400e8edf4': 'at least eight', '5ad3e20e604f3c001a3ff521': '', '5ad3e20e604f3c001a3ff522': '', '5ad3e20e604f3c001a3ff523': '', '5706158575f01819005e7960': 'The Wombles', '5706158575f01819005e7961': 'Very Behind the Times', '5706158575f01819005e7962': 'Uncle Bulgaria', '56f986ed9b226e1400dd1510': 'the forebrain', '57324359b9d445190005e945': 'Bible', '57324359b9d445190005e946': 'River Brethren', '57324359b9d445190005e947': \"Jehovah's Witnesses\", '57324359b9d445190005e948': 'West Point', '573106b7497a881900248b08': 'France', '573106b7497a881900248b0a': 'European', '573106b7497a881900248b0b': 'Otto von Bismarck', '5731785b497a881900248f39': '62%', '5731785b497a881900248f3a': '3.7%', '5731785b497a881900248f3b': '66.4%', '5731785b497a881900248f3c': '36', '5731785b497a881900248f3d': 'fifty', '572a23643f37b31900478727': 'crop domestication and sedentary lifestyles', '572a23643f37b31900478728': 'Formative stage', '5a7d1cfd70df9f001a874fa9': '', '5a7d1cfd70df9f001a874faa': '', '5a7d1cfd70df9f001a874fab': '', '5a7d1cfd70df9f001a874fac': '', '5a7d1cfd70df9f001a874fad': '', '56fa5493f34c681400b0c086': 'wooden spoon', '570b6593ec8fbc190045b9d2': '1991', '570b6593ec8fbc190045b9d3': 'The Black Crowes', '570b6593ec8fbc190045b9d6': 'five weeks', '5a5a45ab9c0277001abe70e8': '', '5a5a45ab9c0277001abe70eb': '', '5a5a45ab9c0277001abe70ec': '', '572849ebff5b5019007da0fe': 'to avoid breaching sovereignty through military invasion', '572849ebff5b5019007da0ff': 'Global War on Terror', '5a85e80ab4e223001a8e72c1': '', '5a85e80ab4e223001a8e72c3': '', '5a85e80ab4e223001a8e72c5': '', '56cd8ffa62d2951400fa6720': 'nods and facial expressions', '56cd8ffa62d2951400fa6721': 'Midna', '56cd8ffa62d2951400fa6722': 'Akiko Kōmoto', '56d128ed17492d1400aabad9': 'grunts', '56d128ed17492d1400aabada': 'nods and facial expressions', '56d128ed17492d1400aabadb': 'Midna', '56d128ed17492d1400aabadc': 'Akiko Kōmoto', '5a8d914edf8bba001a0f9b0c': '', '5a8d914edf8bba001a0f9b0e': '', '57096446ed30961900e8406a': '21st', '57096446ed30961900e8406b': 'Tripura', '57096446ed30961900e8406c': 'Kangra district', '5a3625b0788daf001a5f875b': '', '5a3625b0788daf001a5f875c': '', '5a3625b0788daf001a5f875d': '', '5a3625b0788daf001a5f875e': '', '5a5e5a285bc9f4001a75af1d': '', '5a5e5a285bc9f4001a75af1e': '', '5a5e5a285bc9f4001a75af1f': '', '5a5e5a285bc9f4001a75af20': '', '5a5e5a285bc9f4001a75af21': '', '57315bfaa5e9cc1400cdbf02': 'October 20, 1789', '57315bfaa5e9cc1400cdbf03': 'July 17, 1791', '57315bfaa5e9cc1400cdbf04': 'Rouget de Lisle', '57315bfaa5e9cc1400cdbf05': '1790', '5ad4fc2f5b96ef001a10a88c': '', '5ad4fc2f5b96ef001a10a88d': '', '5ad4fc2f5b96ef001a10a88e': '', '5ad4fc2f5b96ef001a10a890': '', '572fffeb947a6a140053cf38': 'Teflon', '572fffeb947a6a140053cf39': 'phenolic cotton paper', '572fffeb947a6a140053cf3b': 'glass fiber', '5ace942432bba1001ae4aacb': '', '5ace942432bba1001ae4aacc': '', '5ace942432bba1001ae4aacd': '', '573035c8b2c2fd1400568a7c': 'San Diego Regional Airport Authority', '573035c8b2c2fd1400568a7e': '17 million', '573035c8b2c2fd1400568a7f': 'Montgomery Field (MYF) and Brown Field (SDM)', '5ad4db545b96ef001a10a420': '', '5ad4db545b96ef001a10a421': '', '5ad4db545b96ef001a10a422': '', '5ad4db545b96ef001a10a423': '', '5ad4db545b96ef001a10a424': '', '572f83ae947a6a140053c9fb': '1929', '572f83ae947a6a140053c9fc': 'a worldwide economic downturn', '572efd66cb0c0d14000f16cd': 'Berlin', '572efd66cb0c0d14000f16ce': 'press the buttons', '572efd66cb0c0d14000f16cf': 'Lifted: A Cultural History of the Elevator', '57109aada58dae1900cd6ac0': '1730s', '57109aada58dae1900cd6ac1': 'British', '57109aada58dae1900cd6ac3': 'British', '57109aada58dae1900cd6ac4': 'Grand Architect', '5730c888aca1c71400fe5ab7': 'replacements for incandescent and neon indicator lamps', '5730c888aca1c71400fe5ab8': 'US$200 per unit', '5ad18fa2645df0001a2d1f62': '', '5ad18fa2645df0001a2d1f63': '', '5ad18fa2645df0001a2d1f64': '', '5ad18fa2645df0001a2d1f65': '', '5ad18fa2645df0001a2d1f66': '', '57282ec23acd2414000df67c': 'Rukh', '57282ec23acd2414000df67e': '1946', '5acd1ab607355d001abf3538': '', '5acd1ab607355d001abf3539': '', '5acd1ab607355d001abf353a': '', '5acd1ab607355d001abf353b': '', '5acd1ab607355d001abf353c': '', '572848e94b864d19001648c5': 'DVDs', '5728b383ff5b5019007da4e0': \"Stalin's death\", '572a3b486aef0514001553b9': 'Gustav Edler von Hayek', '56cf694f4df3c31400b0d750': 'Alexis Phifer', '56cf694f4df3c31400b0d751': 'Auto-Tune', '56d109f317492d1400aab7ca': '2007', '56d109f317492d1400aab7cb': 'Alexis Phifer', '5725be0738643c19005acc3f': 'Flemish', '5725be0738643c19005acc40': 'Nederlands', '5725be0738643c19005acc42': 'Hollands', '5725be0738643c19005acc43': 'Western Flemish', '572f8892b2c2fd14005681c5': 'London', '572f8892b2c2fd14005681c6': 'Battle of Britain Day', '572f8892b2c2fd14005681c7': 'bomb-load limitations', '56e08070231d4119001ac1ff': '+290', '56e08070231d4119001ac200': 'Tristan da Cunha', '56e08070231d4119001ac202': '1 October 2013', '56e08070231d4119001ac203': '2', '56fe01af19033b140034ce31': 'a computer', '5730878f2461fd1900a9ce91': 'Ladoga and Karelia regions', '5acff7c077cf76001a68669e': '', '5acff7c077cf76001a68669f': '', '5acff7c077cf76001a6866a0': '', '5acff7c077cf76001a6866a1': '', '5acff7c077cf76001a6866a2': '', '56cf4b72aab44d1400b88f75': 'Bob Ewell', '572fc229b2c2fd1400568403': 'Dowding', '571de3ebb64a571400c71dd8': 'slaves', '571de3ebb64a571400c71dd9': 'Sally Hemings', '571de3ebb64a571400c71dda': 'paternity', '5ad2b36bd7d075001a429f90': '', '5ad2b36bd7d075001a429f91': '', '5ad2b36bd7d075001a429f94': '', '572759cb5951b619008f888e': '1890s', '572759cb5951b619008f8891': 'DuPont', '5a8192e331013a001a334cce': '', '5a8192e331013a001a334ccf': '', '5a8192e331013a001a334cd0': '', '5732a3dfcc179a14009dabc5': 'Mesozoic', '5a4eb85eaf0d07001ae8cbf0': '', '5a4eb85eaf0d07001ae8cbf1': '', '5a4eb85eaf0d07001ae8cbf2': '', '5a4eb85eaf0d07001ae8cbf3': '', '56d8dc9cdc89441400fdb350': 'April 3', '56d8dc9cdc89441400fdb352': 'Sultanahmet Square', '56db0a87e7c41114004b4cb1': 'Istanbul', '56db0a87e7c41114004b4cb2': 'Sultanahmet Square', '56db0a87e7c41114004b4cb4': 'Uyghurs', '571a993510f8ca140030518d': 'Middle Eastern Jews and European/Syrian Jews', '56e168ebe3433e1400422ec4': '80%', '56e168ebe3433e1400422ec5': 'Seagram', '56e168ebe3433e1400422ec6': '$5.7 billion', '56e168ebe3433e1400422ec8': 'PolyGram', '5ad15e76645df0001a2d18e3': '', '5ad15e76645df0001a2d18e4': '', '56ce659aaab44d1400b88749': 'toxic chemicals', '56d0051e234ae51400d9c26f': 'to treat waste water without chemicals or electricity', '56ce451caab44d1400b8863c': 'Master of Vajradhara', '56ce451caab44d1400b8863d': 'Yonten Gyatso', '56ce451caab44d1400b8863e': '1616', '56ce451caab44d1400b8863f': 'Yonten Gyatso', '5acec97b32bba1001ae4b407': '', '5acec97b32bba1001ae4b408': '', '5acec97b32bba1001ae4b409': '', '56defb12c65bf219000b3e75': 'eleven', '5ad3eb5a604f3c001a3ff713': '', '5ad3eb5a604f3c001a3ff714': '', '5ad3eb5a604f3c001a3ff715': '', '5ad3eb5a604f3c001a3ff716': '', '5ad28515d7d075001a429888': '', '5ad28515d7d075001a429889': '', '5ad28515d7d075001a42988a': '', '5ad28515d7d075001a42988b': '', '5ad28515d7d075001a42988c': '', '56cbd2356d243a140015ed68': 'solo piano', '56cbd2356d243a140015ed69': 'Duchy of Warsaw', '56cbd2356d243a140015ed6a': '20', '56ce0a3762d2951400fa69d8': 'Warsaw', '56ce0a3762d2951400fa69d9': 'solo piano', '56ce0a3762d2951400fa69da': '20', '56cf54a2aab44d1400b89006': '17 October 1849', '56cf54a2aab44d1400b89008': 'Fryderyk Franciszek Chopin', '56cf54a2aab44d1400b89009': 'solo piano', '56d1ca30e7d4791d009021a9': '20', '56d1ca30e7d4791d009021aa': 'Romantic', '56d1ca30e7d4791d009021ab': '1849', '5727a8874b864d19001639b8': 'seven', '5727a8874b864d19001639b9': 'free movement of persons', '5727a8874b864d19001639ba': '2004', '5733fa9a4776f41900661625': 'European Commission, European Central Bank and International Monetary Fund', '5733fa9a4776f41900661626': '2011', '5733fa9a4776f41900661627': '€78 billion', '5733fa9a4776f41900661628': 'May 2014', '5733fa9a4776f41900661629': '15.3 percent', '570a5e0e6d058f1900182dbc': '2007', '570a5e0e6d058f1900182dbd': 'Faculty of Medicine', '570a5e0e6d058f1900182dbe': 'Alice Gast', '5a4875a284b8a4001a7e788e': '', '5a4875a284b8a4001a7e788f': '', '5a4875a284b8a4001a7e7890': '', '5a4875a284b8a4001a7e7891': '', '56df631296943c1400a5d4a9': 'Atlantic depressions', '56df631296943c1400a5d4aa': 'autumn', '56df631296943c1400a5d4ad': 'south-west', '572e7f0adfa6aa1500f8d048': 'Mycenaean Greek traders', '572e7f0adfa6aa1500f8d04a': 'Aphrodite and Adonis', '57285eda3acd2414000df96f': 'much like people', '572c0480dfb02c14005c6b5f': 'Pascal Boyer', '572c0480dfb02c14005c6b61': 'Greek mythology', '572c0480dfb02c14005c6b62': 'Sigmund Freud', '5a3c823dcc5d22001a521dec': '', '5a3c823dcc5d22001a521ded': '', '5a3c823dcc5d22001a521dee': '', '5a3c823dcc5d22001a521def': '', '5a3c823dcc5d22001a521df0': '', '5728e29f3acd2414000e0117': 'Dionysus', '5728e29f3acd2414000e0118': 'Apollo', '5728e29f3acd2414000e0119': 'Hyperborea', '570b2117ec8fbc190045b842': 'Sweden', '570b2117ec8fbc190045b845': 'United States', '5a1f6f6654a786001a36b2bd': '', '5a1f6f6654a786001a36b2be': '', '5a1f6f6654a786001a36b2bf': '', '5a1f6f6654a786001a36b2c0': '', '5a1f6f6654a786001a36b2c1': '', '5705fcd775f01819005e783a': 'Australian', '5705fcd775f01819005e783b': 'Rupert Murdoch', '56e180f5e3433e1400422f99': '90% to 95%', '5a4d40ce7a6c4c001a2bbc85': '', '5a4d40ce7a6c4c001a2bbc86': '', '5a4d40ce7a6c4c001a2bbc87': '', '5a4d40ce7a6c4c001a2bbc88': '', '5726ee155951b619008f82ad': 'prey', '5726ee155951b619008f82af': 'survival and fecundity', '5a6be2214eec6b001a80a5f2': '', '5a6be2214eec6b001a80a5f3': '', '5a6be2214eec6b001a80a5f4': '', '5a6be2214eec6b001a80a5f5': '', '5a6be2214eec6b001a80a5f6': '', '5728d8e3ff5b5019007da80e': 'Wayne State University', '5728d8e3ff5b5019007da811': 'Society of Jesus', '56f8360ca6d7ea1400e174ab': 'Karađorđevo', '56f8360ca6d7ea1400e174ac': 'Galeb', '56f8360ca6d7ea1400e174ad': 'Aviogenex', '56f8360ca6d7ea1400e174ae': 'Montenegro', '56f8360ca6d7ea1400e174af': 'seagull', '5706969952bb891400689ab4': 'Synthesizing: Ten Ragas to a Disco Beat', '5706969952bb891400689ab6': '1982', '5ad269fad7d075001a4292fe': '', '5ad269fad7d075001a429300': '', '572b8fff111d821400f38f1a': '1914', '572b8fff111d821400f38f1c': 'Texas, Nebraska and Wisconsin', '572b8fff111d821400f38f1e': '70,500', '5a7a198217ab25001a8a032a': '', '5a7a198217ab25001a8a032b': '', '5a7a198217ab25001a8a032c': '', '5a7a198217ab25001a8a032d': '', '5a7a198217ab25001a8a032e': '', '572649865951b619008f6f1d': 'Andreas Papandreou', '572649865951b619008f6f1e': 'Panhellenic Socialist Movement', '572649865951b619008f6f1f': '1980', '572649865951b619008f6f21': '1999', '5ad13a0b645df0001a2d12a8': '', '57266a17708984140094c532': 'the Constitution', '57266a17708984140094c534': 'state law', '5726d83d708984140094d332': 'universal', '5726d83d708984140094d333': 'dual-sovereign system', '5726d83d708984140094d334': 'Indian reservations', '5726d83d708984140094d335': 'the federal Constitution', '572c9ab3dfb02c14005c6bac': '50', '5a79baa717ab25001a8a0002': '', '5a79baa717ab25001a8a0004': '', '5a79baa717ab25001a8a0005': '', '5728b8862ca10214002da658': 'Nawab of Bengal Siraj Ud Daulah', '5728b8862ca10214002da65c': 'monopolized', '57283abb4b864d19001647ae': 'Leonid Kravchuk', '57283abb4b864d19001647af': 'Chernobyl Nuclear Power Plant', '57283abb4b864d19001647b2': '40,000', '57296cccaf94a219006aa3df': 'Germany', '57296cccaf94a219006aa3e1': 'coal', '5ad13603645df0001a2d1189': '', '5ad13603645df0001a2d118a': '', '5ad13603645df0001a2d118b': '', '57342bbc4776f419006619f4': \"Nuno Malo and Miguel d'Oliveira\", '5726d331f1498d1400e8ec6f': 'Royal Niger Company', '5726d331f1498d1400e8ec70': '1900', '5726d331f1498d1400e8ec71': '1 January 1901', '5726d331f1498d1400e8ec72': 'Benin', '570b09a96b8089140040f703': \"carrying a heavy cruiser's complement of defensive weapons and large P-700 Granit offensive missiles\", '5acd884007355d001abf4605': '', '5acd884007355d001abf4606': '', '5acd884007355d001abf4607': '', '5709828fed30961900e84245': 'Native Americans', '5709828fed30961900e84246': 'Isle Royale', '5709828fed30961900e84247': 'Peru', '5709828fed30961900e84248': 'early 20th century', '5a836aeee60761001a2eb68b': '', '5a836aeee60761001a2eb68d': '', '5a836aeee60761001a2eb68e': '', '56e163afe3433e1400422e64': 'Boston Patriots', '56e163afe3433e1400422e65': 'after relocating', '570d410afed7b91900d45db3': 'solid fuel rockets', '570d410afed7b91900d45db5': 'unrotated projectiles', '570d410afed7b91900d45db6': '2-inch', '570d410afed7b91900d45db7': '3-inch', '56cf6aa44df3c31400b0d75f': 'Matthew Trammell', '56d10ab617492d1400aab7ee': 'Matthew Trammell', '56f8d3179b226e1400dd1097': 'France', '57303e5ea23a5019007fcffd': 'Backup and Restore', '5ad49c1fba00c4001a268d90': '', '5ad49c1fba00c4001a268d91': '', '5ad49c1fba00c4001a268d92': '', '5ad49c1fba00c4001a268d93': '', '5ad49c1fba00c4001a268d94': '', '5726adf0dd62a815002e8cc4': 'cathedrals and great churches', '5ad0ddaa645df0001a2d0716': '', '5ad0ddaa645df0001a2d0717': '', '5ad0ddaa645df0001a2d0718': '', '5ad0ddaa645df0001a2d0719': '', '56df778a5ca0a614008f9adb': '33', '56df778a5ca0a614008f9adc': 'National Geographic Society', '56df778a5ca0a614008f9ade': 'hydrofoils', '5731dd950fdd8d15006c65af': 'Portuguese', '5731dd950fdd8d15006c65b0': 'English and Spanish', '5731dd950fdd8d15006c65b1': 'six', '5731dd950fdd8d15006c65b2': 'August 2016', '5a2eeb0aa83784001a7d2569': '', '5a2eeb0aa83784001a7d256a': '', '5a2eeb0aa83784001a7d256b': '', '5a2eeb0aa83784001a7d256c': '', '5a2eeb0aa83784001a7d256d': '', '56f8cf869b226e1400dd1051': 'two', '56f8cf869b226e1400dd1052': 'Southampton City College', '56f8cf869b226e1400dd1054': 'Barton Peveril College', '572846473acd2414000df84f': 'Manhattan Project', '56dfb89e7aa994140058e071': 'The George', '56dfb89e7aa994140058e072': 'Scotland', '5731eaa7e17f3d1400422547': 'Industrial Revolution', '5731eaa7e17f3d1400422549': 'slightly lower', '5731eaa7e17f3d140042254b': 'defaulted on its external loans', '5a7b2ba921c2de001afe9d80': '', '5a7b2ba921c2de001afe9d81': '', '5a7b2ba921c2de001afe9d83': '', '572786b5dd62a815002e9f8a': 'five years', '572786b5dd62a815002e9f8b': 'John van Wyhe', '56e837f437bdd419002c44b2': 'Latin', '56e837f437bdd419002c44b4': 'Old Norse', '56e837f437bdd419002c44b5': 'English', '5ad27318d7d075001a4294ae': '', '5ad27318d7d075001a4294af': '', '5ad27318d7d075001a4294b1': '', '5ad27318d7d075001a4294b2': '', '56e09c507aa994140058e64e': 'H+', '56e790e237bdd419002c414d': 'U.S. News & World Report', '56e790e237bdd419002c414e': '90th', '5acf5c9477cf76001a684c36': '', '5acf5c9477cf76001a684c37': '', '5acf5c9477cf76001a684c38': '', '5acf5c9477cf76001a684c39': '', '5acf5c9477cf76001a684c3a': '', '57284dd0ff5b5019007da13e': '30 July 2003', '5a86027fb4e223001a8e737f': '', '5a86027fb4e223001a8e7380': '', '5a86027fb4e223001a8e7381': '', '5a86027fb4e223001a8e7382': '', '57266970f1498d1400e8dee0': 'Vince McMahon', '5726e302f1498d1400e8eea8': 'Philosophy', '5726e302f1498d1400e8eeaa': 'Plato, Aristotle, Socrates, Augustine, Descartes, Kierkegaard, Nietzsche', '5726e302f1498d1400e8eeac': 'mathematics', '5a7a553c21c2de001afe9b72': '', '5a7a553c21c2de001afe9b75': '', '5a7e59cc48f7d9001a063515': '', '5a7e59cc48f7d9001a063516': '', '5acf588577cf76001a684bd4': '', '5acf588577cf76001a684bd5': '', '5730c30bf6cb411900e24461': 'palagi traders', '5730c30bf6cb411900e24462': \"John (also known as Jack) O'Brien\", '5730c30bf6cb411900e24463': 'Salai', '5730c30bf6cb411900e24464': 'Louis Becke', '57304c5e8ab72b1400f9c401': 'aerial mines', '5727b735ff5b5019007d933f': 'to make it trivial to convert existing western text', '5acd10bc07355d001abf32f4': '', '5acd10bc07355d001abf32f5': '', '5acd10bc07355d001abf32f7': '', '57322fcce17f3d14004226ed': 'Policy Committee', '57322fcce17f3d14004226ee': 'Steering Committee', '5a84a6117cf838001a46aa08': '', '5a84a6117cf838001a46aa09': '', '5a84a6117cf838001a46aa0a': '', '5a84a6117cf838001a46aa0b': '', '5a84a6117cf838001a46aa0c': '', '57317c50a5e9cc1400cdbfc0': '1743', '57317c50a5e9cc1400cdbfc1': 'papal gifts', '57317c50a5e9cc1400cdbfc3': 'designs or canvases', '5726e7d3f1498d1400e8ef84': 'Britain', '5726e7d3f1498d1400e8ef85': 'The Corps of Royal Engineers', '5726e7d3f1498d1400e8ef86': 'Paris', '56fad599f34c681400b0c149': 'Asia', '56fad599f34c681400b0c14a': 'Holden', '56fad599f34c681400b0c14b': 'Afro-Asiatic', '5733e8ccd058e614000b656f': 'Nazi Germany, Fascist Italy, and Imperial Japan', '5733e8ccd058e614000b6571': 'intelligence', '572a3a0b6aef0514001553a2': 'J. M. E. McTaggart', '572a3a0b6aef0514001553a3': 'two', '5a42d2aa4a4859001aac734c': '', '5a42d2aa4a4859001aac734d': '', '56e79b2d00c9c71400d77383': 'Yangtze River', '56e79b2d00c9c71400d77384': 'Nanjing–Beijing railway', '56e79b2d00c9c71400d77387': 'tiger', '5a273563c93d92001a400425': '', '5a273563c93d92001a400426': '', '5a273563c93d92001a400429': '', '5726c3b1f1498d1400e8ea96': 'Pope John Paul II', '5726c3b1f1498d1400e8ea99': 'Andrei Gromyko', '5726c3b1f1498d1400e8ea9a': 'peace day', '5a6bb40d4eec6b001a80a4fd': '', '5a6bb40d4eec6b001a80a4fe': '', '5a6bb40d4eec6b001a80a4ff': '', '5a6bb40d4eec6b001a80a500': '', '56cc643d6d243a140015ef88': 'weak bass response', '56cc643d6d243a140015ef8a': 'high-impedance', '56cc643d6d243a140015ef8b': 'external headphone amplifier', '571a10584faf5e1900b8a882': 'fringe theatre', '571a10584faf5e1900b8a883': 'equity theaters', '571a10584faf5e1900b8a884': '28', '5726638e708984140094c48f': '1990', '5726638e708984140094c492': 'the salination of the Werra river', '5a7cc42be8bc7e001a9e1fda': '', '5a7cc42be8bc7e001a9e1fdb': '', '5a7cc42be8bc7e001a9e1fdc': '', '5a7cc42be8bc7e001a9e1fdd': '', '570aaf434103511400d59928': 'top ranking', '570aaf434103511400d59929': '2008', '570aaf434103511400d5992a': 'fourth', '570aaf434103511400d5992b': 'second', '570aaf434103511400d5992c': '2010', '5ad420ea604f3c001a400729': '', '5ad420ea604f3c001a40072a': '', '5ad420ea604f3c001a40072b': '', '5ad420ea604f3c001a40072c': '', '5ad420ea604f3c001a40072d': '', '56e19df2e3433e1400423034': 'Eastern Catalan', '570c6506fed7b91900d45983': 'joining Barça', '5726086889a1e219009ac16e': 'higher', '5726086889a1e219009ac170': 'Due to the discrete spectral lines rather than a continuous spectrum', '5ad18fb3645df0001a2d1f6c': '', '5ad18fb3645df0001a2d1f6d': '', '5ad18fb3645df0001a2d1f6e': '', '5ad18fb3645df0001a2d1f6f': '', '5acfbe5777cf76001a685bfc': '', '5acfbe5777cf76001a685bfd': '', '5acfbe5777cf76001a685bfe': '', '5acfbe5777cf76001a685bff': '', '572ea043c246551400ce4422': 'two', '572ea043c246551400ce4423': 'Greek and Turkish', '572ea043c246551400ce4424': 'Armenian and Cypriot Maronite Arabic', '5725bdfa271a42140099d10f': '1882', '5725bdfa271a42140099d110': '1886', '5725bdfa271a42140099d111': '1888', '5725bdfa271a42140099d112': '1897', '56cfef3c234ae51400d9c10e': 'Funeral March', '56d3913859d6e41400146795': 'Revolutionary Étude', '56d3913859d6e41400146796': 'Minute Waltz', '5731185a05b4da19006bcd90': 'Aves', '5731185a05b4da19006bcd91': 'Neornithes', '5731185a05b4da19006bcd92': 'Aves', '5731185a05b4da19006bcd93': '9,800 to 10,050', '570d6de5fed7b91900d460b8': '1519–1523', '570d6de5fed7b91900d460b9': 'plague', '570d6de5fed7b91900d460ba': 'Italian republics', '570d6de5fed7b91900d460bb': 'the Germanies', '56f8def59e9bad19000a063e': 'three', '56f8def59e9bad19000a063f': 'Town Quay', '56f8def59e9bad19000a0640': 'Two', '56f8def59e9bad19000a0641': 'Red Funnel', '56de80edcffd8e1900b4b98e': 'Munich stone-lifting contest', '5728a7b83acd2414000dfc03': 'Domestic Organization', '5728a7b83acd2414000dfc04': 'about 1940', '5728a7b83acd2414000dfc05': 'cases of cheating and academic dishonesty', '5728a7b83acd2414000dfc06': '1957', '5728a7b83acd2414000dfc07': 'all students, faculty, and staff', '5acec3f332bba1001ae4b33b': '', '5acec3f332bba1001ae4b33c': '', '57291a3e1d04691400779035': 'eleven', '57291a3e1d04691400779036': 'Western Allies', '57291a3e1d04691400779038': '1957', '5a4745ff5fd40d001a27dd9e': '', '5a4745ff5fd40d001a27dd9f': '', '5a4745ff5fd40d001a27dda0': '', '5a514b63ce860b001aa3fcbe': '', '5a514b63ce860b001aa3fcbf': '', '5a514b63ce860b001aa3fcc0': '', '5730ed3ea5e9cc1400cdbaf3': '99.0%', '5730ed3ea5e9cc1400cdbaf4': '1,918', '5730ed3ea5e9cc1400cdbaf5': '1:18', '57060fde52bb891400689836': '2012', '57060fde52bb891400689837': 'Barack Obama', '572a518ab8ce0319002e2a93': 'commercial centres and routes', '56df7b3d56340a1900b29c0c': 'Plymouth City Council', '56df7b3d56340a1900b29c0d': 'South West Water', '56df7b3d56340a1900b29c0e': 'Western Power Distribution', '56df7b3d56340a1900b29c0f': '2009', '56df7b3d56340a1900b29c10': 'Plympton', '56f8c9d29e9bad19000a04f0': 'cell division', '56f8c9d29e9bad19000a04f2': 'DNA polymerases', '56f8c9d29e9bad19000a04f3': 'Because the DNA double helix is held together by base pairing', '56f8c9d29e9bad19000a04f4': 'semiconservative', '5726e07a5951b619008f8105': 'destroy the guerrillas and their sympathizer citizens in Southern Korea', '5726e07a5951b619008f8108': \"broke the attack's momentum\", '5acd969f07355d001abf47ae': '', '5acd969f07355d001abf47b0': '', '5acd969f07355d001abf47b1': '', '56e0cdc37aa994140058e721': 'Internet Explorer', '56e0cdc37aa994140058e723': 'Mac', '5a4d39a27a6c4c001a2bbc66': '', '5a4d39a27a6c4c001a2bbc67': '', '5a4d39a27a6c4c001a2bbc68': '', '5a4d39a27a6c4c001a2bbc69': '', '5a4d39a27a6c4c001a2bbc6a': '', '57302bbda23a5019007fcee3': 'Liberian timber exports', '57302bbda23a5019007fcee6': '60%', '57302bbda23a5019007fcee7': '2010', '5a62be8af8d794001af1c1fd': '', '5a62be8af8d794001af1c1fe': '', '5a62be8af8d794001af1c200': '', '5733b195d058e614000b6085': 'November 2010', '5733b195d058e614000b6086': '2015', '5aced84b32bba1001ae4b713': '', '5aced84b32bba1001ae4b714': '', '5aced84b32bba1001ae4b715': '', '56e7b1e900c9c71400d77503': 'an extra hour', '56e7b1e900c9c71400d77505': 'three', '5a7fb0f18f0597001ac0007c': '', '5a7fb0f18f0597001ac0007d': '', '56d638e71c8504140094700c': 'immune-stimulating microorganisms', '56d638e71c8504140094700d': 'social interactions', '56d638e71c8504140094700f': '2015', '56d9ddf4dc89441400fdb871': 'strangers', '5731450de6313a140071cda1': 'The Eye in the Sky', '5731450de6313a140071cda3': 'Electronic Warfare/Jamming', '5726f50add62a815002e9630': 'scribes', '5726f50add62a815002e9632': 'a Chaldean astronomer and mathematician', '5726f50add62a815002e9634': \"today's calendars\", '5725d375ec44d21400f3d643': 'Fortaleza del Cerro', '5725d375ec44d21400f3d644': 'a beacon', '5725d375ec44d21400f3d645': '1809', '5725d375ec44d21400f3d646': '1839', '571021d7a58dae1900cd68d0': 'Utah', '571021d7a58dae1900cd68d1': 'June 22, 1965', '571021d7a58dae1900cd68d2': 'Oklahoma', '5ad3f93e604f3c001a3ffaa5': '', '5ad3f93e604f3c001a3ffaa6': '', '5ad3f93e604f3c001a3ffaa8': '', '5733e5704776f41900661451': 'human–animal studies', '5733e5704776f41900661452': 'Anthrozoology', '5733e5704776f41900661454': 'positive', '5733e5704776f41900661455': 'anthropology, sociology, biology, and philosophy', '5ad2f672604f3c001a3fda6c': '', '57302c9aa23a5019007fceff': 'elliptical', '57302c9aa23a5019007fcf00': 'one direction', '572845f7ff5b5019007da09e': 'two', '572845f7ff5b5019007da09f': 'near the Somali village of Baarawe', '572845f7ff5b5019007da0a0': 'French', '572845f7ff5b5019007da0a1': 'Al-Shabaab', '572845f7ff5b5019007da0a2': 'Kenyan', '5a839cf3e60761001a2eb829': '', '5a85b7e3b4e223001a8e71ba': '', '5a85b7e3b4e223001a8e71bc': '', '5a85b7e3b4e223001a8e71bd': '', '57266a2c5951b619008f7203': 'saltpetre', '57266a2c5951b619008f7204': '1673', '57266a2c5951b619008f7205': 'armed forces', '57266a2c5951b619008f7207': '£37,000', '5a8464dd7cf838001a46a7db': '', '5a8464dd7cf838001a46a7dc': '', '5a8464dd7cf838001a46a7dd': '', '570b00026b8089140040f69f': 'AT&T Corporation', '570b00026b8089140040f6a1': '6 Mbit/s', '570b00026b8089140040f6a2': '1 MHz', '5a1f22043de3f40018b26511': '', '572a8012be1ee31400cb8043': 'music, dancing and poetry', '572a8012be1ee31400cb8044': 'Shuhda', '572a8012be1ee31400cb8046': '1258', '572a8012be1ee31400cb8047': 'Abbasid', '5ace804f32bba1001ae4a87a': '', '572b883cf75d5e190021fe33': 'several colleges', '5acd76f907355d001abf437a': '', '5acd76f907355d001abf437b': '', '5acd76f907355d001abf437c': '', '5acd76f907355d001abf437d': '', '5acd76f907355d001abf437e': '', '5731a21de17f3d1400422295': 'limited resources', '5731a21de17f3d1400422296': 'free', '5a77ae98b73996001af5a4f9': '', '5a77ae98b73996001af5a4fb': '', '5a77ae98b73996001af5a4fc': '', '56dde0ea66d3e219004dad7e': 'four', '56dde0ea66d3e219004dad7f': 'the National Defence Act', '5ad37275604f3c001a3fe2b3': '', '5ad37275604f3c001a3fe2b4': '', '5ad3e04d604f3c001a3ff4b3': '', '5ad3e04d604f3c001a3ff4b4': '', '5709b165ed30961900e84427': 'European monarchs', '5a8cd753fd22b3001a8d8f2e': '', '5a8cd753fd22b3001a8d8f2f': '', '5a8cd753fd22b3001a8d8f30': '', '5a8cd753fd22b3001a8d8f32': '', '56e14538e3433e1400422d1e': 'French', '56e14538e3433e1400422d20': 'in public life and education', '56e14538e3433e1400422d22': 'Catalan', '56d296f259d6e414001460f8': 'text', '56d296f259d6e414001460fa': 'āgamas', '56d296f259d6e414001460fb': 'core', '56d296f259d6e414001460fc': 'size and complexity of the Buddhist canons', '56dfdbee7aa994140058e1c9': 'July 2007', '56dfdbee7aa994140058e1ca': 'Carlsberg and Heineken', '572827843acd2414000df5af': 'microvilli', '5ace7ee632bba1001ae4a82b': '', '5ace7ee632bba1001ae4a82c': '', '5ace7ee632bba1001ae4a82d': '', '5ace7ee632bba1001ae4a82e': '', '5728b201ff5b5019007da4a2': 'Japan', '5ad22e93d7d075001a42866a': '', '5ad22e93d7d075001a42866b': '', '5ad22e93d7d075001a42866c': '', '5ad22e93d7d075001a42866d': '', '5ad22e93d7d075001a42866e': '', '5728dbd84b864d1900164fab': '21,616', '5728dbd84b864d1900164fac': 'Hauts-de-Seine, Seine-Saint-Denis and Val-de-Marne', '573015da04bcaa1900d77155': 'the Bears', '573015da04bcaa1900d77156': 'Division III', '573015da04bcaa1900d77157': '19', '573015da04bcaa1900d77158': '2008, 2009', '573015da04bcaa1900d77159': 'John Schael', '5ace3ada32bba1001ae49f8b': '', '5ace3ada32bba1001ae49f8e': '', '56e7ac6c37bdd419002c430e': 'Lukou International Airport', '56e7ac6c37bdd419002c4311': '28 June 1997', '57325124e17f3d140042285b': '1759', '57325124e17f3d140042285e': '1693', '57325124e17f3d140042285f': 'Frederick Philipse', '5727a1eeff5b5019007d9161': 'Fiesta de las Flores y las Frutas', '57276683dd62a815002e9c34': 'early 20th century', '56e032247aa994140058e34c': '1959', '5acf864b77cf76001a6850d0': '', '5acf864b77cf76001a6850d1': '', '5acf864b77cf76001a6850d2': '', '5acf864b77cf76001a6850d3': '', '5acf864b77cf76001a6850d4': '', '57321963e99e3014001e650c': 'Birds', '57321963e99e3014001e650d': 'auspex', '5a7704962d6d7f001a4a9f13': '', '5a7704962d6d7f001a4a9f14': '', '5a7704962d6d7f001a4a9f15': '', '5a7704962d6d7f001a4a9f16': '', '5a7704962d6d7f001a4a9f17': '', '5730a9732461fd1900a9cf64': 'Mesopotamia and the Persian Gulf', '5730a9732461fd1900a9cf65': 'Eridu', '5730a9732461fd1900a9cf67': 'Enki', '5a650f65c2b11c001a425bc5': '', '5a650f65c2b11c001a425bc6': '', '57310f4ae6313a140071cbcc': '19 March 1920', '57310f4ae6313a140071cbcd': 'League of Nations Council', '5a14a7c7a54d4200185292fe': '', '5a14a7c7a54d420018529301': '', '56de7b394396321400ee295a': 'Devonport', '56de7a53cffd8e1900b4b960': 'William Cookworthy', '56de7a53cffd8e1900b4b961': '1768', '56de7a53cffd8e1900b4b962': 'chemist', '56de7a53cffd8e1900b4b963': 'John Smeaton', '5732ae23cc179a14009dabfb': 'Pliocene', '5a4ebec8af0d07001ae8cc26': '', '5a4ebec8af0d07001ae8cc28': '', '572bc28a111d821400f38f77': 'essential for countries to be able to achieve high levels of economic growth', '5acd857607355d001abf4560': '', '5acd857607355d001abf4561': '', '5acd857607355d001abf4563': '', '57337cc94776f41900660baa': '\"he stands provisionally as the last great Anglo-American philosopher before Wittgenstein\\'s disciples spread their misty confusion, sufficiency, and terror.\"', '57337cc94776f41900660bab': 'Bruno Latour', '5ad3d82d604f3c001a3ff36f': '', '5ad3d82d604f3c001a3ff370': '', '5ad3d82d604f3c001a3ff371': '', '5ad3d82d604f3c001a3ff372': '', '56e10bf4cd28a01900c674c3': 'April 19, 1971', '56e10bf4cd28a01900c674c4': 'Vladislav Volkov, Georgi Dobrovolski and Viktor Patsayev', '56fae1d7f34c681400b0c16d': 'Afro-Asiatic', '56fae1d7f34c681400b0c16e': 'Cushitic', '56fae1d7f34c681400b0c16f': 'Afar', '56fae1d7f34c681400b0c170': 'Somali', '56fae1d7f34c681400b0c171': '1900', '5723fc250dadf01500fa1fe1': 'Prince Frederick William of Prussia', '5723fc250dadf01500fa1fe3': '14', '572501720ba9f01400d97c25': 'Prince Frederick William of Prussia', '572501720ba9f01400d97c26': '14', '572501720ba9f01400d97c27': 'Germany', '572501720ba9f01400d97c28': 'Wilhelm', '57266b70708984140094c56a': 'Wilhelm', '5ad17607645df0001a2d1cf1': '', '5ad17607645df0001a2d1cf2': '', '571a2b2410f8ca1400304f29': '7th congressional district', '571a2b2410f8ca1400304f2a': 'Jim McDermott', '571a2b2410f8ca1400304f2b': '1988', '571a2b2410f8ca1400304f2c': 'Ed Murray', '56e7b1d437bdd419002c437b': 'Sunday', '56e7b1d437bdd419002c437c': 'June 30, 2006', '572fd6a6b2c2fd14005684f5': 'Business and Law', '573271ece17f3d140042297f': 'Kaesong sanctuary', '573271ece17f3d1400422980': 'use nuclear force', '573271ece17f3d1400422983': 'Strategic Air Command', '5706b5fa0eeca41400aa0d70': \"Jewel's Catch One\", '5706b5fa0eeca41400aa0d73': 'Ozn', '5ad2932ad7d075001a429ade': '', '5ad2932ad7d075001a429adf': '', '56f8ba089e9bad19000a03c9': 'Junk', '56f8ba089e9bad19000a03ca': 'dance', '56f96a3c9e9bad19000a08ed': 'US$57.7 million', '56f96a3c9e9bad19000a08ee': 'US$62.7 million', '572966e11d046914007793a7': 'additive', '5a74beb142eae6001a389a5a': '', '5a74beb142eae6001a389a5c': '', '56e0c956231d4119001ac392': 'private networks', '5a4d2f747a6c4c001a2bbc17': '', '5a4d2f747a6c4c001a2bbc1a': '', '5ad417d2604f3c001a4003b7': '', '5ad417d2604f3c001a4003b8': '', '5ad417d2604f3c001a4003ba': '', '5726b8addd62a815002e8e27': 'Six', '5726b8addd62a815002e8e29': 'Vatican II', '5726b8addd62a815002e8e2a': 'Canon Law', '5729145a3f37b31900478001': 'several', '5a47387c5fd40d001a27dd7f': '', '5a47387c5fd40d001a27dd80': '', '5a47387c5fd40d001a27dd82': '', '5a47387c5fd40d001a27dd83': '', '5a5140ccce860b001aa3fc88': '', '5a5140ccce860b001aa3fc89': '', '5a5140ccce860b001aa3fc8a': '', '5a5140ccce860b001aa3fc8b': '', '56dfe86b7aa994140058e252': 'John Manners, 3rd Duke of Rutland', '56dfe86b7aa994140058e253': 'general', '56dfe86b7aa994140058e255': 'the Royal George', '572fa6bca23a5019007fc831': 'World War I', '572fa6bca23a5019007fc832': '29 May 1915', '572fa6bca23a5019007fc833': 'Anatolia', '57290895af94a219006a9fad': 'Le Monde and Le Figaro', '57290895af94a219006a9fb0': 'France 24', '56e162a3e3433e1400422e45': 'Boston Garden', '56e162a3e3433e1400422e46': 'two', '56e162a3e3433e1400422e47': '18,624', '56e162a3e3433e1400422e48': '17,565', '5726b8be708984140094cf18': 'Protestant', '5ad11411645df0001a2d0c95': '', '5ad11411645df0001a2d0c96': '', '5ad11411645df0001a2d0c97': '', '5ad11411645df0001a2d0c98': '', '5731bcdc0fdd8d15006c64c3': 'legal scholars', '5731bcdc0fdd8d15006c64c7': 'Massachusetts', '5ad1423f645df0001a2d1420': '', '5ad1423f645df0001a2d1421': '', '5ad1423f645df0001a2d1423': '', '5ad1423f645df0001a2d1424': '', '56f8d9db9b226e1400dd10e0': 'Senegal', '56f8d9db9b226e1400dd10e2': 'Atlantic Ocean', '56f8d9db9b226e1400dd10e3': '11° and 13°N', '5728116c3acd2414000df3a3': 'independence', '5728116c3acd2414000df3a4': 'The European Parliament', '5a7a6eb521c2de001afe9c3b': '', '5a7a6eb521c2de001afe9c3c': '', '5a7a6eb521c2de001afe9c3d': '', '5726134f89a1e219009ac1fd': 'LED lamps', '5726134f89a1e219009ac1fe': 'higher initial cost of alternatives and lower quality of light of fluorescent lamps', '5726134f89a1e219009ac1ff': 'mercury', '5726134f89a1e219009ac200': 'much less', '5ad192db645df0001a2d2008': '', '5ad192db645df0001a2d2009': '', '5ad192db645df0001a2d200a': '', '5ad192db645df0001a2d200b': '', '5ad29f83d7d075001a429c7a': '', '5ad29f83d7d075001a429c7b': '', '5ad29f83d7d075001a429c7c': '', '5ad29f83d7d075001a429c7d': '', '56e6df336fe0821900b8ec10': 'dance-pop', '56e6df336fe0821900b8ec12': 'power pops', '56e6df336fe0821900b8ec14': '18-54', '5710f431b654c5140001fa41': 'Isaac Newton', '57294c7f3f37b31900478213': '1 GW', '57294c7f3f37b31900478214': '10 GW', '5ad12914645df0001a2d0ff6': '', '5ad12914645df0001a2d0ff7': '', '5ad12914645df0001a2d0ff8': '', '570a5d534103511400d59675': 'five', '570a5d534103511400d59677': '£800 million', '570a5d534103511400d59678': 'more than a million', '5a48747484b8a4001a7e7889': '', '5a48747484b8a4001a7e788a': '', '56defd9bc65bf219000b3e9d': 'CJOC', '56defd9bc65bf219000b3e9e': 'CFB Trenton', '56defd9bc65bf219000b3e9f': '427', '5ad3ec3b604f3c001a3ff75a': '', '5ad3ec3b604f3c001a3ff75b': '', '5ad3ec3b604f3c001a3ff75c': '', '5ad3ec3b604f3c001a3ff75d': '', '570b28ab6b8089140040f7a6': 'uninterrupted', '570b28ab6b8089140040f7a7': 'Friday, 15 October 1582', '570b28ab6b8089140040f7a8': 'Thursday, 4 October 1582', '5a3717ea95360f001af1b3ff': '', '5a3717ea95360f001af1b400': '', '5a3717ea95360f001af1b401': '', '5a3717ea95360f001af1b402': '', '57301c4fb2c2fd1400568893': 'corruption and political repression', '5a62a976f8d794001af1c199': '', '56beb67d3aeaaa14008c929a': 'beats', '56d4dd502ccc5a1400d832b1': 'melodies and ideas', '56ddd49566d3e219004dad07': 'Quito Astronomical Observatory', '56ddd49566d3e219004dad08': 'National Polytechnic School', '56ddd49566d3e219004dad09': '1873', '572a982b34ae481900deaba3': 'political science', '572a982b34ae481900deaba4': 'Yale University', '572a982b34ae481900deaba7': 'Vietnam Veterans Against the War', '572956496aef051400154d12': 'World War I and World War II', '572956496aef051400154d13': 'Major-General Glyn Charles Anglim Gilbert', '5ad423a1604f3c001a40084d': '', '5ad423a1604f3c001a40084e': '', '572b7eb8be1ee31400cb83ed': '64Zn', '572b7eb8be1ee31400cb83f0': '65Zn', '5acfc84c77cf76001a685f42': '', '5acfc84c77cf76001a685f44': '', '5acfc84c77cf76001a685f45': '', '5726a5daf1498d1400e8e608': 'John Williams', '5726a5daf1498d1400e8e609': 'London Missionary Society', '5726a5daf1498d1400e8e60a': 'headhunting', '5726a5daf1498d1400e8e60b': 'Robert Louis Stevenson', '5726a5daf1498d1400e8e60c': '1894', '5a6246a3f8d794001af1bf2a': '', '5a6246a3f8d794001af1bf2b': '', '5a6246a3f8d794001af1bf2e': '', '570e38eb0dc6ce1900204ea5': '79 CE', '570e38eb0dc6ce1900204ea6': 'yellow', '570e38eb0dc6ce1900204ea8': '1912', '5ad1167a645df0001a2d0d15': '', '5ad1167a645df0001a2d0d16': '', '5ad1167a645df0001a2d0d17': '', '56ddde4a9a695914005b9626': 'Instituts de technologie', '56cd59a162d2951400fa652a': 'iTunes', '56cd59a162d2951400fa652b': 'third-party', '570b26c7ec8fbc190045b886': 'Xbox 360 Dashboard', '570b26c7ec8fbc190045b887': 'AKQA and Audiobrain', '5a70c2358abb0b001a676178': '', '5a70c2358abb0b001a676179': '', '5a70c2358abb0b001a67617a': '', '5a70c2358abb0b001a67617b': '', '56dfa2414a1a83140091ebe0': '85%', '56dfa2414a1a83140091ebe1': '31%', '5acd54e907355d001abf3d75': '', '57264dcd708984140094c1d7': 'Athens', '57264dcd708984140094c1d9': 'west coast of Turkey', '57264dcd708984140094c1db': 'off the coast of northeast Euboea', '571ae53e9499d21900609b9a': 'Eusebius of Vercelli', '5acec23a32bba1001ae4b2f7': '', '5acec23a32bba1001ae4b2f8': '', '5acec23a32bba1001ae4b2f9': '', '5acec23a32bba1001ae4b2fa': '', '56dfc2b77aa994140058e153': 'the landscape', '56dfc2b77aa994140058e154': 'cutting or burning undesirable plants', '56dfc2b77aa994140058e155': 'slash-and-burn', '56dfc2b77aa994140058e156': 'agriculture', '5acd607f07355d001abf3fbd': '', '5acd607f07355d001abf3fc0': '', '572fffb8a23a5019007fcc2a': 'Octavian', '572fffb8a23a5019007fcc2b': '27 BC', '572fffb8a23a5019007fcc2d': 'Octavian', '573367eed058e614000b5a5e': 'shallow', '573367eed058e614000b5a5f': 'a marine reserve', '5a39963a2f14dd001ac72433': '', '5a39963a2f14dd001ac72434': '', '5ad504cb5b96ef001a10a9d2': '', '5ad504cb5b96ef001a10a9d4': '', '5ad504cb5b96ef001a10a9d5': '', '5ad504cb5b96ef001a10a9d6': '', '5ad68fcb191832001aa7b1f3': '', '5ad68fcb191832001aa7b1f4': '', '5ad68fcb191832001aa7b1f5': '', '5ad68fcb191832001aa7b1f6': '', '5731c1260fdd8d15006c6504': 'penicillins and cephalosporins', '5731c1260fdd8d15006c6505': 'polymyxins', '5731c1260fdd8d15006c6506': 'four', '5733b4cf4776f419006610ca': 'bacterial functions or growth processes', '5733b4cf4776f419006610cb': 'penicillins and cephalosporins', '5733b4cf4776f419006610cc': 'polymyxins', '5a65cfcfc2b11c001a425d58': '', '5a65cfcfc2b11c001a425d59': '', '5a65cfcfc2b11c001a425d5a': '', '5a65cfcfc2b11c001a425d5b': '', '56e79ed037bdd419002c4267': 'specifying the name of a location', '56e79ed037bdd419002c4268': 'when DST rules change', '56e79ed037bdd419002c4269': 'two', '5725f1dc271a42140099d356': '1762', '57261ed8ec44d21400f3d921': 'Nash', '57261ed8ec44d21400f3d923': 'Sir William Chambers', '57261ed8ec44d21400f3d924': '1762', '5a7a4c3d17ab25001a8a0490': '', '5a7a4c3d17ab25001a8a0491': '', '5a7a4c3d17ab25001a8a0492': '', '5a7a4c3d17ab25001a8a0494': '', '572e6bacc246551400ce422e': 'predate the punk rock movement', '572e6bacc246551400ce422f': 'Reynolds', '572e6bacc246551400ce4230': '1978 and 1984', '5a270daac93d92001a4003a1': '', '5a270daac93d92001a4003a4': '', '5a282e9bd1a287001a6d0ac6': '', '5a282e9bd1a287001a6d0ac9': '', '5730888c069b531400832165': 'near railway trunk routes', '5730888c069b531400832166': 'Massachusetts Bay Transportation Authority', '5730888c069b531400832167': 'rapid transit, light rail lines or other non-road public transport systems', '5a4e8143755ab9001a10f4a1': '', '57271739f1498d1400e8f387': 'type 2', '57271739f1498d1400e8f389': 'reverse insulin resistance', '57301d1da23a5019007fcdab': 'two', '5ad4d50b5b96ef001a10a256': '', '5ad4d50b5b96ef001a10a257': '', '5ad4d50b5b96ef001a10a259': '', '5ad4d50b5b96ef001a10a25a': '', '57295a38af94a219006aa307': 'fourth largest', '57295a38af94a219006aa308': 'trance', '5a63e5537f3c80001a150b87': '', '5a63e5537f3c80001a150b88': '', '57317628e6313a140071cf60': 'pentatonic', '56d660e91c850414009470d4': 'May 14', '56d660e91c850414009470d5': 'UNICEF', '5727865cf1498d1400e8fad0': '20%', '5727865cf1498d1400e8fad1': '63', '5727865cf1498d1400e8fad3': 'choral and English language courses', '5ad2043fd7d075001a4281fc': '', '5ad2043fd7d075001a4281fd': '', '57338653d058e614000b5c81': '1879', '57338653d058e614000b5c82': 'Rev. William Corby', '57338653d058e614000b5c83': '17th of May', '57338653d058e614000b5c84': 'Washington Hall', '57338653d058e614000b5c85': 'LaFortune Student Center', '5ad3d325604f3c001a3ff26b': '', '5ad3d325604f3c001a3ff26d': '', '5ad3d325604f3c001a3ff26e': '', '56e0a2a3231d4119001ac2f4': 'Stalin', '56e0a2a3231d4119001ac2f5': 'accusations of collaboration with the invaders and separatism', '56e0a2a3231d4119001ac2f6': 'Georgian SSR', '5ace04ac32bba1001ae4996d': '', '5ace04ac32bba1001ae4996f': '', '5ace04ac32bba1001ae49970': '', '5ace04ac32bba1001ae49971': '', '5727657f708984140094dcf7': 'Dante Alighieri', '5727657f708984140094dcf8': 'Latin as well as Italian', '5727657f708984140094dcf9': 'Tuscan', '5727657f708984140094dcfa': 'Decameron', '5727657f708984140094dcfb': 'Petrarch', '5ad02a8077cf76001a686c54': '', '5ad02a8077cf76001a686c55': '', '5ad02a8077cf76001a686c57': '', '5ad02a8077cf76001a686c58': '', '5726083a89a1e219009ac164': 'Tottenham Hotspur', '5726083a89a1e219009ac165': 'North London derbies', '5726083a89a1e219009ac166': 'Manchester United', '5726083a89a1e219009ac168': '2008', '5acd176407355d001abf3440': '', '5acd176407355d001abf3443': '', '56cd81df62d2951400fa6667': 'Electronic Industry Code of Conduct Implementation Group', '56cd81df62d2951400fa6668': 'Foxconn', '56d134c2e7d4791d00901ff7': 'Foxconn', '56d134c2e7d4791d00901ff8': 'Verité', '56d134c2e7d4791d00901ff9': '2006', '56e10dbdcd28a01900c674e4': 'Sun Jiadong', '5acd455207355d001abf3b88': '', '5acd455207355d001abf3b8b': '', '5727b808ff5b5019007d9358': 'Samuel Rutherford', '5727b808ff5b5019007d9359': 'A. V. Dicey', '5a3af1c53ff257001ab84352': '', '5a3af1c53ff257001ab84353': '', '5a3af1c53ff257001ab84355': '', '5726bd64dd62a815002e8eea': 'Nigeria', '5726bd64dd62a815002e8eeb': '182 million', '5726bd64dd62a815002e8eec': 'seventh', '5726bd64dd62a815002e8eed': 'over 500', '5726bd64dd62a815002e8eee': 'English', '570a60076d058f1900182de2': 'fear', '570a60076d058f1900182de3': 'changes in pulse rate', '570a60076d058f1900182de4': 'baring teeth', '570a60076d058f1900182de5': 'Jonathan Turner', '570a60076d058f1900182de6': 'four', '5ad26126d7d075001a429004': '', '5ad26126d7d075001a429006': '', '5ad26126d7d075001a429007': '', '5ad26126d7d075001a429008': '', '56df0c1a3277331400b4d91b': 'the Pope', '5ad2fda5604f3c001a3fda8f': '', '5ad2fda5604f3c001a3fda90': '', '5ad2fda5604f3c001a3fda91': '', '5ad2fda5604f3c001a3fda92': '', '5ad2fda5604f3c001a3fda93': '', '57293faa6aef051400154be8': 'Genetic studies', '5ad4005e604f3c001a3ffccb': '', '5ad4005e604f3c001a3ffccd': '', '5ad4005e604f3c001a3ffcce': '', '5ad4005e604f3c001a3ffccf': '', '5726cb515951b619008f7e4b': 'The National Autonomous University of Mexico', '5726cb515951b619008f7e4c': '300,000', '5726cb515951b619008f7e4e': '74th', '5726cb515951b619008f7e4f': 'Ciudad Universitaria', '572f9c99a23a5019007fc7d3': '2N', '572f9c99a23a5019007fc7d4': 'a three-terminal device', '572f9c99a23a5019007fc7d7': 'a p–n–p germanium switching transistor', '5a7b83a521c2de001afea0e6': '', '5a7b83a521c2de001afea0e7': '', '56f9409c9b226e1400dd12c6': 'Eighth', '56f9409c9b226e1400dd12c7': '1928', '56f9409c9b226e1400dd12c8': 'Cass Gilbert', '5731628fe6313a140071ceb2': '1 mm or less', '5731628fe6313a140071ceb5': 'private devotion', '5727ba3c2ca10214002d94cc': '1863', '5727ba3c2ca10214002d94cd': 'George Armstrong Custer', '5727ba3c2ca10214002d94ce': 'Iron Brigade', '5727ba3c2ca10214002d94cf': '82%', '56f756c6a6d7ea1400e171d6': 'key elements of the music', '56f756c6a6d7ea1400e171d7': 'block-rhythms', '57109988a58dae1900cd6abb': 'natural history', '57109988a58dae1900cd6abc': 'René-Antoine Ferchault de Réaumur', '570969eaed30961900e840ce': 'Himachal Pradesh', '570969eaed30961900e840d0': 'All major English daily newspapers', '570969eaed30961900e840d1': 'Aapka Faisla, Amar Ujala, Panjab Kesari, Divya Himachal', '570969eaed30961900e840d2': 'Radio and TV', '5a3636d5788daf001a5f878c': '', '5a3636d5788daf001a5f878d': '', '5a3636d5788daf001a5f878e': '', '5a3636d5788daf001a5f878f': '', '5732321ce17f3d140042271a': 'absolute head of state', '5727faefff5b5019007d99d0': 'Unicode Roadmap Committee', '5727faefff5b5019007d99d1': 'maintain the list of scripts that are candidates or potential candidates for encoding', '5acd1d2407355d001abf3589': '', '5acd1d2407355d001abf358a': '', '5acd1d2407355d001abf358b': '', '5acd1d2407355d001abf358c': '', '5730eacaaca1c71400fe5b79': '2007', '5730eacaaca1c71400fe5b7a': 'budget constraints', '5730eacaaca1c71400fe5b7b': '330,000', '5730eacaaca1c71400fe5b7c': 'flight hours for crew training', '571aeca132177014007e9fee': '$301 million', '571aeca132177014007e9ff0': 'Geodon', '571d3bc95efbb31900334ed2': 'False Claims Act', '571d3bc95efbb31900334ed3': 'Eli Lilly', '571d3bc95efbb31900334ed4': 'Geodon', '571d3bc95efbb31900334ed5': '$301 million', '5ad3b079604f3c001a3feca7': '', '5ad3b079604f3c001a3feca8': '', '5ad3b079604f3c001a3feca9': '', '5ad3b079604f3c001a3fecab': '', '570c56adfed7b91900d458d8': '1189', '570c56adfed7b91900d458d9': 'fines, court fees and the sale of charters and other privileges', '5728c567ff5b5019007da662': 'JPMorgan Chase', '5728c567ff5b5019007da663': '$25 million', '5728c567ff5b5019007da664': '$12.5 million', '5728c567ff5b5019007da665': '$32 million', '57300a06b2c2fd1400568789': '548,404', '57300a06b2c2fd140056878b': '9,123', '5ad4188a604f3c001a400403': '', '5ad4188a604f3c001a400404': '', '5ad4188a604f3c001a400405': '', '5ad4188a604f3c001a400406': '', '5ad4188a604f3c001a400407': '', '570b221b6b8089140040f768': '2005', '570b221b6b8089140040f769': 'LifeSize Communications', '570b221b6b8089140040f76c': '1280 by 720', '5a1f2bc43de3f40018b26536': '', '5a1f2bc43de3f40018b26537': '', '5a1f2bc43de3f40018b26538': '', '5a1f2bc43de3f40018b26539': '', '5a1f2bc43de3f40018b2653a': '', '5726ad725951b619008f79ef': 'Nepean Island', '5726ad725951b619008f79f0': 'The providence petrel', '5726ad725951b619008f79f1': 'Phillip Island', '5726ad725951b619008f79f3': 'the whale bird', '5a81b79c31013a001a334dd3': '', '5a81b79c31013a001a334dd4': '', '5ad2c97fd7d075001a42a23a': '', '5ad2c97fd7d075001a42a23b': '', '5ad2c97fd7d075001a42a23c': '', '5ad2c97fd7d075001a42a23d': '', '5ad2c97fd7d075001a42a23e': '', '572f90e2a23a5019007fc767': 'downtown St. Louis', '572f90e2a23a5019007fc76a': 'Robert S. Brookings, Henry Ware Eliot, and William Huse', '5ace156432bba1001ae49a6b': '', '5ace156432bba1001ae49a6d': '', '57313831497a881900248c74': 'Niulakita', '5727b5f02ca10214002d9488': 'Beinecke Rare Book and Manuscript Library', '5727b5f02ca10214002d9489': 'Yale University Art Gallery', '5727b5f02ca10214002d948a': 'Yale Center for British Art', '5727b5f02ca10214002d948b': 'Eli Whitney Museum', '5727b5f02ca10214002d948c': 'Yale', '572a470efed8de19000d5b66': 'Yale University Art Gallery', '5730048aa23a5019007fcc4f': 'Iran', '57098392ed30961900e8425e': '2014', '57098392ed30961900e8425f': '1974, 1978 and 1994', '57098392ed30961900e84260': 'Uruguay', '57098392ed30961900e84261': 'sixteen', '57098392ed30961900e84262': '4–1', '59fb34d4ee36d60018400d65': '', '59fb34d4ee36d60018400d66': '', '59fb34d4ee36d60018400d68': '', '572b749abe1ee31400cb83ab': 'transcendental', '572b749abe1ee31400cb83ac': 'Berkeley', '5a7c8ba9e8bc7e001a9e1eb1': '', '5a7c8ba9e8bc7e001a9e1eb2': '', '5a7c8ba9e8bc7e001a9e1eb3': '', '5a7c8ba9e8bc7e001a9e1eb4': '', '5a7b9f0621c2de001afea1e0': '', '5a7b9f0621c2de001afea1e1': '', '5a7b9f0621c2de001afea1e2': '', '56cbedde6d243a140015edf2': 'Souvenir de Paganini', '56cbedde6d243a140015edf5': 'two', '56cbedde6d243a140015edf6': '17 March 1830', '56cf6af94df3c31400b0d761': 'Souvenir de Paganini', '56cf6af94df3c31400b0d762': 'Vienna', '56cf6af94df3c31400b0d764': 'September 1829', '56d315d159d6e41400146222': 'Niccolò Paganini', '56d315d159d6e41400146223': 'Vienna', '56d315d159d6e41400146225': 'three', '57279c2edd62a815002ea1ef': 'five', '57279c2edd62a815002ea1f1': 'anumāṇa', '5a5e51b25bc9f4001a75aee1': '', '5a5e51b25bc9f4001a75aee2': '', '5a5e51b25bc9f4001a75aee3': '', '5a5e51b25bc9f4001a75aee4': '', '5a5e51b25bc9f4001a75aee5': '', '573254ece99e3014001e66c4': 'Secretary of Defense', '573254ece99e3014001e66c5': 'Chairman of the Joint Chiefs of Staff', '573254ece99e3014001e66c6': 'Augusta National Golf Club', '573254ece99e3014001e66c7': 'Columbia Associates', '573254ece99e3014001e66c8': 'July 1949', '572726a2f1498d1400e8f41d': 'General William Tecumseh Sherman', '572726a2f1498d1400e8f41f': 'Equal Protection Clause of the 14th Amendment', '572726a2f1498d1400e8f420': 'President John F. Kennedy', '5ad40ef3604f3c001a400143': '', '5ad40ef3604f3c001a400144': '', '5ad40ef3604f3c001a400146': '', '5727d1683acd2414000ded2e': 'Grande Île', '5727d1683acd2414000ded30': '1988', '5acd583707355d001abf3e0a': '', '5acd583707355d001abf3e0c': '', '5acd583707355d001abf3e0d': '', '5acd583707355d001abf3e0e': '', '5727e8484b864d1900163fc8': '1228', '5727e8484b864d1900163fc9': 'Middle Ages', '56f75c11a6d7ea1400e17205': 'come scritto', '5734296dd058e614000b6a72': 'Great Falls, Lewistown, Cut Bank and Glasgow', '56d0772c234ae51400d9c2f9': 'Buddhacarita, the Lokottaravādin Mahāvastu, and the Sarvāstivādin Lalitavistara Sūtra', '56d0e42e17492d1400aab689': '5th century CE', '56d1c2d2e7d4791d00902120': 'monastic', '570f887880d9841400ab35a2': 'Queen Victoria', '570f887880d9841400ab35a4': '2015', '5ad354ab604f3c001a3fdd89': '', '5ad354ab604f3c001a3fdd8a': '', '5ad354ab604f3c001a3fdd8b': '', '5ad354ab604f3c001a3fdd8c': '', '5ad354ab604f3c001a3fdd8d': '', '56cda50b62d2951400fa67ac': 'Tantalus Media', '56cda50b62d2951400fa67ad': 'Wii U', '56cda50b62d2951400fa67ae': 'November 12, 2015', '56cda50b62d2951400fa67af': 'March 5, 2016', '56d1335f17492d1400aabc15': 'Tantalus Media', '56d1335f17492d1400aabc17': 'March 4, 2016', '5a8db847df8bba001a0f9ba3': '', '5a8db847df8bba001a0f9ba5': '', '572804792ca10214002d9ba0': 'an abstract conceptual framework', '5a7e003d70df9f001a8753fd': '', '5a7e003d70df9f001a8753ff': '', '5a7e003d70df9f001a875400': '', '5a7e003d70df9f001a875401': '', '5a8103d68f0597001ac00229': '', '570d5aabfed7b91900d45f06': '1609', '570d5aabfed7b91900d45f08': 'a third', '570d5aabfed7b91900d45f09': '1613', '56f8a3aa9b226e1400dd0d23': 'Scientists', '56e7909700c9c71400d772e1': 'January 1912', '56e7909700c9c71400d772e2': 'Sun Yat-sen', '570b69c1ec8fbc190045ba01': \"High 'n' Dry\", '570b69c1ec8fbc190045ba03': 'Quiet Riot', '570b69c1ec8fbc190045ba04': '1983', '5a5a3dd89c0277001abe70c2': '', '5a5a3dd89c0277001abe70c4': '', '56dd1e8366d3e219004dabd9': 'Catholics', '56dd1e8366d3e219004dabda': '22.3%', '56dd1e8366d3e219004dabdb': '19.9%', '56dd1e8366d3e219004dabdc': '1.6%', '5ad02ad977cf76001a686c73': '', '5ad02ad977cf76001a686c75': '', '572fc623947a6a140053cc94': 'The Armenian Army, Air Force, Air Defence, and Border Guard', '572fc623947a6a140053cc96': '1992', '570e6b020b85d914000d7eb7': 'north-west', '570e6b020b85d914000d7eb8': 'India and Pakistan', '570e6b020b85d914000d7eb9': 'Indo-Aryan migration theory', '5a2994e803c0e7001a3e17ec': '', '5a2994e803c0e7001a3e17ee': '', '5a2994e803c0e7001a3e17ef': '', '5a2ab45c5b078a001a2f06c3': '', '5a2ab45c5b078a001a2f06c4': '', '5a2ab45c5b078a001a2f06c5': '', '572ebac003f98919007569b0': 'Colorado', '5a0f25d8d7c85000188645bf': '', '5a0f25d8d7c85000188645c1': '', '5726128a89a1e219009ac1eb': 'high-ranking chiefs', '5726128a89a1e219009ac1ed': 'China', '5726128a89a1e219009ac1ee': 'clothing choice', '5a0cf7c8f5590b0018dab6b5': '', '5a0cf7c8f5590b0018dab6b6': '', '5a0cf7c8f5590b0018dab6b7': '', '56fb2e3bf34c681400b0c1f7': 'a third', '572f54dba23a5019007fc551': '1912', '572f54dba23a5019007fc552': 'Sun Yat-sen', '572f54dba23a5019007fc553': 'Yuan Shikai', '572fdf3904bcaa1900d76e20': 'FC Bayern Munich', '572fdf3904bcaa1900d76e21': '44th', '56f74604aef2371900625a89': 'between the 6th and 10th centuries', '56f74604aef2371900625a8a': 'Orthodox Christianity', '56f74604aef2371900625a8c': '11th century', '56f74604aef2371900625a8d': 'Orthodox', '5ad4bab45b96ef001a109e62': '', '5ad4bab45b96ef001a109e63': '', '5ad4bab45b96ef001a109e64': '', '5ad4bab45b96ef001a109e65': '', '5ad4bab45b96ef001a109e66': '', '5ad16905645df0001a2d1a24': '', '5ad16905645df0001a2d1a25': '', '5ad16905645df0001a2d1a26': '', '5ad16905645df0001a2d1a27': '', '5ad16905645df0001a2d1a28': '', '56cbd8c66d243a140015ed85': 'indirect', '56cbd8c66d243a140015ed88': 'films and biographies', '56ce1138aab44d1400b88428': 'France', '56ce1138aab44d1400b88429': 'Poland', '56cf5a5aaab44d1400b890dd': 'France', '56cf5a5aaab44d1400b890de': 'political insurrection', '56cf5a5aaab44d1400b890df': 'Romantic era', '56de708f4396321400ee28df': 'Constantine the Great', '56de708f4396321400ee28e0': 'Edict of Milan', '56de708f4396321400ee28e1': 'Pontifex Maximus', '5a5abce09c0277001abe7163': '', '5a5abce09c0277001abe7164': '', '5a5abce09c0277001abe7166': '', '57325b9fe99e3014001e670b': 'Alan Rogerson', '57325b9fe99e3014001e670e': 'mind control', '5ad3e86a604f3c001a3ff64f': '', '5ad3e86a604f3c001a3ff650': '', '5ad3e86a604f3c001a3ff651': '', '5ad3e86a604f3c001a3ff652': '', '5ad3e86a604f3c001a3ff653': '', '5733f165d058e614000b663d': 'a highly specialized criminal investigation police', '5725d36038643c19005acdb2': '25%', '5725d36038643c19005acdb3': 'white meat', '5728162d4b864d1900164440': 'The Best', '5728162d4b864d1900164441': 'Platinum', '5ad2a8afd7d075001a429e20': '', '5ad2a8afd7d075001a429e21': '', '5ad2a8afd7d075001a429e22': '', '5ad2a8afd7d075001a429e23': '', '5ad33f9a604f3c001a3fdbce': '', '5ad33f9a604f3c001a3fdbcf': '', '56f7c779aef2371900625c09': 'magnates', '56f7c779aef2371900625c0a': 'magnat', '56f7c779aef2371900625c0c': 'możni', '56f7c779aef2371900625c0d': 'Lithuania', '5722ccb20dadf01500fa1ef3': '1861', '5722ccb20dadf01500fa1ef5': 'chronic stomach trouble', '5722ccb20dadf01500fa1ef6': 'army manoeuvres', '5723d010f6b826140030fc8a': '1861', '5723d010f6b826140030fc8b': 'Conroy and Lehzen', '5723d010f6b826140030fc8c': 'Albert', '5723d010f6b826140030fc8e': '1861', '5724d5ba0a492a1900435636': 'March 1861', '5724d5ba0a492a1900435637': 'typhoid fever', '5724d5ba0a492a1900435638': '14 December 1861', '5724d5ba0a492a1900435639': 'widow of Windsor', '57257e8fcc50291900b28536': 'Conroy and Lehzen', '5ad176e7645df0001a2d1d22': '', '5ad176e7645df0001a2d1d23': '', '5ad176e7645df0001a2d1d24': '', '56f95c439b226e1400dd13a8': 'World War II', '56f95c439b226e1400dd13aa': '1944', '56f95c439b226e1400dd13ab': 'one', '56f95c439b226e1400dd13ac': 'Wotje', '5726ed3ddd62a815002e9573': 'Hanja', '572a268f6aef051400155312': '1772', '5a3bf219cc5d22001a521c3c': '', '5a3bf219cc5d22001a521c40': '', '56fb85aab28b3419009f1dfa': 'Pope Gregory VII', '56fb85aab28b3419009f1dfb': '1122', '5726bd86f1498d1400e8e9b1': 'semantic indicator', '5726bd86f1498d1400e8e9b2': 'semantic indicator', '5726593a5951b619008f7037': 'universal compulsory', '5726593a5951b619008f7038': 'females are exempted from conscription', '5726593a5951b619008f7039': 'nine months', '5726593a5951b619008f703a': '18 and 60', '57280c3f3acd2414000df311': 'London Fire and Emergency Planning Authority', '572913111d0469140077901e': 'three', '572913111d0469140077901f': 'Hamburg and Bremen', '5a4736e95fd40d001a27dd75': '', '5a4736e95fd40d001a27dd76': '', '5a4736e95fd40d001a27dd79': '', '5a513e8ece860b001aa3fc7f': '', '5a513e8ece860b001aa3fc80': '', '5a513e8ece860b001aa3fc81': '', '5725e48589a1e219009ac05a': 'Antiochus VII Sidetes', '5725e48589a1e219009ac05c': '66 BC – 217 AD', '5725e48589a1e219009ac05e': 'Iranian', '56e0bc7b231d4119001ac363': 'Vanguard', '56e0bc7b231d4119001ac365': 'Florida', '56e0bc7b231d4119001ac366': 'Jupiter-C rocket', '572ac792111d821400f38d5e': 'Bashar al-Assad', '572ac792111d821400f38d60': 'Russia', '572ac792111d821400f38d62': 'September 28', '57307352069b5314008320ee': 'Sorbonne University', '5ad29b2ed7d075001a429bc2': '', '5ad29b2ed7d075001a429bc3': '', '5ad29b2ed7d075001a429bc4': '', '5ad29b2ed7d075001a429bc5': '', '56df64a68bc80c19004e4bb4': 'teacher training', '572cb837750c471900ed4cf2': 'state courts', '5a79f25b17ab25001a8a01ff': '', '5a79f25b17ab25001a8a0202': '', '56dc544814d3a41400c267bf': 'molecular biology and genetics', '56dc544814d3a41400c267c0': 'DNA', '5a591bbb3e1742001a15cf90': '', '5a591bbb3e1742001a15cf94': '', '57283f892ca10214002da182': 'cheaper', '5728ea364b864d1900165086': 'Kokin Wakashū', '56f8ec5d9e9bad19000a0702': 'India', '56f8ec5d9e9bad19000a0703': 'the Balkans', '5729355b1d0469140077916d': 'forensic', '5726baf5f1498d1400e8e92d': 'The Guardian', '5726baf5f1498d1400e8e92e': 'Nigel Rumfitt QC', '56fb69e38ddada1400cd63f5': 'Christmas Day 800', '56fb69e38ddada1400cd63f6': 'peasants', '56fb69e38ddada1400cd63f7': '300', '56fb69e38ddada1400cd63f8': 'small farms', '56fb69e38ddada1400cd63f9': 'Scandinavia', '5729fcffaf94a219006aa727': 'Albert Einstein', '5729fcffaf94a219006aa729': 'E = mc²', '5acd46d507355d001abf3bcc': '', '5acd46d507355d001abf3bcd': '', '5acd46d507355d001abf3bcf': '', '5acd46d507355d001abf3bd0': '', '56dcdbe566d3e219004dab36': '1500 BC', '56dcdbe566d3e219004dab37': 'Bantu', '5acff76f77cf76001a686694': '', '5acff76f77cf76001a686695': '', '5acff76f77cf76001a686696': '', '5acff76f77cf76001a686697': '', '5726f52bdd62a815002e9643': 'December 2014', '5726f52bdd62a815002e9644': '2012', '5726f52bdd62a815002e9645': 'the University of Nigeria', '572b72e9be1ee31400cb83a1': 'sixth century AD', '572b72e9be1ee31400cb83a2': 'Čech', '572b72e9be1ee31400cb83a4': 'East Francia', '5a7a0eba17ab25001a8a0280': '', '5a7a0eba17ab25001a8a0281': '', '5a7a0eba17ab25001a8a0282': '', '5a7a0eba17ab25001a8a0283': '', '572a2b821d04691400779801': 'two', '572a2b821d04691400779802': 'fundamental', '572a2b821d04691400779804': 'not time-reversal invariant', '5a42cd804a4859001aac7328': '', '5a42cd804a4859001aac7329': '', '57277595708984140094de35': 'Kvarner', '57277595708984140094de36': 'a man-like doll', '57277595708984140094de37': 'Jure Piškanac', '57277595708984140094de39': 'fritule', '5a5e5b755bc9f4001a75af39': '', '5a5e5b755bc9f4001a75af3a': '', '5a5e5b755bc9f4001a75af3d': '', '57095defed30961900e84004': 'biostatic', '57095defed30961900e84006': 'Muntz metal', '57095defed30961900e84007': 'netting materials', '5a836f64e60761001a2eb6e1': '', '5a836f64e60761001a2eb6e2': '', '5a836f64e60761001a2eb6e3': '', '5a836f64e60761001a2eb6e4': '', '57279d21ff5b5019007d910e': '1896', '57279d21ff5b5019007d9112': '1893', '5a8c8ad7fd22b3001a8d8a6e': '', '5a8c8ad7fd22b3001a8d8a6f': '', '5a8c8ad7fd22b3001a8d8a70': '', '5a8c8ad7fd22b3001a8d8a71': '', '5a8c8ad7fd22b3001a8d8a72': '', '570a6c176d058f1900182e4d': 'their parental germ cells', '570a6c176d058f1900182e4e': '1.2%', '5ad255afd7d075001a428d46': '', '5ad255afd7d075001a428d47': '', '5ad255afd7d075001a428d49': '', '5ad255afd7d075001a428d4a': '', '572a22256aef0514001552fb': 'Battle of Vienna', '572a22256aef0514001552fc': 'Europe', '57261a8e38643c19005acff3': 'Praxagoras of Kos', '57261a8e38643c19005acff4': 'Herophilos', '57261a8e38643c19005acff6': 'Herophilos', '570d9a31df2f5219002ed012': 'Germany', '57316532a5e9cc1400cdbf17': 'Poetry', '57316532a5e9cc1400cdbf18': 'women', '57316532a5e9cc1400cdbf1a': 'Six Chapters of a Floating Life', '57316532a5e9cc1400cdbf1b': 'Cao Xueqin', '572668e2708984140094c519': 'Vanity Fair', '57300da0947a6a140053cffc': 'Roman Empire', '57300da0947a6a140053cffd': '9th century', '57300da0947a6a140053cffe': 'Northern Ireland', '5acd1c9c07355d001abf357f': '', '5acd1c9c07355d001abf3580': '', '5acd1c9c07355d001abf3581': '', '5acd6a7807355d001abf4127': '', '56df6b9d56340a1900b29ae4': 'Nasrani', '56df6b9d56340a1900b29ae6': 'Isa Masih', '5ad2de1cd7d075001a42a57c': '', '5ad2de1cd7d075001a42a57f': '', '5730b1488ab72b1400f9c6a4': 'Super Scope', '5730b1488ab72b1400f9c6a6': 'Mario Paint', '5730b1488ab72b1400f9c6a7': 'light gun', '5a42e7df4a4859001aac7393': '', '5a42e7df4a4859001aac7395': '', '5a42e7df4a4859001aac7396': '', '5a42e7df4a4859001aac7397': '', '5728cbea3acd2414000dfeab': 'Detroit', '5728cbea3acd2414000dfead': 'Movement', '5728cbea3acd2414000dfeaf': 'Hart Plaza', '5726ccbedd62a815002e909a': 'May', '5726ccbedd62a815002e909b': '15 November 2009', '5726ccbedd62a815002e909c': 'Roger', '5726ccbedd62a815002e909d': 'USA', '572a5f33b8ce0319002e2ae8': 'Eritrea, Tunisia, Algiers, the Balkans and Romania', '56e9644c0b45c0140094cdf0': '1,185', '570b609e6b8089140040f8ea': 'Canadian', '570b609e6b8089140040f8ec': 'Irish', '570b609e6b8089140040f8ee': 'Germany', '5a5a32199c0277001abe70a2': '', '5a5a32199c0277001abe70a4': '', '5a5a32199c0277001abe70a6': '', '56ddd24566d3e219004dad05': '1997', '572a8990f75d5e190021fb57': 'Jamia Nayeemia Muradabad', '572a8990f75d5e190021fb5a': 'Hindu-Muslim friction', '5ace820b32bba1001ae4a8cb': '', '5ace820b32bba1001ae4a8cc': '', '5ace820b32bba1001ae4a8cd': '', '5ace820b32bba1001ae4a8ce': '', '5ace820b32bba1001ae4a8cf': '', '57277e4edd62a815002e9eb6': 'traditions', '57277e4edd62a815002e9eb8': 'boerenbruiloft', '570e432f0dc6ce1900204ee1': 'high-density penetrators', '570e432f0dc6ce1900204ee4': 'Persian Gulf', '570e432f0dc6ce1900204ee5': 'Gulf War Syndrome', '5ad113dc645df0001a2d0c8a': '', '5ad113dc645df0001a2d0c8b': '', '5ad113dc645df0001a2d0c8c': '', '5732549b0fdd8d15006c69c7': 'Council on Foreign Relations', '572ba616111d821400f38f34': 'Nouns', '572ba616111d821400f38f35': 'any case', '572ba616111d821400f38f36': 'genitive', '5a7a316817ab25001a8a038e': '', '5a7a316817ab25001a8a038f': '', '5a7a316817ab25001a8a0390': '', '5a7a316817ab25001a8a0391': '', '5a7a316817ab25001a8a0392': '', '56dcfc7b66d3e219004dab7d': 'December 30, 2010', '5ad014c177cf76001a686916': '', '5ad014c177cf76001a686918': '', '5ad014c177cf76001a68691a': '', '5ad0d272645df0001a2d054c': '', '5acd71d807355d001abf428f': '', '5acd71d807355d001abf4290': '', '5acd71d807355d001abf4291': '', '56dff3c2231d4119001abeed': 'The Rovers Return', '56dff3c2231d4119001abeee': 'The Queen Vic', '56dff3c2231d4119001abeef': 'BBC One', '56dff3c2231d4119001abef0': 'ITV', '56ddcf2b66d3e219004dacf5': 'Affiliate Schools', '5726a3fc5951b619008f78b9': 'Angela Merkel', '57279c1aff5b5019007d90e8': 'Saint Finbarr', '57279c1aff5b5019007d90e9': '6th century', '5a5d0b835e8782001a9d5e70': '', '5a5d0b835e8782001a9d5e71': '', '5a5d0b835e8782001a9d5e72': '', '5a7e45da70df9f001a87566d': '', '5a7e45da70df9f001a87566e': '', '5a7e45da70df9f001a87566f': '', '5a7e45da70df9f001a875670': '', '5a7e45da70df9f001a875671': '', '56de71b2cffd8e1900b4b8fe': 'Sir Thomas More and Cardinal John Fisher', '56de71b2cffd8e1900b4b8ff': 'Edward VI', '56de71b2cffd8e1900b4b900': '1612', '5a5ad1eb9c0277001abe71b0': '', '5a5ad1eb9c0277001abe71b2': '', '5a5ad1eb9c0277001abe71b3': '', '5a5ad1eb9c0277001abe71b4': '', '56f8dbf69e9bad19000a0610': 'Cunard Line', '56f8dbf69e9bad19000a0611': 'RMS Queen Elizabeth 2', '56f8dbf69e9bad19000a0612': 'HRH The Duchess of Cornwall', '56f8dbf69e9bad19000a0613': '2011', '56f8dbf69e9bad19000a0614': 'Royal Princess', '572f2ce3a23a5019007fc4b4': '25 kV 50 Hz AC', '5acd72be07355d001abf42c0': '', '5acd72be07355d001abf42c1': '', '5acd72be07355d001abf42c2': '', '570a55836d058f1900182d66': 'South Kensington', '570a55836d058f1900182d67': 'Albertopolis', '570a55836d058f1900182d69': 'Thomas Collcutt', '570a55836d058f1900182d6a': \"Queen's Tower\", '5a4860a984b8a4001a7e7856': '', '5a4860a984b8a4001a7e7857': '', '5a4860a984b8a4001a7e7859': '', '5723df4df6b826140030fcce': 'Recognition', '5726225d271a42140099d4c4': 'Recognition', '5726225d271a42140099d4c6': 'inter-visitation', '5726225d271a42140099d4c7': 'Exclusive Jurisdiction and Regularity', '5acf801b77cf76001a684fe6': '', '5acf801b77cf76001a684fe7': '', '5acf801b77cf76001a684fe8': '', '5acf801b77cf76001a684fe9': '', '5acf801b77cf76001a684fea': '', '56beabab3aeaaa14008c91db': 'Vogue', '56bfafdba10cfb140055123d': 'Vogue', '56bfafdba10cfb140055123e': 'April 2013', '56bfafdba10cfb140055123f': 'Ban Bossy', '56bfafdba10cfb1400551240': 'Flawless', '56bfafdba10cfb1400551241': 'leadership in girls', '56d4d3b12ccc5a1400d83277': 'Chimamanda Ngozi Adichie', '56d4d3b12ccc5a1400d83278': 'Ban Bossy', '56de5ba04396321400ee284d': 'two', '56de5ba04396321400ee284e': 'Loughborough University of Technology', '570a6f996d058f1900182e5d': 'Aristotle', '570a6f996d058f1900182e5e': 'passions', '570a6f996d058f1900182e5f': 'Thomas Aquinas', '5ad24569d7d075001a428a9a': '', '5ad24569d7d075001a428a9b': '', '5ad24569d7d075001a428a9c': '', '5ad24569d7d075001a428a9d': '', '572f6c2cb2c2fd14005680f7': 'Sultan Alauddin Khilji', '572f6c2cb2c2fd14005680fa': '1325', '570d6cf1fed7b91900d460a5': 'breaking up', '570d6cf1fed7b91900d460a6': 'General Canrobert', '570d6cf1fed7b91900d460a8': 'Rezonville', '571ae71a32177014007e9fca': 'Line-extensions', '571d1d495efbb31900334ea9': 'biotechnology', '571d1d495efbb31900334eaa': 'major pharmaceutical multinationals', '5ad399f2604f3c001a3fe843': '', '5ad399f2604f3c001a3fe844': '', '5ad399f2604f3c001a3fe845': '', '5ad399f2604f3c001a3fe846': '', '5726fc5d5951b619008f840f': 'Joséphine de Beauharnais', '5726fc5d5951b619008f8410': '1796', '5726fc5d5951b619008f8411': '26', '5726fc5d5951b619008f8413': 'Rose', '57286d4b4b864d19001649dc': '1979', '5ad13b7e645df0001a2d1301': '', '5ad13b7e645df0001a2d1303': '', '572f6eacb2c2fd140056810b': 'spiral', '572f6eacb2c2fd140056810c': 'photodiode', '5a567d306349e2001acdcdc4': '', '5a567d306349e2001acdcdc5': '', '5a567d306349e2001acdcdc6': '', '5a567d306349e2001acdcdc7': '', '5a567d306349e2001acdcdc8': '', '5735b062dc94161900571f24': 'four', '5735b062dc94161900571f27': 'casinos', '5a4ebffaaf0d07001ae8cc2f': '', '5a4ebffaaf0d07001ae8cc30': '', '5a4ebffaaf0d07001ae8cc31': '', '572f21e6b2c2fd1400567f3f': 'aerial circumnavigation of the world', '572f21e6b2c2fd1400567f40': '1960s', '5a2d5455f28ef0001a52648b': '', '5a2d5455f28ef0001a52648c': '', '5a2d5455f28ef0001a52648d': '', '5a2d5455f28ef0001a52648f': '', '56de43294396321400ee2736': 'unavailability of certain crucial data', '5ad0cfc9645df0001a2d0483': '', '5ad0cfc9645df0001a2d0484': '', '5731fe53e17f3d14004225bb': '1 April 1942', '5731fe53e17f3d14004225bd': 'Harry Hopkins', '5731fe53e17f3d14004225be': 'Washington', '5731fe53e17f3d14004225bf': 'Portuguese Timor', '56e030597aa994140058e32d': \"St. James' Church\", '56e030597aa994140058e32e': 'Plantation House', '570cf05db3d812140066d34a': 'the small intestine', '570cf05db3d812140066d34d': 'a triglyceride', '573236d2e17f3d1400422736': '361', '573236d2e17f3d1400422738': \"Jerusalem's temple\", '57272cf65951b619008f868d': 'Truman', '57272cf65951b619008f868e': '10,000', '57272cf65951b619008f868f': 'Lincoln Memorial', '57272cf65951b619008f8690': 'equality of opportunity', '5ad4105d604f3c001a4001a3': '', '57277dfddd62a815002e9eac': '1879', '57277dfddd62a815002e9ead': 'over 60', '57277dfddd62a815002e9eae': '1954', '57277dfddd62a815002e9eaf': 'Ann Arbor Civic Ballet', '5ace3ac232bba1001ae49f79': '', '5ace3ac232bba1001ae49f7a': '', '5ace3ac232bba1001ae49f7b': '', '5ace3ac232bba1001ae49f7d': '', '572fb2fb947a6a140053cbaa': 'Nikita Khruschev', '572fb2fb947a6a140053cbab': '1953', '572fb2fb947a6a140053cbac': '1955', '572fb2fb947a6a140053cbae': '1967', '57099d82ed30961900e84382': 'October 3, 2010', '57099d82ed30961900e84383': \"The Action Plan 2010–2015 for Canada's Cyber Security Strategy\", '5a557754134fea001a0e1ab5': '', '5a557754134fea001a0e1ab6': '', '5a557754134fea001a0e1ab8': '', '5a5cfad65e8782001a9d5e36': '', '5a5cfad65e8782001a9d5e38': '', '5a5cfad65e8782001a9d5e39': '', '5a5cfad65e8782001a9d5e3a': '', '570d7be1b3d812140066d9e0': 'Archbishop of Paris', '570d7be1b3d812140066d9e1': 'government buildings', '570d7be1b3d812140066d9e2': 'Tuileries Palace', '570d7be1b3d812140066d9e3': 'between 6,000 and 10,000', '570629ba52bb891400689914': 'bit rate', '570629ba52bb891400689915': 'input', '570629ba52bb891400689916': 'Compact Disc', '5730b108069b53140083226a': 'Eight', '56cddcae62d2951400fa6915': 'Alessandro Cremona', '56cddcae62d2951400fa6916': 'Stephanie Sigman', '56cddcae62d2951400fa6917': 'February 2015', '56cf45bcaab44d1400b88ef3': 'Mexico', '56cf45bcaab44d1400b88ef5': 'Alessandro Cremona', '56cf45bcaab44d1400b88ef6': 'Estrella', '5ad22be1d7d075001a42860b': '', '56d9b546dc89441400fdb713': 'run away', '5730e777aca1c71400fe5b45': 'U.S. War Department', '5730e777aca1c71400fe5b48': 'U.S. Army Air Forces', '5730e777aca1c71400fe5b49': 'President Harry S Truman', '5727891b708984140094e032': '3,000', '5727891b708984140094e034': 'blows to the head', '5727891b708984140094e035': 'Mukhtar Shakhanov', '572805f5ff5b5019007d9b1b': 'UTF-8', '5acd211a07355d001abf35fe': '', '5acd211a07355d001abf35ff': '', '5acd211a07355d001abf3600': '', '5acd211a07355d001abf3602': '', '56e7860900c9c71400d77231': 'more than two and a half centuries', '56e7860900c9c71400d77232': 'Nanjing', '56fb7c108ddada1400cd6458': 'Genoa', '56fb7c108ddada1400cd645a': 'double-entry bookkeeping', '57313a17497a881900248c91': 'Dzungar–Qing War', '57313a17497a881900248c92': '1683', '570c5a9bfed7b91900d4590f': 'late 12th and early 13th centuries', '570c5a9bfed7b91900d45910': 'Henry II', '570c5a9bfed7b91900d45911': 'Treaty of Norham', '572a3b61af94a219006aa8e5': '1.5 million', '572a8395111d821400f38b90': 'Metrorail', '572a8395111d821400f38b92': '23', '5ad3984c604f3c001a3fe7ed': '', '5ad3984c604f3c001a3fe7ee': '', '5ad3984c604f3c001a3fe7ef': '', '573036ea947a6a140053d2bc': 'The Merge', '573036ea947a6a140053d2bd': 'The South Bay Expressway', '573036ea947a6a140053d2be': 'Terminal 2', '573036ea947a6a140053d2c0': '37 percent', '5ad4dbaa5b96ef001a10a452': '', '5ad4dbaa5b96ef001a10a453': '', '5ad4dbaa5b96ef001a10a454': '', '56dfb0e97aa994140058dfe5': 'metal detector', '56dfb0e97aa994140058dfe6': 'bullet', '570a5bbf4103511400d59658': '£822.0 million', '570a5bbf4103511400d59659': '£754.9 million', '570a5bbf4103511400d5965a': '£329.5 million', '570a5bbf4103511400d5965c': '£124 million', '5a4869b284b8a4001a7e7868': '', '5a4869b284b8a4001a7e7869': '', '57313c1205b4da19006bcf02': 'Bronze age', '57313c1205b4da19006bcf03': 'Albania', '57313c1205b4da19006bcf04': 'Pergamon', '56e141e2e3433e1400422d05': '15.8%', '56e141e2e3433e1400422d08': 'Over 27,000', '56e06d44231d4119001ac103': 'Wu Chinese', '56e06d44231d4119001ac106': 'slack or breathy', '5acd2a8c07355d001abf378d': '', '5acd2a8c07355d001abf378e': '', '5acd2a8c07355d001abf378f': '', '5acd2a8c07355d001abf3790': '', '56d08e3e234ae51400d9c384': 'Solar chemical processes', '56d08e3e234ae51400d9c385': 'artificial photosynthesis', '5726bd78dd62a815002e8ef4': '10,000', '5726bd78dd62a815002e8ef5': 'Spanish and English', '5726bd78dd62a815002e8ef6': '2007', '5726bd78dd62a815002e8ef7': 'giant, high definition screens', '5709a2e5200fba1400368201': 'during the manufacturing process', '5709a2e5200fba1400368203': 'insecurity', '5a55606b134fea001a0e1a68': '', '5a55606b134fea001a0e1a69': '', '5a55606b134fea001a0e1a6a': '', '5a5cd75a5e8782001a9d5ddc': '', '5a5cd75a5e8782001a9d5ddd': '', '5a5cd75a5e8782001a9d5ddf': '', '5a5cd75a5e8782001a9d5de0': '', '572713ce708984140094d966': '10', '572713ce708984140094d967': '1st century BC', '572713ce708984140094d968': 'The Nine Chapters on the Mathematical Art', '572713ce708984140094d969': 'Cubic equations', '56fa2008f34c681400b0bfc8': 'Populus', '56fa2008f34c681400b0bfc9': 'cherry', '56fa2008f34c681400b0bfca': 'water conducting', '5ad31659604f3c001a3fdb4d': '', '5ad31659604f3c001a3fdb4e': '', '5ad31659604f3c001a3fdb50': '', '572fddea947a6a140053cd7c': 'Lake Sevan', '572fddea947a6a140053cd7d': 'chess, weightlifting and wrestling', '572fddea947a6a140053cd7f': 'Pan-Armenian Games', '570b3f0fec8fbc190045b910': 'January 2002', '570b3f0fec8fbc190045b911': 'al-Qaida', '570b3f0fec8fbc190045b912': 'the Sulu Archipelago', '570b3f0fec8fbc190045b914': 'Abu Sayyaf', '5ad178b7645df0001a2d1d7a': '', '5ad178b7645df0001a2d1d7e': '', '5731417005b4da19006bcf5e': 'Polish, Hungarian and Lithuanian', '5731417005b4da19006bcf5f': 'Prince Roman Mstislavich', '5731417005b4da19006bcf60': '1202', '5ad0185377cf76001a6869ae': '', '5ad0185377cf76001a6869af': '', '5ad0185377cf76001a6869b0': '', '5ad0185377cf76001a6869b2': '', '56f8c2519e9bad19000a0446': 'Sunday', '56f8c2519e9bad19000a0447': 'two', '56f8c2519e9bad19000a0449': 'Southampton and District Sunday Football League', '56e136e7cd28a01900c676bc': 'tidal areas', '56e136e7cd28a01900c676bd': 'Trimountain', '56e78bb537bdd419002c410a': 'Borneo', '5a6b5ebba9e0c9001a4e9f4e': '', '5a6b5ebba9e0c9001a4e9f4f': '', '5a6b5ebba9e0c9001a4e9f51': '', '56cfb1a2234ae51400d9be87': 'one out of four', '56cfb1a2234ae51400d9be88': '24', '5731e07b0fdd8d15006c65e5': 'Islam, Christianity, and Judaism', '56fb2c94f34c681400b0c1eb': '1000', '56fb2c94f34c681400b0c1ed': 'Manorialism', '56fb2c94f34c681400b0c1ee': 'feudalism', '56fb2c94f34c681400b0c1ef': '1095', '5728484dff5b5019007da0ce': '2 February 1207', '5728484dff5b5019007da0cf': 'Livonian Brothers of the Sword', '5728484dff5b5019007da0d0': '1237', '5728484dff5b5019007da0d1': '1346', '5728484dff5b5019007da0d2': 'German rule', '570fe7425ab6b819003910b4': '1957', '570fe7425ab6b819003910b5': '1984', '570fe7425ab6b819003910b6': '1978', '570fe7425ab6b819003910b7': 'Rhode Island', '5ad3f120604f3c001a3ff85b': '', '5ad3f120604f3c001a3ff85c': '', '5ad3f120604f3c001a3ff85d': '', '57267ab2dd62a815002e868a': 'New York, Los Angeles and Chicago', '57267ab2dd62a815002e868c': 'the Marvel Universe', '57267ab2dd62a815002e868d': 'Spider-Man and the Fantastic Four', '5a5fa27aeae51e001ab14b69': '', '5a5fa27aeae51e001ab14b6a': '', '5a5fa27aeae51e001ab14b6b': '', '5726fd98708984140094d7d1': 'Schmoelders', '5aceaec232bba1001ae4b027': '', '5aceaec232bba1001ae4b028': '', '5aceaec232bba1001ae4b029': '', '5aceaec232bba1001ae4b02b': '', '5728f05e3acd2414000e0235': '72.1 million', '5728f05e3acd2414000e0236': 'Notre Dame Cathedral', '5728f05e3acd2414000e0237': '9.2 million', '5728f05e3acd2414000e0238': 'Disneyland Paris', '572819473acd2414000df485': 'wood', '572819473acd2414000df486': '1892', '5a6271a6f8d794001af1bfda': '', '5a6271a6f8d794001af1bfdb': '', '5726ee62dd62a815002e9585': 'competing paradigms', '5726a7b4dd62a815002e8c25': 'greater local control', '5a21d8f78a6e4f001aa08f6e': '', '5a21d8f78a6e4f001aa08f6f': '', '5a21d8f78a6e4f001aa08f70': '', '5a21d8f78a6e4f001aa08f71': '', '5a21d8f78a6e4f001aa08f72': '', '570fa4675ab6b81900390f5e': 'as independent concepts on a separate scale', '5728c1414b864d1900164d58': 'Loxias', '5728c1414b864d1900164d59': 'Musagetes', '5728c1414b864d1900164d5a': 'Manticus', '5726a1d5708984140094cc63': 'Annual Register', '5726a1d5708984140094cc66': '1789', '5ad0b591645df0001a2d0106': '', '5ad0b591645df0001a2d0107': '', '5ad0b591645df0001a2d0109': '', '5ad0b591645df0001a2d010a': '', '56f8997b9b226e1400dd0c93': 'Deoxyribonucleic acid (DNA)', '56f8997b9b226e1400dd0c94': 'Rosalind Franklin', '56f8997b9b226e1400dd0c95': 'James D. Watson and Francis Crick', '56f8997b9b226e1400dd0c96': 'reverse transcription in retroviruses', '56f8997b9b226e1400dd0c97': 'molecular genetics', '57320ba7e99e3014001e6479': 'neighbours', '57320ba7e99e3014001e647b': 'Etruscan', '57320ba7e99e3014001e647c': 'Jupiter, Juno and Minerva', '56f71740711bf01900a44932': 'two', '56f71740711bf01900a44936': 'bilateral', '57278fc7dd62a815002ea070': '59 Club', '57278fc7dd62a815002ea073': \"Eton Manor Boys' Club\", '5ad20a18d7d075001a42822c': '', '5ad20a18d7d075001a42822d': '', '5ad20a18d7d075001a42822e': '', '5ad20a18d7d075001a42822f': '', '5ad20a18d7d075001a428230': '', '56e02a437aa994140058e2dd': '1980s', '5acf875d77cf76001a685102': '', '5acf875d77cf76001a685103': '', '5acf875d77cf76001a685105': '', '56fb879b8ddada1400cd64cf': '1347', '56fb879b8ddada1400cd64d0': '35', '56fb879b8ddada1400cd64d3': 'Florence', '57326cefe99e3014001e67a6': '1989', '56e166ffcd28a01900c67878': 'Maxwell Anderson', '56e166ffcd28a01900c67879': '1971', '56e166ffcd28a01900c6787a': 'Richard Burton', '56e166ffcd28a01900c6787b': 'Rooster Cogburn', '5ad155de645df0001a2d17e4': '', '5726c709f1498d1400e8eb03': '1993', '5726c709f1498d1400e8eb05': 'Tai-lo', '5726c709f1498d1400e8eb06': '2007', '5a1e1aa53de3f40018b264c8': '', '5a1e1aa53de3f40018b264c9': '', '5a1e1aa53de3f40018b264ca': '', '56fc88d898e8fc14001ea7d1': 'sounds', '56fc88d898e8fc14001ea7d2': 'brain', '56fc88d898e8fc14001ea7d3': 'allophones', '5a82122f31013a001a3351a8': '', '5a82122f31013a001a3351a9': '', '572837e7ff5b5019007d9f48': 'Boris Yeltsin', '572837e7ff5b5019007d9f49': 'Vladimir Putin', '5acfafea77cf76001a685870': '', '5acfafea77cf76001a685871': '', '5acfafea77cf76001a685872': '', '5acfafea77cf76001a685873': '', '57240e580a492a1900435609': '1832', '57240e580a492a190043560a': '122', '57240e580a492a190043560b': 'Princess Beatrice', '5725677c69ff041400e58c6b': 'Princess Beatrice', '5725677c69ff041400e58c6d': 'Lord Esher', '5725677c69ff041400e58c6e': 'Giles St Aubyn', '5ad17f1f645df0001a2d1e4b': '', '5ad17f1f645df0001a2d1e4c': '', '5ad17f1f645df0001a2d1e4e': '', '5726d4705951b619008f7f55': '1988', '5726d4705951b619008f7f56': 'World Bank and the International Monetary Fund', '5726d4705951b619008f7f57': '1988 to 1996', '5726d4705951b619008f7f59': '20', '5a284fbcd1a287001a6d0b4a': '', '5a284fbcd1a287001a6d0b4b': '', '5a284fbcd1a287001a6d0b4c': '', '5a284fbcd1a287001a6d0b4d': '', '5a284fbcd1a287001a6d0b4e': '', '5728c7dd2ca10214002da7ac': 'famines', '5728c7dd2ca10214002da7ad': '6.1 million to 10.3 million', '5728c7dd2ca10214002da7ae': '1876–78', '5728c7dd2ca10214002da7af': '1.25 to 10 million', '5728c7dd2ca10214002da7b0': 'Third Plague Pandemic', '5a2716bbc93d92001a4003cb': '', '5a2716bbc93d92001a4003cc': '', '5a0dfd6ed7c85000188644b5': '', '5a0dfd6ed7c85000188644b6': '', '5a0dfd6ed7c85000188644b8': '', '570b27b4ec8fbc190045b890': 'Ms. Pac-Man', '570b27b4ec8fbc190045b891': 'Assault Heroes', '570b27b4ec8fbc190045b892': 'November 3, 2004', '570b27b4ec8fbc190045b893': 'November 22, 2005', '5a70d3ef8abb0b001a6761bb': '', '5a70d3ef8abb0b001a6761bc': '', '5a70d3ef8abb0b001a6761bd': '', '5a70d3ef8abb0b001a6761be': '', '5a70d3ef8abb0b001a6761bf': '', '57284af44b864d19001648d0': 'Ilyas Kashmiri', '57284af44b864d19001648d1': 'Harkat-ul-Jihad al-Islami', '57284af44b864d19001648d4': 'mobilisation of resources for continuation of jihad in Kashmir', '5a85e99bb4e223001a8e72d6': '', '5a85e99bb4e223001a8e72d7': '', '5a85e99bb4e223001a8e72d8': '', '5a85e99bb4e223001a8e72d9': '', '573387acd058e614000b5cb1': 'Knute Rockne', '573387acd058e614000b5cb2': '105', '573387acd058e614000b5cb3': '13', '573387acd058e614000b5cb4': 'three', '573387acd058e614000b5cb5': '1925', '5728be814b864d1900164d3e': 'Latvia and Lithuania', '5728be814b864d1900164d40': 'Nordic-Baltic Eight', '5728be824b864d1900164d41': '1989', '572805304b864d1900164250': '8 December 1962', '572805304b864d1900164252': '12 November 1962', '5a611100e9e1cc001a33ce88': '', '5a611100e9e1cc001a33ce8c': '', '572fefcb947a6a140053ce2f': '121 BC', '56ce35b2aab44d1400b885b4': 'mental illness', '56ce35b2aab44d1400b885b5': 'lawyer', '57288f642ca10214002da46e': 'Over three quarters', '57288f642ca10214002da470': 'over 60', '57288f642ca10214002da472': 'United States Department of Education', '5acea68932bba1001ae4aef2': '', '5706b4af0eeca41400aa0d5b': 'Cybotron', '5706b4af0eeca41400aa0d5f': 'Tony Wilson', '5ad28d6fd7d075001a429a2d': '', '57267786dd62a815002e85f2': 'Metrobús', '57267786dd62a815002e85f3': 'Ecobici', '57267786dd62a815002e85f6': 'biannual', '572814c34b864d1900164420': 'TGS 2007', '5ad2a7a2d7d075001a429e13': '', '5ad2a7a2d7d075001a429e14': '', '5ad2a7a2d7d075001a429e15': '', '5ad33f45604f3c001a3fdbc2': '', '5ad33f45604f3c001a3fdbc3': '', '5ad33f45604f3c001a3fdbc4': '', '57303815947a6a140053d2c6': '1960s', '57303815947a6a140053d2ca': 'Tehran World Festival', '56cf7f014df3c31400b0d85c': 'Jewish people', '56d126e117492d1400aaba96': 'November 26, 2013', '56d126e117492d1400aaba97': 'December 21, 2013', '570e08690dc6ce1900204d98': 'Sir James Weddell', '570e08690dc6ce1900204d99': 'British sealing expeditions', '570e08690dc6ce1900204d9a': 'Antarctic krill', '5ad2623cd7d075001a429088': '', '5ad2623cd7d075001a429089': '', '5ad2623cd7d075001a42908b': '', '5ad2623cd7d075001a42908c': '', '5ad2c367d7d075001a42a132': '', '5ad2c367d7d075001a42a133': '', '5725cc5938643c19005acd25': 'The Solís Theatre', '5725cc5938643c19005acd26': '1856', '5725cc5938643c19005acd28': '1998', '5725cc5938643c19005acd29': '2004', '5731bb10b9d445190005e4cf': 'Christians', '5731bb10b9d445190005e4d1': 'Scottish Catholic', '5731bb10b9d445190005e4d2': 'religious test', '5731bb10b9d445190005e4d3': '1799', '5ad141b5645df0001a2d13f8': '', '5ad141b5645df0001a2d13f9': '', '5ad141b5645df0001a2d13fa': '', '572749fdf1498d1400e8f5aa': 'Eli Whitney', '572749fdf1498d1400e8f5ac': 'Whitneyville', '572749fdf1498d1400e8f5ae': 'Samuel Colt', '572948091d04691400779249': 'Eli Whitney', '572948091d0469140077924a': 'Whitney Avenue', '572948091d0469140077924c': 'The Arsenal of America', '572948091d0469140077924d': '1836', '572a329aaf94a219006aa885': 'drought or pests', '572a329aaf94a219006aa887': 'agrarian communities', '5a7d388a70df9f001a875017': '', '5a7d388a70df9f001a875018': '', '5a7d388a70df9f001a87501a': '', '5a7d388a70df9f001a87501b': '', '57098140200fba14003680cd': '3 ppm', '57098140200fba14003680cf': '30 mg/kg', '5a8371c6e60761001a2eb71b': '', '5a8371c6e60761001a2eb71e': '', '5a8371c6e60761001a2eb71f': '', '572771b85951b619008f8a09': '2007', '5a81a8b031013a001a334d31': '', '5a81a8b031013a001a334d33': '', '5a81a8b031013a001a334d34': '', '57267c335951b619008f7457': 'November 2, 2014', '57267c335951b619008f7458': 'Jon Lester', '57267c335951b619008f7459': '$155 million', '57267c335951b619008f745a': '97–65', '570d3bd1fed7b91900d45d65': '18th century', '570d3bd1fed7b91900d45d66': 'Gregory Maians and Perez Bayer', '570d3bd1fed7b91900d45d67': '1776', '570d3bd1fed7b91900d45d68': 'woven silk and ceramic tiles', '570d3bd1fed7b91900d45d69': 'Charles III', '57260e5b271a42140099d405': 'traditional', '57260e5b271a42140099d407': 'Euhemerism', '57260e5b271a42140099d408': 'Epicurus', '5727ec41ff5b5019007d9890': 'Europe', '5727ec41ff5b5019007d9892': 'US', '573036c4947a6a140053d2b2': 'traveling wave antennas', '57283ac5ff5b5019007d9f90': 'the judicial system of courts', '5acfb20177cf76001a685908': '', '5acfb20177cf76001a685909': '', '5acfb20177cf76001a68590a': '', '5acfb20177cf76001a68590b': '', '5725cedc38643c19005acd57': 'Herbert Chapman', '5725cedc38643c19005acd58': 'revolutionary', '5725cedc38643c19005acd59': '1930s', '5725cedc38643c19005acd5b': '1930 FA Cup Final', '5acd005507355d001abf3159': '', '5acd005507355d001abf315a': '', '5728d7c4ff5b5019007da7f6': 'Medicaid', '5a503d98ce860b001aa3fb1f': '', '5a503d98ce860b001aa3fb20': '', '5a503d98ce860b001aa3fb21': '', '56dcf8b79a695914005b94a6': 'multi-party', '56dcf8b79a695914005b94a7': 'Congolese Labour Party', '56dcf8b79a695914005b94a8': 'Parti Congolais du Travail', '5ad00faf77cf76001a686849': '', '5ad00faf77cf76001a68684a': '', '5acd893b07355d001abf4637': '', '5acd893b07355d001abf4638': '', '5acd893b07355d001abf4639': '', '5728283a2ca10214002d9f70': 'fused bundles of cilia', '5728283a2ca10214002d9f72': 'polychaetes', '5ace7f8d32bba1001ae4a859': '', '5ace7f8d32bba1001ae4a85a': '', '5ace7f8d32bba1001ae4a85b': '', '5ace7f8d32bba1001ae4a85c': '', '56f71a5e3d8e2e1400e3735a': '1934', '56f71a5e3d8e2e1400e3735b': 'Milan Gorkić', '56f71a5e3d8e2e1400e3735c': 'Moscow', '56f71a5e3d8e2e1400e3735e': 'Tito', '5ad18f44645df0001a2d1f4f': '', '572ba8ea111d821400f38f3e': 'low cost private schools', '572ba8ea111d821400f38f40': 'Africa and Asia', '5acd809c07355d001abf449a': '', '5acd809c07355d001abf449b': '', '5acd809c07355d001abf449c': '', '5acd809c07355d001abf449e': '', '57062c2552bb891400689928': '7 July 1994', '57062c2552bb891400689929': 'l3enc', '57062c2552bb89140068992a': '.mp3', '57062c2552bb89140068992b': 'WinPlay3', '56e0a41f7aa994140058e68b': 'April 17, 1946', '56e0a41f7aa994140058e68c': 'East Prussia', '5ace063c32bba1001ae49995': '', '5ace063c32bba1001ae49996': '', '5726cc10f1498d1400e8eb81': 'voice coil-based head actuator systems', '5726cc10f1498d1400e8eb83': 'the structure in a typical (cone type) loudspeaker', '5ad16abe645df0001a2d1a74': '', '5ad16abe645df0001a2d1a75': '', '5ad16abe645df0001a2d1a76': '', '5ad16abe645df0001a2d1a77': '', '56e8379037bdd419002c44ab': 'Beijing', '56e8379037bdd419002c44ac': 'Standard Mandarin', '5ad27b63d7d075001a429653': '', '5a860699b4e223001a8e73c1': '', '5a860699b4e223001a8e73c2': '', '57283e8bff5b5019007d9fe2': 'July 16, 1945', '57283e8bff5b5019007d9fe4': 'between 20 and 22 kilotons', '5726fad05951b619008f8409': 'Allen Metz and Carol Benson', '5726fad05951b619008f840b': 'Madonna', '572e7f8003f98919007566db': '1473', '572e7f8003f98919007566dc': 'Republic of Venice', '572e7f8003f98919007566dd': '1489', '572e7f8003f98919007566df': '1539', '57270821708984140094d8d3': 'Louisiana State Agricultural Center', '57270821708984140094d8d4': '2005', '570ff9cda58dae1900cd679c': 'state collateral review', '570ff9cda58dae1900cd679e': '53', '5ad3f6d9604f3c001a3ffa01': '', '5ad3f6d9604f3c001a3ffa02': '', '5ad3f6d9604f3c001a3ffa04': '', '56de0abc4396321400ee2566': '875 CE', '5a18e9499aa02b0018605f26': '', '5a18e9499aa02b0018605f27': '', '5a18e9499aa02b0018605f28': '', '5a18e9499aa02b0018605f29': '', '5726c7305951b619008f7dd3': 'Australian federal taxes', '5726c7305951b619008f7dd4': 'David Buffett', '5726c7305951b619008f7dd6': 'July 1, 2016', '5726c7305951b619008f7dd7': 'social', '5a81c94f31013a001a334eb3': '', '56cd796762d2951400fa65ef': '2009', '56cd796762d2951400fa65f0': '14.21%', '56d12e4917492d1400aabb84': '220 million', '56d12e4917492d1400aabb86': '2013', '56dfb914231d4119001abd06': 'alcohol', '56dfb914231d4119001abd07': 'food and drink', '56dfb914231d4119001abd08': 'Holiday Inn', '56dfb914231d4119001abd09': 'innkeepers', '572904223f37b31900477f83': 'Homo sapiens', '572904223f37b31900477f84': '2 million years ago', '572904223f37b31900477f85': '1.5', '572904223f37b31900477f87': 'African Homo erectus', '56dddab666d3e219004dad31': '1568', '5a11c08c06e79900185c3547': '', '5a11c08c06e79900185c3548': '', '5a11c08c06e79900185c3549': '', '5a11c08c06e79900185c354b': '', '5a1c89fcb4fb5d001871468d': '', '5726dcbf708984140094d3fd': 'Congress for Progressive Change', '5726dcbf708984140094d3fe': 'Muhammadu Buhari', '5726dcbf708984140094d3ff': '12,214,853', '57277434708984140094de01': 'The Fiscus Judaicus', '57277434708984140094de02': '96 CE', '5ace989e32bba1001ae4abe7': '', '5729f6b13f37b31900478617': 'Earthquakes', '5729f6b13f37b31900478618': 'heat', '5729f6b13f37b31900478619': 'thermal energy', '5729f6b13f37b3190047861a': 'elastic strain', '5acd416807355d001abf3aa3': '', '5acd416807355d001abf3aa4': '', '5acd416807355d001abf3aa5': '', '5acd416807355d001abf3aa6': '', '572a4f507a1753140016ae92': 'ethnic Russians, Belarusians, and Ukrainians', '5a3c02eacc5d22001a521d22': '', '5a3c02eacc5d22001a521d23': '', '5a3c02eacc5d22001a521d24': '', '56d1109d17492d1400aab894': 'two-thirds', '57342937d058e614000b6a64': 'bacalhau', '57342937d058e614000b6a65': 'grilled sardines and caldeirada', '57342937d058e614000b6a67': 'beef, pork, lamb, or chicken', '57301519b2c2fd1400568827': 'three ounces', '57301519b2c2fd1400568829': 'current', '57301519b2c2fd140056882a': 'thermal strains', '5ace96bf32bba1001ae4ab4d': '', '5ace96bf32bba1001ae4ab4e': '', '57338497d058e614000b5c4f': 'Paul Volcker', '57338497d058e614000b5c50': 'Paul Volcker', '57277a72708984140094deb1': 'nationalist movements and ethnic disputes', '57277a72708984140094deb2': 'Romania', '57277a72708984140094deb4': \"Congress of People's Deputies\", '5724041d0ba9f01400d97b19': '1870', '5724041d0ba9f01400d97b1a': 'Trafalgar Square', '5724041d0ba9f01400d97b1b': 'Radical MPs', '5724041d0ba9f01400d97b1c': 'arm', '5724041d0ba9f01400d97b1d': 'Joseph Lister', '57255c8a69ff041400e58c39': 'Trafalgar Square', '57255c8a69ff041400e58c3b': 'typhoid fever', '57267189708984140094c640': 'establishment of the Third French Republic', '57267189708984140094c641': '1870', '57267f57f1498d1400e8e1cb': 'carbolic acid spray', '57267f57f1498d1400e8e1cc': 'typhoid fever', '57267f57f1498d1400e8e1cd': 'Radical MPs', '5ad17959645df0001a2d1db4': '', '5ad17959645df0001a2d1db7': '', '5ad17959645df0001a2d1db8': '', '5a0cfad6f5590b0018dab6ce': '', '5a0cfad6f5590b0018dab6cf': '', '5a0cfad6f5590b0018dab6d0': '', '5a0cfad6f5590b0018dab6d2': '', '56e83bdf37bdd419002c44bc': 'Italian and Spanish', '56e83bdf37bdd419002c44bd': 'body parts', '5ad27222d7d075001a429486': '', '56f7165e3d8e2e1400e37336': 'Kumrovec', '56f7165e3d8e2e1400e37337': '1892', '56f7165e3d8e2e1400e37338': 'Croat', '56f7165e3d8e2e1400e37339': 'Slovene', '5ad42d05604f3c001a40094e': ''}, 'similar_text': {'5731e624e17f3d140042251b': {'truth': '4000 films', 'predicted': '4000', 'question': 'In more that 100 years how many films have been produced in Egypt?'}, '572805f84b864d1900164260': {'truth': 'Manhattan Project', 'predicted': '', 'question': 'What high profile controversial project was Von Neumann a prinipal of?'}, '572805f84b864d1900164261': {'truth': '60 in pure mathematics, 20 in physics, and 60 in applied mathematics', 'predicted': '150 papers in his life; 60 in pure mathematics, 20 in physics, and 60 in applied mathematics', 'question': 'Of his published works, what topics were they covering?'}, '56de6a2e4396321400ee28ad': {'truth': '40 kilometres', 'predicted': '40 kilometres radius', 'question': \"How far from its studio could the BBC's broadcast originally reach?\"}, '56de6a2e4396321400ee28b0': {'truth': 'a British television set', 'predicted': '', 'question': 'What did the RCA employees use in order to receive the BBC signal?'}, '5a83215fe60761001a2eb427': {'truth': '', 'predicted': 'a British television set', 'question': 'What were engineers experimenting with in Alexandra?'}, '5726dad6708984140094d3ab': {'truth': '7.5 million dollars (31.5 million Reichsmark)', 'predicted': '7.5 million dollars', 'question': 'How much did the transfer of the Lithuanian Strip cost the Soviet Union?'}, '5726dad6708984140094d3ae': {'truth': 'until August 1, 1942', 'predicted': 'August 1, 1942', 'question': 'How long did the amendment extend the trade agreements?'}, '5ad28656d7d075001a4298de': {'truth': '', 'predicted': 'two and a half months', 'question': 'How long did germans have to relocate from the baltic states before the amendment of secret protocols?'}, '57309103069b531400832192': {'truth': 'two', 'predicted': 'at least two', 'question': 'ATC responsibilities are usually divided into how many main areas?'}, '5a4e914a755ab9001a10f4ff': {'truth': '', 'predicted': 'via radio or other communications links', 'question': 'How does apron control direct aircraft?'}, '5709b308ed30961900e84433': {'truth': '\"not worth a continental\"', 'predicted': 'not worth a continental\"', 'question': 'The quick loss in value of paper money resulted in which phrase being hear?'}, '5709b308ed30961900e84434': {'truth': 'article 1', 'predicted': 'article 1, section 10', 'question': 'The loss in value resulted in a clause being written in which article in the US Constitution?'}, '5a8cdd89fd22b3001a8d8f6a': {'truth': '', 'predicted': '1862', 'question': 'Which year was it when paper money was first issued without the backing of the Articles of Confederation?'}, '5a8cdd89fd22b3001a8d8f6c': {'truth': '', 'predicted': 'War of 1812', 'question': 'Which other war also caused a disconnect between the United States Constitution?'}, '5a8cdd89fd22b3001a8d8f6d': {'truth': '', 'predicted': 'not worth a continental\"', 'question': 'The quick loss in value of silver resulted in which phrase being heard?'}, '5a8cdd89fd22b3001a8d8f6e': {'truth': '', 'predicted': 'article 1, section 10', 'question': 'The loss in value resulted in a clause being written in which article in the Articles of Confederation?'}, '56d1ef6ae7d4791d0090259b': {'truth': 'suffering', 'predicted': '', 'question': 'What do unawakend people experience?'}, '56bf940da10cfb1400551189': {'truth': 'twenty-fifth birthday', 'predicted': 'twenty-fifth', 'question': \"What birthday did Beyonce's album B'Day celebrate?\"}, '56d4bc642ccc5a1400d83193': {'truth': 'Green Light', 'predicted': 'Déjà Vu\", featuring Jay Z, reached the top five on the Billboard Hot 100 chart. The second international single \"Irreplaceable\" was a commercial success worldwide, reaching number one in Australia, Hungary, Ireland, New Zealand and the United States. B\\'Day also produced three other singles; \"Ring the Alarm\", \"Get Me Bodied\", and \"Green Light', 'question': \"Which single from B'Day was only released in the U.K.?\"}, '56e7305b37bdd419002c3de5': {'truth': 'weekend', 'predicted': 'near a weekend midnight', 'question': 'During what part of the week is the time change most often scheduled?'}, '56e7305b37bdd419002c3de7': {'truth': 'weekday schedules', 'predicted': '', 'question': \"What do we avoid disrupting by doing the time shift during days most people don't work?\"}, '572b8eb4111d821400f38f08': {'truth': 'a major language', 'predicted': '', 'question': \"What didn't Dobrovský think Czech had a chance of returning as?\"}, '572b8eb4111d821400f38f09': {'truth': 'a Czech linguistic revival', 'predicted': '', 'question': 'What did Josef Jungmann advocate for?'}, '57281dc22ca10214002d9e2a': {'truth': 'one thousand civilians', 'predicted': 'one thousand', 'question': 'How many people died on the first German raid on London?'}, '5a36b2f895360f001af1b304': {'truth': '', 'predicted': 'about one thousand', 'question': 'How many civilians were killed in the East End on 7 September 1940?'}, '5a36b2f895360f001af1b305': {'truth': '', 'predicted': '13', 'question': 'On what date in September 1939 did Britain declare war on Nazi Germany?'}, '5726ceaa5951b619008f7e94': {'truth': \"Ottoman Empire of the Tsar's\", 'predicted': 'Ottoman Empire', 'question': 'Who recognized and gave Russia the special guardian role?'}, '5a56991a6349e2001acdce72': {'truth': '', 'predicted': 'AIDS crisis', 'question': 'What crisis began in 2004?'}, '5a56991a6349e2001acdce73': {'truth': '', 'predicted': '38.8%', 'question': 'What percentage of women have HIV?'}, '5a56991a6349e2001acdce74': {'truth': '', 'predicted': 'Themba Dlamini', 'question': 'Who declared a humanitarian crisis in Africa?'}, '572fc1dd947a6a140053cc5c': {'truth': 'The league held its first season in 1992–93', 'predicted': '1992–93', 'question': 'When did the Premier League hold its first season?'}, '572fc1dd947a6a140053cc5d': {'truth': 'was originally composed of 22 clubs.', 'predicted': '22', 'question': 'Originally, how many clubs did the Premier League have?'}, '572fc1dd947a6a140053cc5e': {'truth': 'The first ever Premier League goal was scored by Brian Deane of Sheffield United in a 2–1 win against Manchester United.', 'predicted': 'Brian Deane', 'question': 'Who scored the first ever goal for the Premier League'}, '572fc1dd947a6a140053cc5f': {'truth': 'Luton Town, Notts County and West Ham United were the three teams relegated from the old first division', 'predicted': 'Luton Town, Notts County and West Ham United', 'question': \"Which blubs were relegated from the old first division at the end of the 1991-1992 season and didn't take part in the first Premier League season?\"}, '5ad0c457645df0001a2d026c': {'truth': '', 'predicted': '22', 'question': 'During the leagues first season in 1991 how many clubs was it made up of?'}, '5ad0c457645df0001a2d026d': {'truth': '', 'predicted': '1992–93', 'question': 'In which year was the league made up of 21 clubs and had its first season?'}, '5725f36689a1e219009ac0f4': {'truth': 'first season', 'predicted': '', 'question': 'What did Arsenal want to commemorate by wearing dark red shirts in their last season at Highbury?'}, '5acd0cc307355d001abf3257': {'truth': '', 'predicted': 'Highbury', 'question': 'In what stadium did Arsenal play after the 2005-06 season?'}, '5acd0cc307355d001abf3258': {'truth': '', 'predicted': '1933', 'question': 'In what year did Herbert Chapman become manager of Arsenal?'}, '5730c36ab7151e1900c0152c': {'truth': 'Greek theologians of Byzantium', 'predicted': '', 'question': \"What Empire held  Grecian teachers of the virginity of Mary's conception ?\"}, '5730c36ab7151e1900c0152d': {'truth': 'St. Gregory Nazianzen, his explanation of the \"purification\" of Jesus and Mary', 'predicted': '', 'question': 'Who gave a reason for the purging of evil for the Blessed Virgin and her first child ?'}, '5730c36ab7151e1900c0152e': {'truth': 'the circumcision', 'predicted': 'purification\" of Jesus and Mary at the circumcision', 'question': 'What procedure was being performed while he gave his reasoning ?'}, '5730c36ab7151e1900c0152f': {'truth': 'Luke', 'predicted': '', 'question': 'Who was compelled to write of this instance that was also an author of one of the book of the Bible ?'}, '56cf306baab44d1400b88dea': {'truth': 'gay rights movement', 'predicted': 'gay rights', 'question': 'What movement is the Stonewall Inn most famously associated with?'}, '56cfe6d2234ae51400d9c046': {'truth': 'New Jersey', 'predicted': 'New York and New Jersey', 'question': 'The Statue of Liberty is also in what other US state?'}, '56cfe6d2234ae51400d9c047': {'truth': 'New Jersey', 'predicted': 'New York and New Jersey', 'question': 'Ellis Island is considered in New York state and which other?'}, '570af6876b8089140040f644': {'truth': 'the 2010s', 'predicted': '2010s', 'question': 'In what decade did developers extend the capabilities of videoconferencing to more devices?'}, '570af6876b8089140040f645': {'truth': 'Mobile collaboration systems', 'predicted': '', 'question': 'What allows people in remote locations the ability to video-conference with colleagues far away?'}, '570af6876b8089140040f646': {'truth': 'still image streaming', 'predicted': 'mobile applications as well, such as those that allow for live and still image streaming', 'question': 'What is one example of an application that videoconferencing manufacturers have begun to offer?'}, '5a1f38073de3f40018b26542': {'truth': '', 'predicted': 'live and still image streaming', 'question': 'What is one type of application that mobile collaboration manufacturers offer?'}, '5727d383ff5b5019007d962e': {'truth': 'Jesuit Matteo Ricci', 'predicted': 'Matteo Ricci', 'question': 'Who made predictions in 1601?'}, '573198280fdd8d15006c63ca': {'truth': 'a Nike advertisement', 'predicted': '', 'question': 'What was the first video to reach a million views?'}, '5acd677a07355d001abf40d8': {'truth': '', 'predicted': 'Ronaldinho', 'question': 'What hockey star was in the Nike advertisement?'}, '56f9313f9b226e1400dd1285': {'truth': 'C', 'predicted': 'Avenue C', 'question': 'The first part of 13th Street is a dead end from which Avenue?'}, '572fcc11947a6a140053ccd3': {'truth': 'Spirochaetes of the genus Borrelia', 'predicted': 'Spirochaetes', 'question': 'What bacteria is an exception to single circular chromosome rule?'}, '5726c029f1498d1400e8ea33': {'truth': 'Pavarotti', 'predicted': 'Luciano Pavarotti', 'question': 'Who performed with Brian May in 1998 at a benefit concert?'}, '5727dad64b864d1900163e9e': {'truth': 'the USB specification also defines limits to the size of a connecting device', 'predicted': 'To address a weakness present in some other connector standards, the USB specification also defines limits to the size of a connecting device in the area around its plug', 'question': 'How is a weakness addressed in some other connector standards?'}, '5727dad64b864d1900163e9f': {'truth': 'fit within the size restrictions or support a compliant extension cable that does', 'predicted': '', 'question': 'Due to size restrictions compliant devices must what?'}, '572800942ca10214002d9b14': {'truth': 'the inrush current', 'predicted': 'inrush current', 'question': 'What does the USB specification limit?'}, '572800942ca10214002d9b16': {'truth': 'ultra low-power suspend mode when the USB host is suspended', 'predicted': 'ultra low-power suspend mode', 'question': 'What are USB devices required to enter?'}, '572800942ca10214002d9b17': {'truth': 'cut off the power supply to USB devices when they are suspended', 'predicted': 'cut off the power supply', 'question': 'Many USB host interfaces do not what?'}, '572e82aacb0c0d14000f120c': {'truth': 'coal mines', 'predicted': 'coal mines, by the mid-19th century elevators were operated with steam power and were used for moving goods in bulk in mines and factories', 'question': 'For which industry were elevators first used?'}, '56df20e5c65bf219000b3f7a': {'truth': 'Galilean village', 'predicted': 'Galilean village of Nazareth', 'question': 'What village did Jesus come from?'}, '5ad2d8e3d7d075001a42a456': {'truth': '', 'predicted': 'Jesus', 'question': 'Who does Judaism accept as the Messiah?'}, '5ad2d8e3d7d075001a42a458': {'truth': '', 'predicted': 'Judaism', 'question': 'Which religion agrees that Jesus is the Messiah?'}, '5ad2d8e3d7d075001a42a459': {'truth': '', 'predicted': 'Judaism', 'question': 'Which religion does not accept Yehudim as the Messiah?'}, '5ad2d8e3d7d075001a42a45a': {'truth': '', 'predicted': 'Judaism', 'question': 'Which religion does not accept Notzri as the Messiah?'}, '56f89cb39e9bad19000a01c7': {'truth': 'the 1930s and 1940s', 'predicted': '1930s and 1940s', 'question': 'In what time span were the theories to integrate molecular genetic with Darwinian evolution developed?'}, '56f89cb39e9bad19000a01c8': {'truth': 'the modern evolutionary synthesis', 'predicted': 'modern evolutionary synthesis', 'question': 'What are the theories that integrate molecular genetics with Darwinian evolution called?'}, '56f89cb39e9bad19000a01ca': {'truth': '\"that which segregates and recombines with appreciable frequency.\"', 'predicted': '\"that which segregates and recombines with appreciable frequency', 'question': 'What is the definition of the concept of the gene as a unit of natural selection?'}, '5728b45f3acd2414000dfd16': {'truth': 'Wodeyar', 'predicted': 'Wodeyar dynasty', 'question': 'What kingdom was founded in Mysore in 1400 CE?'}, '5728b45f3acd2414000dfd18': {'truth': 'the French', 'predicted': 'French', 'question': 'What country promised aid to Mysore to fight the British?'}, '5728b45f3acd2414000dfd19': {'truth': 'Hyder Ali', 'predicted': 'Hyder Ali and his son Tipu Sultan', 'question': 'Who took over rule of Mysore in the 18th century?'}, '5727e6f8ff5b5019007d97f8': {'truth': 'six red bands in the Tibetan flag', 'predicted': 'the six red bands in the Tibetan flag', 'question': 'How are the original ancestors of the Tibetan people represented?'}, '5ad0021777cf76001a686750': {'truth': '', 'predicted': 'the Se, Mu, Dong, Tong, Dru and Ra', 'question': 'What groups represented by the red bands in the flag of India are the original ancestors of the Tibetan people?'}, '5ad0021777cf76001a686751': {'truth': '', 'predicted': 'Tibetans', 'question': 'What other ethnic groups live throughout all of Tibet?'}, '5ad0021777cf76001a686752': {'truth': '', 'predicted': 'ethnic Tibetans and some other ethnic groups', 'question': 'What does the population of Tibet consist of?'}, '572f9a2ba23a5019007fc7c9': {'truth': 'blast', 'predicted': '', 'question': 'What type of furnace was functional in China in 722 BC?'}, '572f9a2ba23a5019007fc7cb': {'truth': 'wrought', 'predicted': 'wrought iron and steel', 'question': 'What type of iron could pig iron be converted into?'}, '5731c7ade17f3d14004223d8': {'truth': 'indulgences', 'predicted': 'the Church and the papacy, but concentrated upon the selling of indulgences and doctrinal policies about purgatory, particular judgment, and the authority of the pope', 'question': 'What did the theses argue against selling?'}, '5731c7ade17f3d14004223da': {'truth': 'Wittenberg', 'predicted': 'Castle Church in Wittenberg', 'question': \"Where was All Saints' Church?\"}, '572f9e5504bcaa1900d76aee': {'truth': 'genus Mycoplasma', 'predicted': 'Mycoplasma', 'question': 'What are one of the smallest bacteria?'}, '570a661f6d058f1900182e0a': {'truth': 'the appraisal theory of emotions', 'predicted': 'appraisal theory of emotions', 'question': 'What theory was developed by Arnold?'}, '570a661f6d058f1900182e0b': {'truth': '2002', 'predicted': '1922–2002', 'question': 'When did Richard Lazarus die?'}, '5ad27982d7d075001a4295b1': {'truth': '', 'predicted': '1922–2002', 'question': 'When did Richard Lazarus work?'}, '572b8f5d111d821400f38f14': {'truth': 'Germany', 'predicted': '', 'question': 'Of the five countries with the greatest use of Czech, which country had the lowest percent of use?'}, '5a7a185f17ab25001a8a0320': {'truth': '', 'predicted': '24.86 percent', 'question': 'How long has Slovakia been a member of the EU?'}, '5731b71e0fdd8d15006c6483': {'truth': 'Baptist theologian', 'predicted': '', 'question': 'Who was Roger Williams?'}, '5731b71e0fdd8d15006c6484': {'truth': '\"[A] hedge or wall of separation between the garden of the church and the wilderness of the world\"', 'predicted': '', 'question': 'What phrase did Roger Williams first use?'}, '5ad13aec645df0001a2d12f2': {'truth': '', 'predicted': 'Thomas Jefferson', 'question': \"Who used William's phrase as a description of the Third Amendment and its restriction on the legislative branch?\"}, '572eb077c246551400ce451a': {'truth': 'Triton', 'predicted': '', 'question': 'What near Neptune did a spacecraft visit dangerously close?'}, '572eb077c246551400ce451b': {'truth': 'Neptune All Night', 'predicted': '', 'question': 'What program aired on PBS about Neptune?'}, '56df6c5a56340a1900b29af8': {'truth': '82.1', 'predicted': '78.3 years for men and 82.1', 'question': 'As of 2014, what was the life expectancy of female Plymouth residents?'}, '56df6c5a56340a1900b29af9': {'truth': 'lowest', 'predicted': 'the lowest', 'question': \"Where did Plymouth's life expectancy rank out of the regions of South West England?\"}, '5733f55e4776f419006615ac': {'truth': 'four', 'predicted': '', 'question': 'How many provinces does Pakistan have?'}, '5733f55e4776f419006615ad': {'truth': '205,344 square kilometres (79,284 square miles)', 'predicted': '205,344 square kilometres', 'question': 'How large is Punjab?'}, '5a68b42b8476ee001a58a76c': {'truth': '', 'predicted': '205,344 square kilometres', 'question': 'What is the area of Pakistan?'}, '5a68b42b8476ee001a58a76d': {'truth': '', 'predicted': 'Sindh', 'question': 'What does Pakistan mean?'}, '57274eb3f1498d1400e8f60a': {'truth': 'Thale cress, Arabidopsis thaliana', 'predicted': 'Arabidopsis', 'question': 'What was the first plant to have its genome sequenced?'}, '57274eb3f1498d1400e8f60e': {'truth': 'new knowledge about plant function', 'predicted': '', 'question': 'What results from  sequencing of DNA pairs?'}, '571adfb39499d21900609b72': {'truth': 'Scriptural study and of Greek', 'predicted': '', 'question': 'What did students learn in the school in Alexandria?'}, '5ace995332bba1001ae4ac33': {'truth': '', 'predicted': 'Alexandrian School', 'question': 'What is the name of the school where Athanasius learned Hebrew?'}, '5aceb60032bba1001ae4b119': {'truth': '', 'predicted': 'Hebrew', 'question': 'What part of the Old Testament did he not know?'}, '5aceb60032bba1001ae4b11b': {'truth': '', 'predicted': 'Scriptures', 'question': 'What copy of the scriptures did he have in exile?'}, '56deeaae3277331400b4d811': {'truth': 'information', 'predicted': 'information rather than matter to be fundamental', 'question': 'Digital physicists consider what to be more important than matter?'}, '5725ca1589a1e219009abeac': {'truth': 'U.S. Libraries initiative with a goal of \"ensuring that if you can get to a public library, you can reach the internet', 'predicted': 'ensuring that if you can get to a public library, you can reach the internet', 'question': 'what is the US libraries initiative '}, '5725ca1589a1e219009abead': {'truth': \"Only 35% of the world's population has access to the Internet\", 'predicted': '35%', 'question': 'How much of the worlds population can reach the internet'}, '5725ca1589a1e219009abeae': {'truth': 'The foundation has given grants, installed computers and software, and provided training and technical support in partnership with public libraries nationwide', 'predicted': 'installed computers and software', 'question': 'what have the grants provided public libraries'}, '5a0cdf07f5590b0018dab5fa': {'truth': '', 'predicted': '35%', 'question': \"How much of the world's population has access to libraries?\"}, '5a0cdf07f5590b0018dab5fb': {'truth': '', 'predicted': 'grants, installed computers and software', 'question': 'What has the library given to increase access and knowledge?'}, '5a0cdf07f5590b0018dab5fc': {'truth': '', 'predicted': 'this foundation helps move public libraries', 'question': 'What is the digital age initiative?'}, '5a0cdf07f5590b0018dab5fd': {'truth': '', 'predicted': 'if you can get to a public library', 'question': 'What has reaching the internet enabled?'}, '5731f389e99e3014001e6416': {'truth': 'Vestals', 'predicted': '', 'question': 'What priesthood was reserved solely for women?'}, '57264dedf1498d1400e8db90': {'truth': 'United International Bureaux for the Protection of Intellectual Property', 'predicted': '', 'question': 'What name did the merged secretariats adopt?'}, '5a157229a54d420018529436': {'truth': '', 'predicted': 'administrative secretariats', 'question': 'What position was established by the Paris convention in 1886 and the Berne Convention 1883?'}, '5731e0ad0fdd8d15006c65ec': {'truth': 'the specific relation between politics and religion', 'predicted': 'the specific relation between politics and religion in the United States', 'question': 'What does Bellah use the term \"civil religion\\' to describe?'}, '5731e0ad0fdd8d15006c65ef': {'truth': 'a religious dimension', 'predicted': 'religious dimension', 'question': 'What has the separation of church and state failed to deny the political realm of?'}, '5ad14c5d645df0001a2d164c': {'truth': '', 'predicted': 'the constitution of the United States', 'question': 'What does Bellah say the separation of church and state is not grounded firmly in?'}, '5ad14c5d645df0001a2d164f': {'truth': '', 'predicted': 'God', 'question': ' What word does Bellah ask how a President is justified to not use?'}, '5709c252200fba14003682b2': {'truth': 'third in the U.S', 'predicted': 'third', 'question': 'How did Houston rank in the U.S. for business?'}, '5ad420c6604f3c001a400717': {'truth': '', 'predicted': 'Forty', 'question': 'How many foreign governments maintain trade and commercial offices in Texas?'}, '57273aa2dd62a815002e99c2': {'truth': 'Order of the Garter', 'predicted': 'English Order of the Garter', 'question': 'What was the chivalric order established by Edward III in 1348?'}, '5ad022b677cf76001a686b5f': {'truth': '', 'predicted': 'Charles I of Hungary', 'question': 'Who co-founded the Order of St. George?'}, '572f9e8204bcaa1900d76af6': {'truth': 'Tang', 'predicted': 'Tang dynasty', 'question': 'What dynasty can the oldest wooden buildings in China be dated to?'}, '572f9e8204bcaa1900d76af7': {'truth': '907 AD', 'predicted': '618–907 AD', 'question': 'What is considered to be the last year of the Tang dynasty?'}, '572f9e8204bcaa1900d76af8': {'truth': 'ceramic roof tiles', 'predicted': 'a collection of scattered ceramic roof tiles', 'question': \"What type of object is the only evidence of Han's wooden constructions?\"}, '56def0acc65bf219000b3e3c': {'truth': 'significance', 'predicted': 'Jesus has a unique significance', 'question': 'Regardless of beliefs, Christians all agree that Jesus has a unique what?'}, '57301bfca23a5019007fcd81': {'truth': 'increased body mass', 'predicted': '', 'question': 'What is one common result of using antibiotics from a young age?'}, '57301bfca23a5019007fcd84': {'truth': 'unclear', 'predicted': 'it is unclear whether or not antibiotics', 'question': 'Do antibiotics cause obesity in humans?'}, '57301bfca23a5019007fcd85': {'truth': 'weighed against the beneficial effects', 'predicted': '', 'question': 'Why do physicians use antibiotics on infants when the relationship has been proven? '}, '57328cf2b3a91d1900202e33': {'truth': 'increased body mass', 'predicted': '', 'question': 'What can happen if people are exposed to antibiotics at a young age?'}, '56f8ee329e9bad19000a071b': {'truth': '67.7%', 'predicted': '', 'question': 'What was the gross primary enrollment rate for males?'}, '56f8ee329e9bad19000a071c': {'truth': '40%', 'predicted': '', 'question': 'What was the gross primary enrollment rate for females?'}, '572eaae1dfa6aa1500f8d288': {'truth': 'Hijazi', 'predicted': 'Hijazi script', 'question': 'What script were the Birmingham Quran fragments written in?'}, '572eaae1dfa6aa1500f8d289': {'truth': 'Arabic', 'predicted': '', 'question': 'Which modern script descends from the script on the Birmingham Quran fragments?'}, '5ad218cfd7d075001a42840a': {'truth': '', 'predicted': 'dots and chapter separators', 'question': \"What feature of the Birmingham Quran fragments' text make some doubt that it is newer than other known versions of the Quran?\"}, '57299fb33f37b31900478518': {'truth': 'has no axis of dominating altitudes, but in every portion the summits rise to rather uniform heights', 'predicted': '', 'question': 'What is common among all the mountains in the range?'}, '57299fb33f37b31900478519': {'truth': 'None of the summits reaches the region of perpetual snow.', 'predicted': '', 'question': 'What is the climate like on the summits?'}, '5ace1dc132bba1001ae49b1f': {'truth': '', 'predicted': 'two unequal portions', 'question': 'How many portions is the Great Appalachian Valley divided into?'}, '5ace1dc132bba1001ae49b22': {'truth': '', 'predicted': 'The Great Appalachian Valley', 'question': 'What divides the mountain range equally?'}, '57318f33a5e9cc1400cdc081': {'truth': 'geometric', 'predicted': 'geometric patterns from the simple to the complex', 'question': 'What is the most common pattern for Portuguese pavement?'}, '5728498d3acd2414000df8a1': {'truth': 'the powers of foreign policy and national defense as exclusive federal powers', 'predicted': 'foreign policy and national defense', 'question': 'In nearly all federalism countries, central powers enjoy what?'}, '5728498d3acd2414000df8a2': {'truth': 'federation would not be a single sovereign state', 'predicted': '', 'question': 'Per the UN definition, what is federalism?'}, '5728498d3acd2414000df8a3': {'truth': 'Germany retain the right to act on their own behalf at an international level,', 'predicted': '', 'question': 'What is the German Empire?'}, '5acfc10177cf76001a685cd2': {'truth': '', 'predicted': 'the Australian Constitution', 'question': 'Per the US definition, what is federalism?'}, '56f8c9d99b226e1400dd1003': {'truth': 'teleost fishes', 'predicted': 'teleost', 'question': 'The forebrain is everted in what type of fishes?'}, '56f8c9d99b226e1400dd1004': {'truth': 'forebrain area', 'predicted': '', 'question': 'Which part of the brain has led to many distortions among different species?'}, '5733f37ed058e614000b6651': {'truth': 'primarily as a self-defense force whose mission is to protect the territorial integrity of the country and provide humanitarian assistance and security', 'predicted': 'to protect the territorial integrity of the country', 'question': 'What is the primary purpose of the Portuguese armed forces?'}, '5733f37ed058e614000b6653': {'truth': '$5.2 billion, representing 2.1 percent of GDP', 'predicted': '$5.2 billion', 'question': 'How much money was spent on the Portuguese armed forced in 2009?'}, '572ac154be1ee31400cb8216': {'truth': 'President Bush and not American troops in general', 'predicted': 'President Bush', 'question': 'Who was Kerry saying was stuck in Iraq?'}, '5726069c38643c19005acf63': {'truth': 'highly localized', 'predicted': 'Gymnasiums and their Greek education, for example, were for Greeks only. Greek cities and colonies may have exported Greek art and architecture as far as the Indus, but these were mostly enclaves of Greek culture for the transplanted Greek elite. The degree of influence that Greek culture had throughout the Hellenistic kingdoms was therefore highly localized and based mostly on a few great cities like Alexandria and Antioch', 'question': 'What are the areas of concentration from where Greek culture eminates?'}, '5726069c38643c19005acf64': {'truth': '2.5', 'predicted': '2.5 percent', 'question': 'What percent of the Seleucid empire were comprised of native elites?'}, '57270870dd62a815002e9822': {'truth': 'The capacitance', 'predicted': 'capacitance', 'question': 'What value of some capacitors decreases with age?'}, '57270870dd62a815002e9824': {'truth': 'The type of dielectric', 'predicted': 'The type of dielectric, ambient operating and storage temperatures', 'question': 'What is one of the most important aging factors in capacitors?'}, '57270870dd62a815002e9825': {'truth': 'ambient operating and storage temperatures', 'predicted': '', 'question': 'What is another important factor which governs how a capacitor ages?'}, '57270870dd62a815002e9826': {'truth': 'the Curie point', 'predicted': 'Curie point', 'question': 'At what point can the aging effect of a capacitor be reversed if the component is heated beyond?'}, '5acf5e9177cf76001a684c86': {'truth': '', 'predicted': 'capacitance', 'question': 'What value of some capacitors increases with age?'}, '5acf5e9177cf76001a684c87': {'truth': '', 'predicted': 'degradation of the dielectric', 'question': 'What causes the increase of capacitance in ceramic capacitors as they age?'}, '5732a8a6328d981900601fed': {'truth': 'the Antarctic Circumpolar Current', 'predicted': 'Antarctic Circumpolar Current', 'question': 'Which current resulted in the cooling of Antarctica?'}, '5a4ebc82af0d07001ae8cc14': {'truth': '', 'predicted': 'permanent ice cap', 'question': 'What semi-perminant feature developed in Antarctica?'}, '56f8b9549e9bad19000a03b5': {'truth': 'A broad operational definition', 'predicted': 'broad operational', 'question': 'What sort of definition can be used to conveniently encompass the complexity of diverse phenomena?'}, '5727cf924b864d1900163dae': {'truth': 'Richard Owen,', 'predicted': 'Richard Owen', 'question': 'Who was the leading naturalist in Britain?'}, '5727cf924b864d1900163daf': {'truth': 'bitterly attacked Huxley, Hooker and Darwin, but also signalled acceptance of a kind of evolution as a teleological plan in a continuous \"ordained becoming\"', 'predicted': 'bitterly attacked Huxley, Hooker and Darwin,', 'question': 'How did Owen respond to On the Origin of Species with his review?'}, '5727cf924b864d1900163db0': {'truth': \"Huxley had emphasised anatomical similarities between apes and humans, contesting Owen's view that humans were a separate sub-class\", 'predicted': 'a separate sub-class', 'question': 'What was the debate between Huxley and Owen concerning humans and apes?'}, '5727cf924b864d1900163db1': {'truth': 'legendary 1860 Oxford evolution debate', 'predicted': 'Oxford evolution', 'question': 'What was the primary debate at the British Association for the Advancement of Science meeting of 1860?'}, '5727cf924b864d1900163db2': {'truth': 'Darwin published his own explanation in the Descent of Man (1871)', 'predicted': '1871', 'question': 'When did Darwin publish his own explanation of the question of the evolution of man and ape?'}, '572841842ca10214002da1ae': {'truth': 'Weapons Systems Evaluation Group', 'predicted': 'Weapons Systems Evaluation Group (WSEG)', 'question': 'In 1950 von Neumann became a consultant for what organization?'}, '572a1ef73f37b31900478701': {'truth': '2002', 'predicted': '', 'question': 'When was the original division of 10 advanced to a division of 5?'}, '572a1ef73f37b31900478702': {'truth': 'Danielle Stordeur and Frédéric Abbès', 'predicted': '', 'question': 'What are the names of two researchers who divided neolithic chronology into five periods?'}, '572ecec0cb0c0d14000f15b7': {'truth': 'The Yellow River', 'predicted': '', 'question': 'What emptied out to the south of the Shandong Peninsula?'}, '5731ce3ab9d445190005e576': {'truth': \"the Thirty Years' War\", 'predicted': \"Thirty Years' War\", 'question': 'What war was waged from 1618 to 1648?'}, '5731ce3ab9d445190005e578': {'truth': 'the Peace of Westphalia', 'predicted': 'Peace of Westphalia', 'question': \"What treaty ended the Thirty Years' War?\"}, '572774e65951b619008f8a5c': {'truth': 'the Carnival de la Laetare', 'predicted': 'Carnival de la Laetare', 'question': 'What Carnival takes place on Laetare Sunday?'}, '572a2c791d04691400779811': {'truth': '1998', 'predicted': '', 'question': 'In what year did the network end American programming?'}, '572a2c791d04691400779813': {'truth': 'a handful of British programs, and a few American movies and off-network repeats', 'predicted': '', 'question': 'What foreign programming is shown on the CBC after 1998?'}, '5a54e92d134fea001a0e1772': {'truth': '', 'predicted': 'CTV and Global', 'question': 'What two private Canadian broadcasters did the CBC merge with?'}, '5a54e92d134fea001a0e1774': {'truth': '', 'predicted': 'Canadian', 'question': 'What type of programming is permitted to be broadcast in Canada?'}, '57261f2fec44d21400f3d92f': {'truth': 'villa of Cassander at Vergina', 'predicted': 'Cassander at Vergina', 'question': 'What is the first example of Hellenistic period royal palace?'}, '5706ef6c9e06ca38007e9225': {'truth': '7th', 'predicted': '7th century', 'question': 'Which century did the lower-case script for the Greek Alphabet originate?'}, '5728e7254b864d1900165062': {'truth': \"explained the empirical refutations of Newton's theory\", 'predicted': '', 'question': 'What did general relativity do that made it tentatively acceptable when it was proposed?'}, '572cb395dfb02c14005c6c01': {'truth': 'no plenary reception statute at the federal level that continued the common law', 'predicted': '', 'question': 'How did the federal agencies differ from their English counter-parts?'}, '572cb395dfb02c14005c6c02': {'truth': 'granted federal courts the power to formulate legal precedent', 'predicted': '', 'question': 'What did the missing plenary reception do?'}, '572cb395dfb02c14005c6c03': {'truth': 'the federal Judiciary Acts', 'predicted': '', 'question': 'Where do the federal courts fall?'}, '572cb395dfb02c14005c6c04': {'truth': 'Article Three', 'predicted': '', 'question': 'Where is judicial power found in the original Constitution?'}, '572cb395dfb02c14005c6c05': {'truth': 'implied judicial power of common law courts to formulate persuasive precedent', 'predicted': '', 'question': 'What does Article Three give federal agencies?'}, '5a79d0b117ab25001a8a00c9': {'truth': '', 'predicted': 'at the federal level', 'question': 'Where is the plenary reception statute found in the Constitution?'}, '570d61a5b3d812140066d7a6': {'truth': 'visit of the pope to Valencia', 'predicted': 'the visit of the pope to Valencia', 'question': 'What happened at the same time as the crash, which may have contributed to the government downplaying it?'}, '570d61a5b3d812140066d7a7': {'truth': 'book of train breakdowns', 'predicted': 'the book of train breakdowns', 'question': 'What evidence related to the crash remains missing?'}, '5ace853932bba1001ae4a954': {'truth': '', 'predicted': 'septa', 'question': \"What are annelids' body cavities combined with other segments by?\"}, '5ace853932bba1001ae4a955': {'truth': '', 'predicted': 'septa', 'question': \"What are annelids' body cavities separated from eyes by?\"}, '56e087957aa994140058e5c3': {'truth': 'normal', 'predicted': 'normal form', 'question': 'When hydrogen gas is in standard temperature and pressure, what form is it considered in>'}, '56df81eb5ca0a614008f9bbe': {'truth': 'El Draco', 'predicted': 'El Draco meaning \"The Dragon', 'question': 'What did the Spanish nickname Sir Francis Drake?'}, '5727a6233acd2414000de8d7': {'truth': 'Southern Connecticut State University', 'predicted': 'Southern Connecticut', 'question': 'What state university is located in New Haven?'}, '572a37a3af94a219006aa8bd': {'truth': 'heart of downtown', 'predicted': 'at the heart of downtown', 'question': 'Where is Yale University located.'}, '5727ff933acd2414000df1bd': {'truth': 'Anyone can do this', 'predicted': '', 'question': 'Is it difficult to transfer recording from historic interest to newer technologies?'}, '5727ff933acd2414000df1be': {'truth': 'professional archivists', 'predicted': '', 'question': 'What would offer the highest quality transfers of historic interest?'}, '5727ff933acd2414000df1c0': {'truth': 'without any further damage to the source recording', 'predicted': '', 'question': 'Is an original destroyed when transferred to digital format?'}, '5727ff933acd2414000df1c1': {'truth': 'manipulated to remove analog flaws', 'predicted': '', 'question': 'What is one benefit of transferring an older format to a newer format?'}, '56f957d89e9bad19000a0854': {'truth': 'Morningside Park', 'predicted': 'Morningside Avenue at Morningside Park', 'question': 'At which park does W 122nd Street end?'}, '56e07ea2231d4119001ac1eb': {'truth': 'a former print and BBC journalist', 'predicted': '', 'question': 'Who runs Saint Helena online?'}, '56e07ea2231d4119001ac1ec': {'truth': 'Saint FM and the St Helena Independent', 'predicted': '', 'question': 'Who is partnered with Saint Helena online?'}, '5729234a1d046914007790a7': {'truth': 'racial classification', 'predicted': '', 'question': 'What is not a natural taxonomy of the human species?'}, '5729234a1d046914007790a9': {'truth': 'methodologies', 'predicted': 'two separate methodologies', 'question': 'What are diversity partition and clustering analysis are examples of?'}, '572b8b54111d821400f38efd': {'truth': 'Age of Enlightenment a half-century earlier', 'predicted': 'Age of Enlightenment', 'question': \"What inspired the Czech's national pride?\"}, '572b8b54111d821400f38efe': {'truth': 'accomplishments', 'predicted': \"their people's accomplishments\", 'question': 'What did Czech historians emphasize about their countrymen?'}, '572b8b54111d821400f38f00': {'truth': 'Czech National Revival', 'predicted': 'the Czech National Revival', 'question': 'What is the period during the mid-eighteenth century also remembered as?'}, '5a7a161417ab25001a8a030c': {'truth': '', 'predicted': 'Czech and other non-Latin languages', 'question': 'What emotion did the Counter-Reformation support during the mid-eighteenth century?'}, '570a85944103511400d59804': {'truth': 'the Norman Conquest', 'predicted': 'Norman Conquest', 'question': 'What event led to English temporarily losing its importance as a literary language?'}, '5a678d67f038b7001ab0c2b5': {'truth': '', 'predicted': 'Ælfric of Eynsham', 'question': 'What writer helped develop the Winchester standard?'}, '570ddc210dc6ce1900204ccd': {'truth': 'more likely', 'predicted': 'more', 'question': 'Are girls reaching sexual maturation early more or less likely to develop eating disorers?'}, '570ddc210dc6ce1900204ccf': {'truth': 'early maturing', 'predicted': 'early', 'question': 'Are early or late maturing girls more exposed to alcohol and drug abuse?'}, '570ddc210dc6ce1900204cd1': {'truth': 'inexperienced', 'predicted': 'Those who have had such experiences tend to perform not as well in school as their \"inexperienced\" peers', 'question': 'Who performs better in school: sexually experienced or inexperienced teen females?'}, '572fde5e04bcaa1900d76dfd': {'truth': 'eight', 'predicted': '', 'question': 'How many majors are available to students in the business school at Washington University?'}, '5ace2b5c32bba1001ae49c79': {'truth': '', 'predicted': 'Olin Business School', 'question': 'What university did Mahendra R. Gupta graduate from?'}, '5ace2b5c32bba1001ae49c7b': {'truth': '', 'predicted': 'Undergraduate BSBA students take 40–60% of their courses within the business school', 'question': 'How can undergraduate students attend BSBA?'}, '5ace2b5c32bba1001ae49c7c': {'truth': '', 'predicted': '40–60%', 'question': 'What percentage of an MBA students courses are at the business school?'}, '56f8d8959e9bad19000a05e1': {'truth': 'PAIGC (African Party for the Independence of Guinea and Cape Verde)', 'predicted': 'PAIGC', 'question': 'What party did Sanha belong to?'}, '56f8d8959e9bad19000a05e3': {'truth': 'more than 20', 'predicted': '20', 'question': 'How many minor political parties are there?'}, '56f8d8959e9bad19000a05e4': {'truth': 'President', 'predicted': 'President Rachide Sambu-balde Malam Bacai Sanhá died. He belonged to PAIGC', 'question': 'What office did Sanha hold in 2012?'}, '5726c9c25951b619008f7e1f': {'truth': 'dispatched the United States Seventh Fleet to the Taiwan Strait', 'predicted': 'dispatched the United States Seventh Fleet', 'question': \"What did President Truman do to prevent hostilities between the People's Republic of China and Taiwan?\"}, '5726c9c25951b619008f7e21': {'truth': 'American aggression in the guise of the UN', 'predicted': 'American aggression in the guise of the UN\"', 'question': 'What provoked China to join the war?'}, '5726e581f1498d1400e8ef29': {'truth': 'The corridors are treated as public places like streets, and the route between the station and the city centre is open all night', 'predicted': 'as public places like streets', 'question': 'how are the shopping center corridors treated '}, '5726e581f1498d1400e8ef2a': {'truth': \"Parts of the city's network of canals, which were filled to create the shopping center and central station area, will be recreated\", 'predicted': \"Parts of the city's network of canals\", 'question': 'What is being recreated '}, '5726e581f1498d1400e8ef2b': {'truth': 'The Jaarbeurs, one of the largest convention centres in the Netherlands, is located at the west side of the central railway station', 'predicted': 'The Jaarbeurs', 'question': 'what is located on the west side of the rail station'}, '5a580e33770dc0001aeeffa0': {'truth': '', 'predicted': 'Hoog Catharijne', 'question': 'What shopping center is located in the city center?'}, '5a580e33770dc0001aeeffa3': {'truth': '', 'predicted': 'The Jaarbeurs', 'question': 'What is one of the largest convention centres in Europe?'}, '56fc3f0e00a8df190040382c': {'truth': 'recent years.', 'predicted': 'recent years', 'question': 'When did Evolutionary Phonology come into being?'}, '56fc3f0e00a8df190040382d': {'truth': 'integrated', 'predicted': 'integrated approach to phonological theory', 'question': 'What sort of approach did Evolutionary Phonology take?'}, '56e14acbcd28a01900c6774b': {'truth': '\"clean picture\"', 'predicted': 'clean picture', 'question': 'what was the policy that Universal followed in its early years?'}, '56e14acbcd28a01900c6774d': {'truth': 'Universal was losing money', 'predicted': 'generating more profit while Universal was losing money', 'question': 'Why did Laemmle change his position on \"unclean pictures\"?'}, '5ad13864645df0001a2d122d': {'truth': '', 'predicted': 'April 1927', 'question': 'When did Carl Laemmle decide to establish a clean picture policy?'}, '5728d51a4b864d1900164f08': {'truth': 'native construction, native blacksmithing, native textile design', 'predicted': 'native construction, native blacksmithing', 'question': 'What parts of native culture does the Viljandi Culture Academy highlight?'}, '56fa85dd8f12f3190063016f': {'truth': 'the United Kingdom', 'predicted': 'United Kingdom', 'question': 'Which European country first deployed HD content using the new DVB-T2 standard?'}, '5ad3bee3604f3c001a3fef4b': {'truth': '', 'predicted': 'United Kingdom', 'question': 'Which European country first deployed SD content using the new DVB-T2 standard?'}, '5ad3bee3604f3c001a3fef4e': {'truth': '', 'predicted': 'Digital TV Group', 'question': ' What does DTD stand for?'}, '5732bcead6dcfa19001e8a98': {'truth': '\"less than lethal\" or \"less-lethal\"', 'predicted': 'less than lethal', 'question': 'What should non-lethal weapons properly be called?'}, '5732bcead6dcfa19001e8a9b': {'truth': 'allows police to use deadly force against any person who poses a significant threat to them or civilians', 'predicted': 'allows police to use deadly force', 'question': 'What is South Africa\\'s \"shoot-to-kill\" policy?'}, '5acece2e32bba1001ae4b4ab': {'truth': '', 'predicted': 'less than lethal', 'question': 'What should lethal weapons properly be called?'}, '5acece2e32bba1001ae4b4ad': {'truth': '', 'predicted': 'use of firearms or deadly force', 'question': 'What is supposed to be the first resort for police?'}, '5acece2e32bba1001ae4b4ae': {'truth': '', 'predicted': 'allows police to use deadly force', 'question': 'What is North Africa\\'s \"shoot-to-kill\" policy?'}, '572ee3a8c246551400ce477e': {'truth': 'the eunuchs', 'predicted': '', 'question': 'Who arrested Chen Fan in a failed plot?'}, '572ee3a8c246551400ce4780': {'truth': 'he was forced to commit suicide', 'predicted': 'suicide', 'question': 'How did Dou Wu pass away?'}, '572ee3a8c246551400ce4781': {'truth': 'the eunuchs', 'predicted': 'eunuchs', 'question': 'Which group was favorable to Zhang Huan?'}, '5727903ddd62a815002ea085': {'truth': 'US$95 to US$300', 'predicted': '', 'question': 'What was the initial cost range of early recording devices?'}, '56cd828562d2951400fa6670': {'truth': '2010', 'predicted': '', 'question': 'In what year did Chinese Foxconn emplyees kill themselves?'}, '56cd828562d2951400fa6672': {'truth': 'Apple prototype', 'predicted': '', 'question': 'What disappeared in 2009 prior to the suicide of a Foxconn employee?'}, '56f81f0ea6d7ea1400e173d8': {'truth': '1,200 kilometres', 'predicted': '1,200', 'question': 'How many kilometres do the Alps stretch?'}, '56f81f0ea6d7ea1400e173db': {'truth': 'the \"four-thousanders\"', 'predicted': 'four-thousanders', 'question': 'The Alpine region is also known as what? '}, '57313c12a5e9cc1400cdbd6c': {'truth': 'dot-matrix', 'predicted': 'seven-segment, starburst and dot-matrix', 'question': 'What is another format that Alphanumeric LEDs are available in?'}, '57313c12a5e9cc1400cdbd6f': {'truth': 'liquid crystal displays', 'predicted': 'rising use of liquid crystal displays', 'question': 'What has reduced the popularity of numeric LED displays?'}, '56f744beaef2371900625a78': {'truth': 'The common practice period', 'predicted': '', 'question': 'What began with the Baroque era?'}, '56f744beaef2371900625a7a': {'truth': 'around 1820', 'predicted': '1820', 'question': 'When did the classical era end?'}, '56f744beaef2371900625a7b': {'truth': 'about 1910', 'predicted': '1910', 'question': 'When did the romantic era end?'}, '56d5307f2593cc1400307abc': {'truth': 'about 9,000', 'predicted': '9,000', 'question': 'What was the previous population of Yingxiu?'}, '56d5307f2593cc1400307abf': {'truth': 'Eight schools', 'predicted': 'Eight', 'question': 'What is the number of schools that collapsed in Dujiangyan?'}, '572a69cefed8de19000d5bfe': {'truth': 'Zinc', 'predicted': 'Zinc supplementation', 'question': 'What has been used and shown successful in a decrease in incidence of diarrheal disease?'}, '572a69cefed8de19000d5bff': {'truth': 'zinc supplementation', 'predicted': '', 'question': 'WHat strategy was found out to be more cost effective?'}, '5a0e3462d7c8500018864549': {'truth': '', 'predicted': 'by promoting better eating practices', 'question': 'How can zinc deficiencies be combated?'}, '5a0e3462d7c850001886454b': {'truth': '', 'predicted': 'vitamin A supplementation', 'question': 'What dietary deficiency was found to be less effective in reducing diarrhea incidence?'}, '57110273b654c5140001fab2': {'truth': 'Charles Porset', 'predicted': 'Porset', 'question': 'Who believed the avoidance of thematic and heirarhical systems allowed free interpretation of the works and caused them to beomce an example of eglitarianism?'}, '572817474b864d190016445c': {'truth': 'seven stories', 'predicted': 'up to seven stories', 'question': 'How high were some of the buildings in urban Germany?'}, '5acd6c4a07355d001abf4184': {'truth': '', 'predicted': 'painted bricks, woodwork and majolica', 'question': 'What type of facade does the Palais du Rhin have?'}, '572f820a04bcaa1900d76a3a': {'truth': 'Île Saint-Jean (present-day Prince Edward Island)', 'predicted': '', 'question': 'What is Lie Saint-Jean called today?'}, '572f820a04bcaa1900d76a3b': {'truth': '4,000 French troops repulsed 16,000 British', 'predicted': '4,000', 'question': 'How much were the French outnumbered at the Battle of Carillion?'}, '5728f64eaf94a219006a9e7d': {'truth': 'Chinese arts', 'predicted': '', 'question': 'What was Japanese culture influenced by?'}, '5728f64eaf94a219006a9e7f': {'truth': '1275', 'predicted': '1275–1351', 'question': 'When was Muso Soseki born?'}, '5a6a14595ce1a5001a9696da': {'truth': '', 'predicted': 'Malcolm Turnbull', 'question': \"Who is the Abbott Party's leader?\"}, '570d2b46b3d812140066d4de': {'truth': '\"Enlisted Reserve Corps\" and \"Officer Reserve Corps\"', 'predicted': 'Officer Reserve Corps', 'question': 'Who filled vacancies in the Regular Army?'}, '5acecfac32bba1001ae4b4fa': {'truth': '', 'predicted': 'World War I', 'question': 'When was the National Army started?'}, '5acecfac32bba1001ae4b4fb': {'truth': '', 'predicted': 'the Regular Army, the Organized Reserve Corps, and the State Militias', 'question': 'What was added to the National Army?'}, '57261b6dec44d21400f3d8f1': {'truth': 'challenging', 'predicted': 'challenging conditions that demand repetitive manual labor', 'question': 'What type of conditions do many garment works endure?'}, '57261b6dec44d21400f3d8f2': {'truth': 'Mass-produced', 'predicted': 'Mass-produced clothing', 'question': 'What type of clothing is frequently the product of sweatshops?'}, '57261b6dec44d21400f3d8f3': {'truth': 'long work hours', 'predicted': '', 'question': 'What is a feature of sweatshops beyond lack of benefits and representation?'}, '57261b6dec44d21400f3d8f4': {'truth': 'industrialized', 'predicted': '', 'question': 'Poor conditions found in developing countries may also be found in what type of nations?'}, '5a0cfe3af5590b0018dab6fc': {'truth': '', 'predicted': 'sweatshops', 'question': 'Here my clothing is often produced in what?'}, '57268816dd62a815002e885c': {'truth': 'the second HoHoKam Park', 'predicted': 'HoHoKam Park', 'question': 'Where was the former location in Mesa?'}, '57268816dd62a815002e885d': {'truth': '25,000 square feet', 'predicted': '25,000', 'question': 'How many square feet does Fitch Park provide?'}, '5727ba9f4b864d1900163ba3': {'truth': '13th-century', 'predicted': '13th', 'question': 'In what century did Nimbarka live?'}, '5a5e608c5bc9f4001a75af88': {'truth': '', 'predicted': 'Krishna', 'question': \"Who is Radha's consort?\"}, '572629d9ec44d21400f3db2a': {'truth': 'Anglo-Dutch Wars of the 17th and 18th centuries', 'predicted': 'Anglo-Dutch Wars', 'question': 'Name the wars that was caused by the intense competition between the EIC and Dutch East India Company'}, '572629d9ec44d21400f3db2b': {'truth': 'ousting the Portuguese in 1640–41', 'predicted': 'ousting the Portuguese', 'question': 'what caused the Dutch to expand thier spice trade in the malaccan straits?'}, '5726572af1498d1400e8dc7e': {'truth': 'indigo dye', 'predicted': '', 'question': \"why type of  dye was one of the East  India company's main  products?\"}, '5a8456db7cf838001a46a75f': {'truth': '', 'predicted': '1717', 'question': 'What year did the Mughal emperor strictly require customs duties?'}, '5a8456db7cf838001a46a760': {'truth': '', 'predicted': 'Bengal', 'question': 'What region was made available to black market traders by the Mughal emperor?'}, '5a8456db7cf838001a46a762': {'truth': '', 'predicted': 'cotton, silk, indigo', 'question': \"What type of dye was one of the North India company's main products?\"}, '5ad186c5645df0001a2d1e97': {'truth': '', 'predicted': 'Jesus Christ', 'question': 'Who did Mary recognize herself as?'}, '5ad186c5645df0001a2d1e98': {'truth': '', 'predicted': \"Unitarians, Christadelphians and Jehovah's Witnesses\", 'question': 'Which Nontrinitarian churches once acknowledged Mary as \"Mother of God\"?'}, '5ad186c5645df0001a2d1e99': {'truth': '', 'predicted': 'Jesus Christ', 'question': 'Whom did Mary give access to the evil heredity of the human race?'}, '56e0fd33231d4119001ac54c': {'truth': 'Chen Fangyun and his colleagues', 'predicted': 'Chen Fangyun', 'question': 'Who first came up with the idea for a Chinese satellite navigation system?'}, '56e0fd33231d4119001ac54d': {'truth': 'in the 1980s', 'predicted': '1980s', 'question': 'When did Chen Fangyun come up with the idea for a satellite navigation system?'}, '5acd345807355d001abf3948': {'truth': '', 'predicted': 'three', 'question': 'The American National Space Administration said the satellite navigation system would be developed in how many steps?'}, '572ea349cb0c0d14000f13b9': {'truth': 'Municipal art galleries', 'predicted': '', 'question': 'What types of places can Cypriots visit to experience art?'}, '57271234f1498d1400e8f31f': {'truth': 'proto-dentistry', 'predicted': '', 'question': 'What do the findings in the graveyards show evidence of?'}, '57271234f1498d1400e8f321': {'truth': 'Suśrutasamhitā of Suśruta', 'predicted': 'Suśrutasamhitā', 'question': 'What is the name of the text that has information regarding Ayurveda?'}, '57271234f1498d1400e8f322': {'truth': 'surgical procedures', 'predicted': '', 'question': 'What information is in the Susrutasamhita of Susruta?'}, '56f7e518aef2371900625c4c': {'truth': 'clans (ród)', 'predicted': 'clans', 'question': 'What ruled over the tribes?'}, '56f7e518aef2371900625c4d': {'truth': 'theoretically descending from a common ancestor', 'predicted': '', 'question': 'What did the clans people all have in common?'}, '56f7e518aef2371900625c4e': {'truth': 'related by blood or marriage', 'predicted': '', 'question': 'What gave them a sense of solidarity? '}, '56f7e518aef2371900625c4f': {'truth': 'grόd', 'predicted': '', 'question': 'What were stronghold called?'}, '57268e54dd62a815002e896a': {'truth': 'Kuwait', 'predicted': 'Oman (1957), Jordan (1958) and Kuwait', 'question': \"Where did Britain's army attack in 1961?\"}, '5ad13aca645df0001a2d12d6': {'truth': '', 'predicted': 'in order to govern', 'question': 'Why did Callaghan trade with the larger parties?'}, '572690d45951b619008f76cd': {'truth': 'golden Angel of Independence', 'predicted': 'Angel of Independence', 'question': 'What is the most popular icon of Mexico City?'}, '572690d45951b619008f76d0': {'truth': '28.8 km (17.9 mi)', 'predicted': '28.8 km', 'question': 'How long is the longest avenue in Mexico City?'}, '5ace33db32bba1001ae49dfa': {'truth': '', 'predicted': 'CBS Songs', 'question': 'SAK Entertainment acquired which publishing arm?'}, '5726a1ccf1498d1400e8e56a': {'truth': 'Agriculture and food and drink production continue to be major industries in the county, employing over 15,000 people', 'predicted': '', 'question': 'What area employs 15000 people in the couinty'}, '5726a1ccf1498d1400e8e56b': {'truth': 'Apple orchards were once plentiful, and Somerset is still a major producer of cider', 'predicted': '', 'question': 'What type of orchids used to be pleantiful '}, '5726a1ccf1498d1400e8e56d': {'truth': 'Cheddar cheese—some of which has the West Country Farmhouse Cheddar Protected Designation of Origin (PDO).', 'predicted': '', 'question': 'What area is PDO '}, '5acf76ad77cf76001a684e78': {'truth': '', 'predicted': '15,000', 'question': 'How many people work in the Apple Orchards?'}, '5731d56fe17f3d1400422471': {'truth': 'the modern Pentecostal movement', 'predicted': 'Pentecostal', 'question': 'What modern movement began in the 20th century?'}, '5731d56fe17f3d1400422473': {'truth': 'the Charismatic movement', 'predicted': 'Charismatic', 'question': 'What movement did Pentecostalism create?'}, '5728b2813acd2414000dfcf7': {'truth': 'Portuguese for \"mandolin\"', 'predicted': 'mandolin', 'question': 'What does bandolim mean?'}, '5728b2813acd2414000dfcfb': {'truth': 'over 17', 'predicted': '17', 'question': 'How many active mandolin orchestras does the Madiera Island have? '}, '5ad22fced7d075001a42869a': {'truth': '', 'predicted': 'mandolin', 'question': ' What does bandolin mean?'}, '572ec004c246551400ce45f9': {'truth': 'he was now forced to withdraw further into Prussian-controlled territory', 'predicted': 'break off his invasion of Bohemia, he was now forced to withdraw further into Prussian-controlled territory', 'question': \"What was Frederick's response to the Russian invasion?\"}, '5727721add62a815002e9d04': {'truth': 'Ottoman', 'predicted': 'Ottoman Empire', 'question': 'Which empire completed its conquest of the Byzantines at the end of the 15th century?'}, '5727721add62a815002e9d06': {'truth': 'Vladislaus I', 'predicted': 'Vladislaus I of Hungary', 'question': 'Which Hungarian ruler was killed at the Battle of Varna?'}, '5ad02eec77cf76001a686d5e': {'truth': '', 'predicted': 'Ottoman Empire', 'question': 'Which empire completed its conquest of the Byzantines at the end of the 14th century?'}, '5ad02eec77cf76001a686d60': {'truth': '', 'predicted': 'Vladislaus I of Hungary', 'question': 'Which Hungarian ruler was saved at the Battle of Varna?'}, '5ad02eec77cf76001a686d61': {'truth': '', 'predicted': 'count John Hunyadi', 'question': 'Who was appointed regent-governor of the Kingdom of Hungary in 1464?'}, '5ad02eec77cf76001a686d62': {'truth': '', 'predicted': 'Pope Pius II', 'question': 'Who gave the title of Champion of Christ to Sean Hyundai?'}, '572782b6dd62a815002e9f31': {'truth': 'Left-wing politics', 'predicted': 'Left-wing', 'question': 'What kind of politics have been strong in the municipal government?'}, '5ace451232bba1001ae4a147': {'truth': '', 'predicted': 'Debbie Dingell', 'question': 'Who is the representative for Ann Arbor in the 21st congressional district?'}, '57313b16e6313a140071cd50': {'truth': 'anti-Manchu writer', 'predicted': '', 'question': 'Who did Yongzheng behead?'}, '57313b16e6313a140071cd52': {'truth': '1723', 'predicted': '', 'question': 'When did Yongzheng ban christianity?'}, '57327ed206a3a419008aca89': {'truth': 'Latin literary texts', 'predicted': 'close study of Latin literary texts', 'question': 'How were humanist able to identify the development of humanist thought?'}, '57327ed206a3a419008aca8a': {'truth': 'Patristic literature', 'predicted': 'manuscripts of Patristic literature as well as pagan authors', 'question': 'What was included in this quest for knowledge of the belief system?'}, '57327ed206a3a419008aca8b': {'truth': 'philology', 'predicted': '', 'question': 'If your were unsure of the authenticity of an ancient text how could you verify it?'}, '5a81ff2b31013a001a335086': {'truth': '', 'predicted': 'Greek manuscripts, not only of Plato and Aristotle, but also of the Christian Gospels', 'question': 'What was missing in this quest for knowledge of the belief system?'}, '5726d3475951b619008f7f28': {'truth': 'General MacArthur', 'predicted': '', 'question': 'Who was not concerned about the idea of Chinese troops moving south into Korea?'}, '5726d3475951b619008f7f29': {'truth': 'the greatest slaughter', 'predicted': 'there would be the greatest slaughter', 'question': 'What was believed would happen if the Chinese entered the conflict?'}, '57318a1305b4da19006bd25c': {'truth': 'Muslim', 'predicted': '', 'question': 'Who conquered the Eastern Provinces of the Byzantine empire?'}, '57318a1305b4da19006bd25f': {'truth': 'remained a flourishing art form', 'predicted': 'flourishing', 'question': 'The Umayyad Dynasty made mosaic making do what in the Islamic culture?'}, '56e477998c00841900fbafa1': {'truth': \"art for art's sake\", 'predicted': 'art', 'question': \"What is a reactionary thing to limit formalism's meaning to?\"}, '56e477998c00841900fbafa2': {'truth': 'quest for perfection or originality', 'predicted': '', 'question': 'What sort of quest lacks purpose?'}, '56e477998c00841900fbafa3': {'truth': 'form', 'predicted': '', 'question': 'What ends up being reduced in quality by this quest?'}, '5acf9e8d77cf76001a68557e': {'truth': '', 'predicted': 'perfection or originality', 'question': ' What ends up being maximized in quality by this quest?'}, '572816302ca10214002d9d9a': {'truth': \"in front of St. George's Cathedral.\", 'predicted': \"front of St. George's Cathedral\", 'question': 'Where was the moleben held?'}, '5726a792dd62a815002e8c1d': {'truth': '\"Crazy for You\" and \"Gambler\"', 'predicted': 'Crazy for You\" and \"Gambler', 'question': \"What is the name of Madonna's two new singles?\"}, '5726a792dd62a815002e8c1e': {'truth': '\"Into the Groove\"', 'predicted': 'Into the Groove', 'question': 'What song did the comedy Desperately Seeking Susan promote?'}, '5726fd0f5951b619008f8424': {'truth': 'into the city.', 'predicted': 'the city', 'question': 'Where did the Russians retreat to? '}, '570d3ebefed7b91900d45d8d': {'truth': '20', 'predicted': '20%', 'question': \"What percentage of Spanish exports does Valencia's port handle?\"}, '572f02fbcb0c0d14000f1708': {'truth': 'their respective governments', 'predicted': '', 'question': 'What dispatched of The Dutch East India Company and the British East India Company?'}, '5732488d0fdd8d15006c68ec': {'truth': 'Fort Lewis', 'predicted': 'Fort Lewis, Washington', 'question': 'At what military installation was the 15th Infantry based?'}, '5732488d0fdd8d15006c68ed': {'truth': 'Kenyon Joyce', 'predicted': 'Major General Kenyon Joyce', 'question': 'In the spring of 1941, who commanded IX Corps?'}, '57266708708984140094c4e5': {'truth': 'London, Paris and New York.', 'predicted': 'London, Paris and New York', 'question': 'What cities influenced how department stores in Germany operated? '}, '56dedfc3c65bf219000b3d9d': {'truth': 'the USC Sol Price School of Public Policy', 'predicted': 'USC Sol Price School of Public Policy', 'question': 'What school within the University of Southern California does the Schwarzenegger Institute for State and Global Policy belong to?'}, '56f8cc9a9b226e1400dd1030': {'truth': 'tourism', 'predicted': 'advent of tourism', 'question': 'What made farming less dominant in the 20th century? '}, '572ff633a23a5019007fcbbb': {'truth': '1935', 'predicted': '', 'question': 'When did Reza Shan request that Iran officially be referred to as Iran and not Persia?'}, '5ad4c0485b96ef001a109f30': {'truth': '', 'predicted': '1989', 'question': 'When did the second Indiana Jones movie come out?'}, '5ad4c0485b96ef001a109f33': {'truth': '', 'predicted': 'Indiana Jones and the Last Crusade', 'question': \"What is Spielberg's most profitable movie?\"}, '5ad4c0485b96ef001a109f34': {'truth': '', 'predicted': 'Batman', 'question': \"What is Tim Burton's most profitable movie?\"}, '5ad243b2d7d075001a428a1a': {'truth': '', 'predicted': 'Paul Ekman', 'question': 'Who has agreed that emotions are discrete?'}, '5726a88e5951b619008f794f': {'truth': 'the end of the 20th century', 'predicted': 'end of the 20th century', 'question': 'When did republicanism and the capability approach arise?'}, '5726a88e5951b619008f7951': {'truth': 'The capability approach', 'predicted': 'capability approach', 'question': 'Mahbub ul Haq and Amartya Sen pioneered what approach?'}, '5a21dc2f8a6e4f001aa08faf': {'truth': '', 'predicted': 'may be free', 'question': 'To a liberal, what is the state of a political perspective that is not interfered with? '}, '5a21dc2f8a6e4f001aa08fb0': {'truth': '', 'predicted': 'capability approach', 'question': 'What is the approach of republicanism as developed by Mahbub ul Haq and Amartya Sen?'}, '572722c0708984140094da53': {'truth': 'a rotational motion on the rotor', 'predicted': '', 'question': 'What is the main winding on a squirrel cage motor capable of withstanding?'}, '572722c0708984140094da54': {'truth': 'series non-polarized starting capacitor', 'predicted': '', 'question': 'What sort of capacitors are used on the second winding of a squirrel cage motor?'}, '572722c0708984140094da55': {'truth': 'introduce a lead in the sinusoidal current', 'predicted': '', 'question': 'What does the capacitor on the second winding of a squirrel cage motor do?'}, '572722c0708984140094da56': {'truth': 'disconnects the capacitor', 'predicted': 'disconnects', 'question': 'What does the centrifugal switch do the capacitor when the rotor achieves operating speed?'}, '572722c0708984140094da57': {'truth': 'to the side of the motor housing', 'predicted': 'the side of the motor housing', 'question': 'Where is the start capacitor commonly mounted?'}, '5acf8ff377cf76001a685279': {'truth': '', 'predicted': 'the side of the motor housing', 'question': ' Where is the start capacitor commonly mounted?'}, '572f78f5b2c2fd1400568161': {'truth': 'structured document-oriented database', 'predicted': 'structured document-oriented', 'question': 'What kind of database is XML?'}, '572f78f5b2c2fd1400568163': {'truth': 'machine-to-machine data', 'predicted': 'machine-to-machine data interoperability standard', 'question': 'How is XML used in enterprise database management?'}, '572f78f5b2c2fd1400568164': {'truth': 'ACID-compliant transaction processing', 'predicted': '', 'question': 'What type of processing is used in enterprise database software?'}, '57313c97e6313a140071cd6a': {'truth': '3,400 books', 'predicted': '3,400', 'question': 'How many books were in the Siku Quanshu?'}, '57313c97e6313a140071cd6b': {'truth': '36,304 volumes', 'predicted': '36,304', 'question': 'How many volumes were in the Siku Quanshu?'}, '56f754a3a6d7ea1400e171be': {'truth': 'Marcin Woźniak and colleagues', 'predicted': 'Marcin Woźniak', 'question': 'Who searched for specifically Slavic sub-group of R1a1a [M17]?'}, '5ad4c6d55b96ef001a10a01d': {'truth': '', 'predicted': 'a date far too old to be Slavic', 'question': 'What is the evolutionary effective mutation rate more in line with?'}, '5ad4c6d55b96ef001a10a01e': {'truth': '', 'predicted': 'M458', 'question': 'What marker did Wozniak discover?'}, '56cecf68aab44d1400b88ab7': {'truth': '7,000', 'predicted': 'over 7,000', 'question': 'How many schoolrooms collapsed in the quake?'}, '5727c9722ca10214002d9632': {'truth': 'external portable USB hard disk drives', 'predicted': '', 'question': 'Several manufacturers offer what?'}, '5727c9722ca10214002d9633': {'truth': 'performance comparable to internal drives', 'predicted': 'performance', 'question': 'What do external portable USB hard drive disks offer?'}, '5727c9722ca10214002d9635': {'truth': 'a \"translating device\"', 'predicted': 'a \"translating device\" that bridges between a drive\\'s interface to a USB interface port', 'question': 'What do the external drives typically include?'}, '570a88724103511400d5981b': {'truth': 'in Houston', 'predicted': 'Houston', 'question': 'Where do most university graduates stay after acquiring a degree?'}, '5ad42096604f3c001a400702': {'truth': '', 'predicted': '24,000', 'question': ' How many local jobs are produced by the University of Texas?'}, '5ad42096604f3c001a400704': {'truth': '', 'predicted': 'Houston', 'question': ' Where do most university graduates go to after acquiring a degree?'}, '5ad42096604f3c001a400705': {'truth': '', 'predicted': '80.5%', 'question': ' Even after five years, how many graduates remain in Texas?'}, '57282c833acd2414000df63d': {'truth': 'turned inside out to extend', 'predicted': 'turned inside out to extend it', 'question': \"What does 'everted' mean?\"}, '57282c833acd2414000df63e': {'truth': 'pharynx', 'predicted': 'muscular pharynx', 'question': 'What part of a polychaete can be everted?'}, '57282c833acd2414000df640': {'truth': 'seizing prey, biting off pieces of vegetation, or grasping dead and decaying matter', 'predicted': 'seizing prey', 'question': 'What do annelids use jaws for?'}, '57282c833acd2414000df641': {'truth': 'palps covered in cilia', 'predicted': '', 'question': \"What do some annelids have 'crowns' of?\"}, '5ace874132bba1001ae4a97f': {'truth': '', 'predicted': 'muscular pharynx', 'question': 'What part of a polychaete can be inverted?'}, '573238630fdd8d15006c6860': {'truth': 'Vestals', 'predicted': '', 'question': 'What group did Gratian seek the abolish?'}, '572618d0ec44d21400f3d8c3': {'truth': 'Antikythera mechanism', 'predicted': '', 'question': 'What is the name of the 37 gear computer which noted the motions of the Sun and Moon?'}, '572618d0ec44d21400f3d8c4': {'truth': '10th', 'predicted': '10th century', 'question': 'Until what century were similar devices like the Antikythera mechanism found?'}, '5709720ded30961900e84162': {'truth': 'Grape juice', 'predicted': 'Grape', 'question': 'What juice is made when grapes are crushed and blended?'}, '5727aa682ca10214002d9338': {'truth': 'French alone is the official language', 'predicted': '', 'question': 'What does the French constitution state for the language in Aslace?'}, '5727aa682ca10214002d933a': {'truth': 'one in ten children', 'predicted': 'one in ten', 'question': 'With Alsatian language on the decline, what is the ration of children using the language regularly today?'}, '5a6fe7e38abb0b001a67603a': {'truth': '', 'predicted': 'one in four', 'question': 'How many children speak French?'}, '57269b6b708984140094cb76': {'truth': 'they will be soluble, dissolving into the mixture', 'predicted': 'they will be soluble', 'question': 'What happens when an alloy is mixed with a molten base?'}, '5a2052ef54a786001a36b301': {'truth': '', 'predicted': 'admixture', 'question': 'What is a mixture of impure substances?'}, '5a2052ef54a786001a36b302': {'truth': '', 'predicted': 'admixture', 'question': 'What is a pure substance that retains the characteristics of a metal?'}, '5a2052ef54a786001a36b305': {'truth': '', 'predicted': 'wrought iron', 'question': 'What other metal is pure like an alloy?'}, '57105362b654c5140001f8ce': {'truth': 'varied', 'predicted': 'Government responses varied widely', 'question': 'Was government response to the Enlightenment uniformly positive or widely varied?'}, '57105362b654c5140001f8cf': {'truth': 'middle', 'predicted': 'middle classes', 'question': 'In which class did the Enlightenment reach deepest, expressing a nationalistic tone?'}, '572818a9ff5b5019007d9d24': {'truth': 'mass grave', 'predicted': 'a mass grave of Ukrainian and Polish victims of Stalinist terror', 'question': 'At what sight was the Bykivnia meeting held?'}, '57096228ed30961900e84042': {'truth': 'within and outside the country', 'predicted': 'outside', 'question': 'Has the demand increased inside or outside the country?'}, '5a362317788daf001a5f8748': {'truth': '', 'predicted': 'lack of marketing facilities', 'question': 'Why did the production of woolen and pashmina shawls decline?'}, '5a362317788daf001a5f874a': {'truth': '', 'predicted': 'increased', 'question': 'Has the demand for horese-hair bangles increased or decreased?'}, '57265630708984140094c2d2': {'truth': 'Stephan Kinsella', 'predicted': '', 'question': 'Who has objected to the idea of IP because \"property\" implies scarcity?'}, '57265630708984140094c2d3': {'truth': 'tangible', 'predicted': '', 'question': 'Having no natural scarcity makes IP different from what kind of property?'}, '57265630708984140094c2d4': {'truth': 'indefinitely', 'predicted': '', 'question': 'How much can IP be duplicated without diminishing the original?'}, '572675a3dd62a815002e85b3': {'truth': '400 million', 'predicted': 'roughly 400 million', 'question': 'How many people were added to the British Empire between 1815 and 1914?'}, '572675a3dd62a815002e85b4': {'truth': 'Russia', 'predicted': 'Russia in central Asia', 'question': \"Who was Britain's last serious rival after Napoleon?\"}, '5726dc97708984140094d3f6': {'truth': 'northern Soviet Union near Murmansk', 'predicted': 'Murmansk', 'question': 'Where was the sub base located?'}, '5726dc97708984140094d3f7': {'truth': 'both the Atlantic and the Pacific', 'predicted': '', 'question': 'Which oceans did the sub base provide access to?'}, '5ad28a70d7d075001a4299ba': {'truth': '', 'predicted': 'Murmansk', 'question': 'Where was the plane base located?'}, '5728a75e4b864d1900164b92': {'truth': 'Bartolomeo Bortolazzi', 'predicted': '', 'question': 'Who popularised the Cremonese Mandolin?'}, '5728a75e4b864d1900164b94': {'truth': 'four single-strings', 'predicted': 'four', 'question': 'How many strings did the Cremonese Mandolin have?'}, '5728a75e4b864d1900164b95': {'truth': 'uncomfortable to play', 'predicted': 'uncomfortable', 'question': 'Did Bortolazzi like playing the new wire strung mandolins? '}, '5728a75e4b864d1900164b96': {'truth': 'less pleasing...hard, zither-like tone', 'predicted': '', 'question': 'What did Bortolazzi say about the sound? '}, '5ad211e7d7d075001a4282e0': {'truth': '', 'predicted': 'four', 'question': ' How many double strings did the Cremonese Mandolin have?'}, '5726e2bf708984140094d4c5': {'truth': 'prove the existence of God', 'predicted': 'prove the existence of God and His creation of the world scientifically and through reason and logic', 'question': 'What did Avicenna hope to do through his work?'}, '5726e2bf708984140094d4c7': {'truth': 'the prophets', 'predicted': 'prophets', 'question': 'Who did Avicenna view as inspired philosophers?'}, '5ace618b32bba1001ae4a4cf': {'truth': '', 'predicted': 'Avicenna', 'question': 'Who wrote the curriculum at Islamic religious schools?'}, '5acea21632bba1001ae4ae19': {'truth': '', 'predicted': 'Muslim', 'question': 'What region was Avicenna?'}, '5acea21632bba1001ae4ae1c': {'truth': '', 'predicted': '19th', 'question': \"Up until what century was Avicenna's work slightly influential?\"}, '572ac9cbbe1ee31400cb8257': {'truth': 'Rosemary Forbes Kerry', 'predicted': 'Rosemary Forbes', 'question': \"What was Kerry's mother's name?\"}, '572ac9cbbe1ee31400cb8259': {'truth': 'third-richest', 'predicted': 'third', 'question': 'Where would Kerry have ranked among richest US presidents, adjusted for inflation?'}, '572947026aef051400154c4d': {'truth': 'married Pocahontas', 'predicted': '', 'question': 'What is one of the main things John Rolfe is known for?'}, '572947026aef051400154c4e': {'truth': '1612,', 'predicted': '1612', 'question': 'When did the English begin their intentional settlement of Bermuda?'}, '572947026aef051400154c50': {'truth': 'oldest continually inhabited English town in the New World.', 'predicted': '', 'question': 'What is St. George credited with?'}, '5ad40858604f3c001a3ffee3': {'truth': '', 'predicted': 'Pocahontas', 'question': 'Who did Rolfe John marry?'}, '5726fec6dd62a815002e973f': {'truth': '15 m (50 ft)', 'predicted': '', 'question': 'How much ice and snow is minimally necessary to begin to slide on steep glaciers?'}, '5a358b22788daf001a5f8625': {'truth': '', 'predicted': 'armchair-shaped geological feature (such as a depression between mountains enclosed by arêtes', 'question': 'What geological features formed by glaciers?'}, '56e763f737bdd419002c3f2d': {'truth': 'tissue paper', 'predicted': '', 'question': 'What is the lightest density of paper produced?'}, '5ad50a975b96ef001a10aa90': {'truth': '', 'predicted': '250 kg/m3', 'question': 'What is the range of paper weight?'}, '572f8a4904bcaa1900d76a69': {'truth': 'A-delta', 'predicted': '', 'question': 'What are some spinal cord fibers exclusive to?'}, '572f8a4904bcaa1900d76a6d': {'truth': 'somatosensory', 'predicted': 'primary and secondary somatosensory', 'question': 'Pain which is distinctly located also activates what cortices? '}, '5acd38ac07355d001abf3980': {'truth': '', 'predicted': 'the brain', 'question': 'Where do pain signals travel up to from the thalamus?'}, '5acd38ac07355d001abf3981': {'truth': '', 'predicted': 'spinal cord fibers', 'question': 'What are dynamic wide range neurons?'}, '5acd38ac07355d001abf3982': {'truth': '', 'predicted': 'touch, pressure and vibration', 'question': 'What signals do the A-delta fibers carry?'}, '5acd38ac07355d001abf3983': {'truth': '', 'predicted': 'the motivational element of pain', 'question': 'What does the cingulate cortex anterior embody?'}, '572688895951b619008f7603': {'truth': 'he personally recommended his successor', 'predicted': 'personally recommended his successor', 'question': 'What did Jack Brickhouse do when he approached his retirement age?'}, '572745d8f1498d1400e8f593': {'truth': 'draw in talent and ideas from all segments of the population', 'predicted': 'draw in talent and ideas from all segments of the population. By creating this diverse workforce, these employers and companies gain a competitive advantage in an increasingly global economy', 'question': 'Having a diverse workplace allows for employers to do what?'}, '572745d8f1498d1400e8f594': {'truth': 'U.S. Equal Employment Opportunity Commission', 'predicted': 'Equal Employment Opportunity Commission', 'question': 'Which organization claims that private sector employers believe having a diverse workplace is beneficial?'}, '570d2d61b3d812140066d4ee': {'truth': 'more than $850 million', 'predicted': '$850 million', 'question': 'How much money has GE invested in renewable energy commercialization?'}, '570d2d61b3d812140066d4f0': {'truth': 'more than 4,900', 'predicted': '4,900', 'question': 'As of 2009, how many people did GE employ in its renewable energy initiatives?'}, '59d291ec2763a600182840cf': {'truth': '', 'predicted': '2008', 'question': 'In what year was Kelman Ltd. formed?'}, '59d291ec2763a600182840d0': {'truth': '', 'predicted': '4,900', 'question': 'How many people did GE employ overall as of 2009?'}, '59d291ec2763a600182840d2': {'truth': '', 'predicted': '$850 million', 'question': 'How much was GE worth as of 2002?'}, '572a54e07a1753140016aeb4': {'truth': 'reminding broadcasters that analog transmitters had to be shut off by the deadline in mandatory markets', 'predicted': '', 'question': 'Why did the CTRC send out a bulletin to broadcasters?'}, '5a5535e0134fea001a0e19d6': {'truth': '', 'predicted': \"CBC's plans for transitioning to digital\", 'question': 'The senior director of the CRTC issued a letter to address what concern in November of 2010?'}, '5731dd32e17f3d14004224b9': {'truth': 'no-aid', 'predicted': 'no-aid position', 'question': 'What position do Jeffries and Ryan argue was the reason for support from a coalition of separationists?'}, '5731dd32e17f3d14004224bc': {'truth': 'anti-Catholic sentiment', 'predicted': '', 'question': 'What diminished after 1980?'}, '5726bbc15951b619008f7c59': {'truth': 'Raleigh also has two alternative high schools.', 'predicted': 'two alternative high schools', 'question': 'Does Raleigh have alternate high schools?'}, '571008d5a58dae1900cd67ea': {'truth': \"sexual orientation based on the relative amounts of heterosexual and homosexual experience or psychic response in one's history at a given time.\", 'predicted': 'sexual orientation', 'question': 'What does the Kinsey scale provide a classification for?'}, '571008d5a58dae1900cd67ed': {'truth': 'the actual amount of overt experience or psychic response', 'predicted': '', 'question': 'What positions does the KInsey scale not use?'}, '57265269708984140094c253': {'truth': 'They were originally kept as songbirds, and they are thought to have been regularly used in song contests.', 'predicted': 'They were originally kept as songbirds, and they are thought to have been regularly used in song contests', 'question': 'Have quails ever been used for entertainment purposes ?'}, '57265269708984140094c254': {'truth': 'it is found in bushy places, in rough grassland, among agricultural crops, and in other places with dense cover.', 'predicted': 'bushy places', 'question': 'Where can quails typically be found in the wild?'}, '57265269708984140094c255': {'truth': 'seeds, insects, and other small invertebrates.', 'predicted': 'seeds, insects, and other small invertebrates', 'question': 'What is the typical diet consist of for qails ?'}, '57265269708984140094c256': {'truth': 'modern domesticated flocks are mostly of Japanese quail (Coturnix japonica)', 'predicted': 'Japan', 'question': 'From what country do most domesticated  quails today descend from ?'}, '5a86079eb4e223001a8e73ce': {'truth': '', 'predicted': 'Japan', 'question': 'What country do all domesticated quails descend from?'}, '572b65e4be1ee31400cb835a': {'truth': 'twelve bird species', 'predicted': 'twelve', 'question': 'How many bird species have been driven to extinction in Guam?'}, '572b65e4be1ee31400cb835b': {'truth': \"ko'ko' birds\", 'predicted': '', 'question': 'What other bird was very common before WWII according to the elders?'}, '5ace640e32bba1001ae4a551': {'truth': '', 'predicted': \"Guam rail (or ko'ko' bird in Chamorro) and the Guam flycatcher\", 'question': 'What is thought to have been the first bird species on Guam?'}, '5ace640e32bba1001ae4a552': {'truth': '', 'predicted': \"ko'ko' birds\", 'question': 'What was the most populous Guam bird prior to World War II?'}, '5ace640e32bba1001ae4a554': {'truth': '', 'predicted': \"Guam rail (or ko'ko' bird in Chamorro) and the Guam flycatcher\", 'question': 'Which bird species was first driven to extinction by brown tree snakes?'}, '56e8daed0b45c0140094cd17': {'truth': 'coronation site', 'predicted': 'coronation', 'question': 'What was the abbey to Norman kings?'}, '56e8daed0b45c0140094cd19': {'truth': 'Richard II', 'predicted': '', 'question': 'Who was reigning when Henry Yevele finished his work on the abbey?'}, '56e8daed0b45c0140094cd1a': {'truth': 'Cosmati', 'predicted': 'Cosmati pavement', 'question': 'What kind of pavement was commissioned for in front of the High Altar?'}, '5ad3f42d604f3c001a3ff92b': {'truth': '', 'predicted': 'coronation site', 'question': 'What was the abbey to Norman queens?'}, '5ad3f42d604f3c001a3ff92e': {'truth': '', 'predicted': 'Cosmati', 'question': 'What kind of pavement was commissioned for in front of the Low Altar?'}, '5728ee664b864d19001650b4': {'truth': 'Yuan', 'predicted': 'Yuan dynasty', 'question': 'Which Chinese dynasty was founded by Mongols?'}, '5730501a396df91900096052': {'truth': 'The Boulton Paul Defiant', 'predicted': 'Boulton Paul Defiant', 'question': 'What performed better during night fighting?'}, '5728b1163acd2414000dfcd3': {'truth': 'The Dubliners,', 'predicted': 'The Dubliners', 'question': \"What was John Sheahan and Barney Mckenna's band called?\"}, '5728b1163acd2414000dfcd4': {'truth': 'UK luthier Roger Bucknall of Fylde Guitars', 'predicted': '', 'question': 'Who made the instruments used by the Dubliners?'}, '5ad22b4bd7d075001a4285f0': {'truth': '', 'predicted': 'Rory Gallagher', 'question': ' What Irish guitarist played the mandolin off stage?'}, '570a6e2f4103511400d596fa': {'truth': 'terror', 'predicted': '', 'question': 'What is an example of an extreme form of fear?'}, '570a6e2f4103511400d596fb': {'truth': 'embarrassment', 'predicted': '', 'question': 'What would be an example of mild shame?'}, '5ad24351d7d075001a4289e1': {'truth': '', 'predicted': 'Psychotherapist', 'question': \" What is Michael Graham's interest?\"}, '5ad24351d7d075001a4289e4': {'truth': '', 'predicted': 'Moods', 'question': ' What are intense feelings that lack a contextual stimulus called?'}, '570f40f65ab6b81900390eb5': {'truth': 'Melatonin', 'predicted': '', 'question': 'At its onset, what can be measured in blood or saliva?'}, '572a630e7a1753140016aefa': {'truth': 'a \"lifetime of looking beyond the horizon\"', 'predicted': '\"lifetime of looking beyond the horizon', 'question': \"What was the reason given for Hayek's 1991 award from the President?\"}, '5726f3eef1498d1400e8f0c7': {'truth': '3500 BC', 'predicted': '', 'question': 'How far back do the Mesopotamian people go?'}, '572fa91e04bcaa1900d76b67': {'truth': 'concentration gradients across membranes', 'predicted': 'concentration gradients', 'question': 'What is crucial for biochemical reactions?'}, '572fa91e04bcaa1900d76b68': {'truth': 'between the cytoplasm and the periplasmic space', 'predicted': 'across the cell membrane between the cytoplasm and the periplasmic space', 'question': 'How does electron transit occur in bacteria?'}, '5731e810e17f3d1400422537': {'truth': 'Abdu El Hamouli, Almaz and Mahmoud Osman, who influenced the later work of Sayed Darwish, Umm Kulthum, Mohammed Abdel Wahab and Abdel Halim Hafez', 'predicted': 'Sayed Darwish, Umm Kulthum, Mohammed Abdel Wahab and Abdel Halim Hafez', 'question': 'What artist are considered the golden age of Egyptian music?'}, '56e7b14c37bdd419002c4372': {'truth': 'six', 'predicted': '', 'question': 'How many teams participated in the 2015 CAFL tournament?'}, '56e7b14c37bdd419002c4373': {'truth': 'November', 'predicted': '', 'question': 'In what month did the CAFL tournament occur?'}, '5733f0774776f4190066156d': {'truth': 'The Council of Ministers', 'predicted': '', 'question': 'What group acts as the presidential cabinet?'}, '5733f0774776f4190066156e': {'truth': 'define the broad outline of its policies in a programme, and present it to the Assembly for a mandatory period of debate', 'predicted': 'define the broad outline of its policies in a programme', 'question': \"What process is required of each government's policies?\"}, '5733f0774776f4190066156f': {'truth': 'an absolute majority of deputies', 'predicted': '', 'question': \"What is needed to reject a cabinet's policy?\"}, '57296ab01d046914007793e8': {'truth': 'bulk metallic glasses', 'predicted': 'bulk metallic glasses (BMG)', 'question': 'What are thick alloys made in layers called?'}, '57296ab01d046914007793e9': {'truth': 'zirconium', 'predicted': '', 'question': 'What does Liquidmetal Technologies use for their alloys?'}, '57296ab01d046914007793ea': {'truth': 'amorphous steel', 'predicted': '', 'question': 'What type of metal makes better alloys than traditional steel?'}, '5a67151af038b7001ab0c1c6': {'truth': '', 'predicted': 'W. Klement', 'question': 'Who coined the term alloys?'}, '5a67151af038b7001ab0c1ca': {'truth': '', 'predicted': 'Caltech', 'question': 'At what university did Klement produce BMGs?'}, '5acfc92477cf76001a685f88': {'truth': '', 'predicted': 'Cubist', 'question': 'What style is The Desmoiselles?'}, '56dcf8689a695914005b94a1': {'truth': 'Congolese Observatory of Human Rights', 'predicted': 'the Congolese Observatory of Human Rights', 'question': 'Which group provided oversight for the electoral process in 2009?'}, '5ad00a9d77cf76001a6867e2': {'truth': '', 'predicted': 'the Congolese Observatory of Human Rights', 'question': 'Who said the election was marked by very high turnout?'}, '5ad00a9d77cf76001a6867e3': {'truth': '', 'predicted': 'the Congolese Observatory of Human Rights', 'question': 'What governmental organization commented on the election?'}, '56f78981aef2371900625baa': {'truth': 'the German Empire', 'predicted': 'Spain sold the islands to the German Empire', 'question': 'In 1884, which country purchased the Marshall Islands?'}, '56f952ba9b226e1400dd1313': {'truth': 'the German Empire', 'predicted': 'German Empire', 'question': 'Who bought the Marshall Islands from the Spanish in 1884?'}, '56f952ba9b226e1400dd1314': {'truth': 'Japan', 'predicted': 'Empire of Japan', 'question': 'Who occupied the Marshall Islands during the First World War?'}, '56cfebbd234ae51400d9c0c7': {'truth': '30% (4.65 EJ/yr)', 'predicted': '30%', 'question': 'How much energy does an HVAC system use in commercial locations?'}, '56cfebbd234ae51400d9c0c8': {'truth': '50% (10.1 EJ/yr)', 'predicted': '50%', 'question': 'How much energy does an HVAC system use in residential locations?'}, '5730eef6a5e9cc1400cdbb07': {'truth': 'unstressed /e/ and /a/ following palatalized consonants and preceding a stressed syllable', 'predicted': '', 'question': 'What is pronounced [a] in Southern Russian?'}, '5730eef6a5e9cc1400cdbb08': {'truth': 'unstressed /e/ and /a/ following palatalized consonants and preceding a stressed syllable', 'predicted': '', 'question': \"What is pronounced [ɪ] in Moscow's dialect?\"}, '5730eef6a5e9cc1400cdbb09': {'truth': 'modern Belarusian and some dialects of Ukrainian (Eastern Polesian)', 'predicted': '', 'question': 'What does Southern Russian have a linguistic continuum with?'}, '5a864143b4e223001a8e7509': {'truth': '', 'predicted': 'akanye', 'question': 'What is called yakanye in Standard and Northern dialects?'}, '572808cd2ca10214002d9c22': {'truth': 'Sunday, 27 April 2014', 'predicted': '27 April 2014', 'question': 'On what date was John XXIII and Pope John Paul II declared saints?'}, '5a61447ae9e1cc001a33d02e': {'truth': '', 'predicted': '3 June 2013', 'question': \"When was the 50th anniversary of Pope Francis's death?\"}, '5a61447ae9e1cc001a33d02f': {'truth': '', 'predicted': 'Bergamo', 'question': 'Where is Pope Francis originally from?'}, '56dfbe7c231d4119001abd71': {'truth': 'Asynchronous Transfer Mode', 'predicted': '', 'question': 'what does atm stand for in relation to internet providers? '}, '56dfbe7c231d4119001abd72': {'truth': 'customers with more demanding requirements', 'predicted': '', 'question': 'what is high-speed dsl used for? '}, '5a10d9ce06e79900185c3420': {'truth': '', 'predicted': 'ATM', 'question': \"What's the abbreviation for synchronous transfer mode\"}, '5a10d9ce06e79900185c3421': {'truth': '', 'predicted': 'SONET', 'question': 'What are the abbreviations for asynchronous optical networking'}, '572e8519c246551400ce42b3': {'truth': 'encased inside a vertical cylinder', 'predicted': 'vertical cylinder', 'question': 'The water pump supplied water pressure to a plunger located where?'}, '572fa79aa23a5019007fc83c': {'truth': 'October 2, 2008', 'predicted': '2008', 'question': 'When was the only vice presidential debate held at Washington University?'}, '5ace1b9532bba1001ae49adb': {'truth': '', 'predicted': '2008', 'question': 'In what year was the Washington University Athletic Complex built?'}, '5ace1b9532bba1001ae49adf': {'truth': '', 'predicted': 'presidential and vice-presidential', 'question': 'Was it presidential or vice presidential debate held at Washington University in 2000?'}, '5725eaf7271a42140099d31b': {'truth': 'two ways', 'predicted': 'two', 'question': 'In how many ways can one define luminous efficacy of a light source?'}, '5725c71189a1e219009abe88': {'truth': 'The Reinvent the Toilet Challenge is a long-term research and development effort to develop a hygienic, stand-alone toilet', 'predicted': '', 'question': 'What is reinvent the toilet trying to develop'}, '5725c71189a1e219009abe89': {'truth': 'This challenge is being complemented by another investment program to develop new technologies for improved pit latrine emptying', 'predicted': '', 'question': 'What compliments the challenge '}, '5725c71189a1e219009abe8a': {'truth': 'The aim of the \"Omni Processor\" is to convert excreta (for example fecal sludge) into beneficial products such as energy and soil nutrients', 'predicted': '', 'question': 'What does the Omni processor do'}, '572734eb5951b619008f86b6': {'truth': '1919–20.', 'predicted': '', 'question': 'what year did competition resume after world war 1?'}, '572734eb5951b619008f86b9': {'truth': \"didn't celebrate its centenary year until 1980–81\", 'predicted': '1980–81', 'question': 'When did the competition celebrate its centennial? '}, '5a8c802dfd22b3001a8d8954': {'truth': '', 'predicted': 'Ricky Villa', 'question': 'Who replaced the goal scored by Steven Gerrard?'}, '5735c0d8e853931400426b49': {'truth': '1955', 'predicted': '1906–1955', 'question': 'When did Tribhuvan die?'}, '5735c0d8e853931400426b4a': {'truth': '1920', 'predicted': '1920–1972', 'question': 'What was the birth year of King Mahendra?'}, '5735c0d8e853931400426b4c': {'truth': 'Birendra', 'predicted': '', 'question': 'Who was the penultimate king of Nepal?'}, '5735c0d8e853931400426b4d': {'truth': 'medieval', 'predicted': '', 'question': 'During what era was the Hanumandhoka Palace constructed?'}, '572830622ca10214002da034': {'truth': 'generally in moist leaf litter', 'predicted': 'moist leaf litter', 'question': 'Where do earthworms prefer to live on the surface?'}, '572830622ca10214002da036': {'truth': 'storks', 'predicted': 'robins to storks', 'question': 'What is the largest bird that eats earthworms?'}, '5ace8a7232bba1001ae4a9db': {'truth': '', 'predicted': 'moist leaf litter', 'question': 'Where do earthworms prefer to live in space?'}, '56ddfe9d4396321400ee2539': {'truth': 'three', 'predicted': '', 'question': \"Rather than four-year Bachelor's degrees, Politeknik offer a diploma after how many years?\"}, '56f8d4209b226e1400dd10af': {'truth': 'reasons of sustainability', 'predicted': 'reasons of sustainability of the fragile Alpine terrain', 'question': 'Why are villages considering becoming car free zones?'}, '56e79c1c00c9c71400d773a1': {'truth': 'a humid subtropical climate', 'predicted': 'humid subtropical', 'question': 'What type of climate does Nanjing enjoy?'}, '56e79c1c00c9c71400d773a2': {'truth': 'the East Asian monsoon', 'predicted': 'East Asian', 'question': 'What monsoon affects Nanjing?'}, '56e79c1c00c9c71400d773a3': {'truth': 'Chongqing and Wuhan', 'predicted': 'Chongqing and Wuhan, Nanjing is traditionally referred to as one of the \"Three Furnacelike Cities\" along the Yangtze River', 'question': 'Nanjing is one of three \"Furnacelike\" cities. What are the other two cities?'}, '56e79c1c00c9c71400d773a4': {'truth': '115 days', 'predicted': '115', 'question': 'How many days of rain does Nanjing get a year, on average?'}, '56e79c1c00c9c71400d773a5': {'truth': '1,983 hours', 'predicted': '1,983', 'question': 'How many hours of bright sunshine does Nanjing get each year?'}, '570ac16f4103511400d5998d': {'truth': 'heavily laden aircraft', 'predicted': 'aircraft', 'question': 'What cannot launch using a ski-jump due to their high loaded weight?'}, '5acd84c307355d001abf453a': {'truth': '', 'predicted': 'penalty it exacts on aircraft size, payload, and fuel load (and thus range)', 'question': 'What is the advantage of the ski-jump?'}, '5acd84c307355d001abf453b': {'truth': '', 'predicted': 'aircraft', 'question': 'What can launch using a ski-jump due to their high loaded weight?'}, '5acd84c307355d001abf453c': {'truth': '', 'predicted': 'a catapult or JATO rocket', 'question': 'What do lightly laden aircraft sometimes require the assistance from?'}, '5acd84c307355d001abf453e': {'truth': '', 'predicted': 'with a minimal armament and fuel load', 'question': 'How is the Russian SU-33 unable to launch from the carrier Admiral Kuznetsov?'}, '572901d03f37b31900477f73': {'truth': '75', 'predicted': '75%', 'question': \"What percentage of France's Jewish population survived the holocaust?\"}, '56cdd83862d2951400fa68e0': {'truth': 'MGM', 'predicted': '', 'question': 'Who ended up with ownership of Spectre?'}, '56cdd83862d2951400fa68e3': {'truth': '2013', 'predicted': '', 'question': 'In what year were rights to Spectre worked out?'}, '56cf39c4aab44d1400b88eba': {'truth': 'MGM', 'predicted': '', 'question': 'Which film studio won the full copyright film rights to Spectre?'}, '5ad22870d7d075001a42855b': {'truth': '', 'predicted': 'Danjaq, LLC', 'question': 'Who is a brother company to Eon Productions?'}, '57267c12dd62a815002e86c6': {'truth': 'the First World War', 'predicted': 'First World War', 'question': \"The passed Home Rule Bill wasn't implemented because of which war?\"}, '57321a39e99e3014001e651c': {'truth': 'since prehistoric times', 'predicted': 'prehistoric times', 'question': 'When were birds represented in early cave paintings?'}, '57321a39e99e3014001e6520': {'truth': 'The Rime of the Ancient Mariner', 'predicted': \"Samuel Taylor Coleridge's The Rime of the Ancient Mariner\", 'question': 'The relationship between an albatross and a sailor is the central theme of what book?'}, '5731f0ffe99e3014001e63ed': {'truth': 'Decius Mus', 'predicted': 'Decius', 'question': 'What Roman general had a dream of his fate in battle?'}, '5731f0ffe99e3014001e63ef': {'truth': 'disastrous consequences', 'predicted': '', 'question': 'By dying what did Decius avoid for the battle?'}, '5731f0ffe99e3014001e63f0': {'truth': 'Livy', 'predicted': '', 'question': 'Who wrote a detailed account of the demise of Decius Mus?'}, '57324b39b9d445190005e9d1': {'truth': 'an Armenian-styled barbecue', 'predicted': 'barbecue', 'question': 'What is khorovats?'}, '57324b39b9d445190005e9d2': {'truth': 'Armenian flat bread', 'predicted': '', 'question': 'What is lavash?'}, '57324b39b9d445190005e9d3': {'truth': 'a popular dessert made from filo dough', 'predicted': 'dessert made from filo dough', 'question': 'What is paklava?'}, '57324b39b9d445190005e9d5': {'truth': 'a rice-stuffed pumpkin dish', 'predicted': 'rice-stuffed pumpkin dish', 'question': 'What is ghapama?'}, '5a53f1c6bdaabd001a386814': {'truth': '', 'predicted': 'khorovats', 'question': 'What is the favorite fruit that is part of Armenian-style barbecue?'}, '5a53f1c6bdaabd001a386817': {'truth': '', 'predicted': 'Peaches', 'question': 'What is a dish called that uses fruits, grapes and figs?'}, '5a53f1c6bdaabd001a386818': {'truth': '', 'predicted': 'cabbage', 'question': 'What is a popular grape leaf in Armenia?'}, '5727f9a14b864d190016410c': {'truth': 'early 1930s', 'predicted': '', 'question': 'When was the Western Electric System introduced?'}, '5727f9a14b864d190016410d': {'truth': 'Bell Telephone Laboratories and Western Electric', 'predicted': 'Bell Telephone Laboratories', 'question': 'What two companies worked together to develop the Western Electric System?'}, '5727f9a14b864d190016410e': {'truth': 'Western Electric system', 'predicted': '', 'question': 'What system was used by Warner Brothers?'}, '5727f9a14b864d1900164110': {'truth': 'improve the overall quality', 'predicted': 'improve the overall quality of disc recording and playback', 'question': 'What was the Western Electric System believed to do?'}, '572a31481d0469140077982f': {'truth': 'nine Digimon movies', 'predicted': 'nine', 'question': 'How many Digimon movies have been released in Japan?'}, '572a31481d04691400779831': {'truth': 'October 6, 2000', 'predicted': '', 'question': 'What year was Digimon: The movie released in the US/Canada?'}, '5a110ff906e79900185c3515': {'truth': '', 'predicted': 'nine', 'question': 'How many Digimon movies have been released in the United States?'}, '56de2d84cffd8e1900b4b62c': {'truth': 'not', 'predicted': 'a building is not truly a work of architecture unless it is in some way \"adorned\"', 'question': 'Does Ruskin believe all buildings are works of architecture?'}, '56e4731e8c00841900fbaf93': {'truth': 'the aesthetic', 'predicted': 'aesthetic', 'question': 'What was most significant in architecture according to Ruskin?'}, '56e4731e8c00841900fbaf94': {'truth': 'it is in some way \"adorned\"', 'predicted': 'it is in some way \"adorned', 'question': \"To be true architecture in Ruskin's opinion what should be done to a structure?\"}, '5acf9cb577cf76001a6854df': {'truth': '', 'predicted': 'string courses or rustication', 'question': 'What features at maximum did Ruskin insist on for a building to be considered functional?'}, '5725ee0a38643c19005aceab': {'truth': 'infrared radiation', 'predicted': 'infrared', 'question': 'What type of radiation makes up the majority of tungsten filament emissions?'}, '5725ee0a38643c19005aceac': {'truth': 'the light emitted does not appear white,', 'predicted': '', 'question': 'What is the flaw in the color of light produced by an incandescent bulb?'}, '5725c64b271a42140099d18f': {'truth': 'universities in the U.S., Europe, India, China and South Africa, have received grants to develop innovative on-site and off-site waste treatment solutions', 'predicted': 'Europe, India, China and South Africa', 'question': 'What countries have received grants '}, '5725c64b271a42140099d190': {'truth': 'The grants were in the order of 400,000 USD for their first phase, followed by typically 1-3 million USD for their second phase', 'predicted': '400,000 USD', 'question': 'How much were the grants for '}, '5725c64b271a42140099d191': {'truth': 'many of them investigated resource recovery or processing technologies for excreta or fecal sludge.', 'predicted': 'resource recovery or processing technologies for excreta or fecal sludge', 'question': 'What did many investigate '}, '5a0cc174f5590b0018dab542': {'truth': '', 'predicted': 'resource recovery or processing technologies for excreta or fecal sludge', 'question': 'What did the Reinvent the Toilet Challenge investigate?'}, '5a0cc174f5590b0018dab544': {'truth': '', 'predicted': '400,000 USD', 'question': 'After receiving 1-3 million for the first phase how much were the grants for the second phase?'}, '5a0cc174f5590b0018dab545': {'truth': '', 'predicted': 'more than a dozen', 'question': 'How many teams received grants to help universities?'}, '5a0cc174f5590b0018dab546': {'truth': '', 'predicted': 'processing technologies for excreta or fecal sludge', 'question': 'Since the launch of investigating resource recovery what did the grants develop?'}, '5ad1248d645df0001a2d0f21': {'truth': '', 'predicted': '1,360 GW', 'question': 'By the beginning of 2011, total renewable power capacity worldwide exceeded what number?'}, '5ad27e04d7d075001a4296f6': {'truth': '', 'predicted': 'Detroit techno music', 'question': 'What is Juan House an originator of?'}, '5ad27e04d7d075001a4296f7': {'truth': '', 'predicted': 'the exclusive association of particular tracks with particular clubs and DJs', 'question': 'What did House claim the term house Atkins reflected?'}, '5ad27e04d7d075001a4296f8': {'truth': '', 'predicted': 'to maintain such exclusives', 'question': 'Why were DJs inspired to create their own Atkins records?'}, '5ad27e04d7d075001a4296f9': {'truth': '', 'predicted': 'Juan Atkins', 'question': 'Who was the originator of DJ music?'}, '5ad27e04d7d075001a4296fa': {'truth': '', 'predicted': 'house\" records', 'question': 'What was Juan Atkins inspired to create?'}, '570e437d0dc6ce1900204eeb': {'truth': 'Martin Heinrich Klaproth', 'predicted': '', 'question': 'Who discovered uranium?'}, '570e437d0dc6ce1900204eec': {'truth': 'Berlin', 'predicted': '', 'question': 'In what city was uranium discovered?'}, '570e437d0dc6ce1900204eed': {'truth': '1789', 'predicted': '', 'question': 'In what year did the discovery of uranium occur?'}, '570e437d0dc6ce1900204eee': {'truth': 'sodium diuranate', 'predicted': '', 'question': 'What did Klaproth probably create when he dissolved pitchblende in nitric acid?'}, '570e437d0dc6ce1900204eef': {'truth': 'William Herschel', 'predicted': '', 'question': 'Who discovered the planet Uranus?'}, '56e6d988de9d371400068084': {'truth': 'the 2005-2007 economic downturn', 'predicted': '', 'question': 'Along with the recession, what broad economic trend marked the decline of adult contemporary radio stations?'}, '5729667c3f37b3190047833b': {'truth': 'anarchists and the Church, specifically the Archbishop of Palermo', 'predicted': 'anarchists and the Church', 'question': 'Who was blamed for the week long rebellion of 1866?'}, '5a3ea39b5a76c5001a3a83f0': {'truth': '', 'predicted': 'The majority of Sicilians', 'question': 'Who prefered the Savoia kingdom to independence?'}, '5a3ea39b5a76c5001a3a83f3': {'truth': '', 'predicted': 'Italian', 'question': 'What government enacted pro-sicilian policies?'}, '572e7aa8dfa6aa1500f8d00f': {'truth': 'American', 'predicted': '', 'question': 'Which version of North American football has smaller end zones?'}, '5a0dcbf06e16420018587b3a': {'truth': '', 'predicted': 'neutral zone', 'question': 'In America what term is used for both Canadian foobal and Americam football'}, '5a0dcbf06e16420018587b3b': {'truth': '', 'predicted': '10 yards', 'question': 'An American football field is how much wider than a Canadian football field?'}, '5a0dcbf06e16420018587b3c': {'truth': '', 'predicted': '12 players', 'question': 'Canadian and American football have the same number of what on the field?'}, '5a0dcbf06e16420018587b3d': {'truth': '', 'predicted': 'gain 10 yards', 'question': 'American football players have three downs to do what?'}, '5a0f39cfdecec900184754ed': {'truth': '', 'predicted': 'three', 'question': 'In American football how many downs are needed to gain 10 yards?'}, '5a0f39cfdecec900184754ee': {'truth': '', 'predicted': 'at least 1 yard', 'question': 'When a down begins, how far away must the American defending team be?'}, '56df4fb48bc80c19004e4a60': {'truth': 'South Oklahoma City', 'predicted': '', 'question': 'Which side is known for primarily being industrial?'}, '572f39d804bcaa1900d7679f': {'truth': 'seed-dispersal', 'predicted': '', 'question': 'What does a plant get out of forming fruit?'}, '572f39d804bcaa1900d767a0': {'truth': 'fragile', 'predicted': 'too fragile', 'question': 'What are many mutualistic relationships, thus failing to survive competition? '}, '5a3ad7933ff257001ab842b4': {'truth': '', 'predicted': 'land plant life', 'question': 'What did mutualistic relationships spread to eventually become?'}, '56df84d756340a1900b29cd3': {'truth': 'the public Horace Mann School for the Deaf', 'predicted': 'Horace Mann School', 'question': 'What name does the Boston School for Deaf Mutes go by now?'}, '56df84d756340a1900b29cd4': {'truth': 'April', 'predicted': 'April 1871', 'question': 'What month did Bell go to Boston?'}, '56df84d756340a1900b29cd6': {'truth': 'Hartford', 'predicted': 'Hartford, Connecticut', 'question': 'What city was the American Asylum in?'}, '5726c1825951b619008f7d52': {'truth': 'approximately two hundred square miles', 'predicted': '', 'question': 'During the ice age, what area of land was above water?'}, '5726c1825951b619008f7d53': {'truth': 'The top of the seamount has gone through periods of complete submergence', 'predicted': '', 'question': 'Has the seamount always been above sealevel?'}, '572931113f37b319004780c9': {'truth': 'a submarine volcano', 'predicted': 'submarine volcano', 'question': 'What type of volcano forms the archipelago?'}, '572931113f37b319004780ca': {'truth': 'periods of complete submergence,', 'predicted': '', 'question': 'Why is the top of the seamount formed by marine organisms?'}, '572931113f37b319004780cb': {'truth': 'an island', 'predicted': '', 'question': 'What was the result of the whole cladera being above sea level during the Ice Ages?'}, '5ad3e9c6604f3c001a3ff6a4': {'truth': '', 'predicted': 'a range', 'question': 'What is the volcano part of?'}, '5ad3e9c6604f3c001a3ff6a6': {'truth': '', 'predicted': 'submarine volcano', 'question': 'What does the seamount form?'}, '57279543f1498d1400e8fcc7': {'truth': 'commune, canton and federal levels', 'predicted': 'the commune, canton and federal levels', 'question': 'What are the legal jurisdictions that Swiss citizens are subject to?'}, '57279543f1498d1400e8fcc8': {'truth': 'direct', 'predicted': 'direct democracy', 'question': 'What type of democracy was defined in the 1848 federal constitution?'}, '5730187d947a6a140053d0d2': {'truth': 'Sabino Creek', 'predicted': '', 'question': 'What is the Foothills west of?'}, '5730187d947a6a140053d0d4': {'truth': 'Oracle Road', 'predicted': '', 'question': 'What is the Foothills east of?'}, '5730187d947a6a140053d0d5': {'truth': 'upscale outdoor shopping mall', 'predicted': 'an upscale outdoor shopping mall', 'question': 'What is La Encantada?'}, '573428154776f419006619b6': {'truth': 'Catalina Foothills', 'predicted': 'Catalina Foothills, located in the foothills of the Santa Catalina Mountains', 'question': 'Where are the most expensive homes in the Tucson metro area?'}, '573428154776f419006619b7': {'truth': 'River Road', 'predicted': '', 'question': 'What is the southern edge of the Catalina Foothills area?'}, '5a74e30342eae6001a389b0c': {'truth': '', 'predicted': 'Osiris', 'question': 'What was the underworld called?'}, '5a74e30342eae6001a389b0d': {'truth': '', 'predicted': 'Osiris', 'question': 'Who ruled the ancient Egyptians?'}, '56ce7376aab44d1400b887a8': {'truth': 'the 16th', 'predicted': '16th', 'question': 'In what century were the first modern greenhouses constructed?'}, '56d0875b234ae51400d9c34a': {'truth': 'produce cucumbers year-round for the Roman emperor Tiberius', 'predicted': 'cucumbers', 'question': 'What was one of the first uses of a greenhouse?'}, '572eb9a703f98919007569a7': {'truth': 'Ibn al-Haytham and Al-Biruni', 'predicted': '', 'question': 'Which two Muslim scientists did Salam celebrate as inventors of empirical methods?'}, '5ad2292fd7d075001a42857c': {'truth': '', 'predicted': 'Abdus Salam', 'question': 'Which biologist quoted the Quran in his address after receiving the Nobel Prize?'}, '5ad2292fd7d075001a42857f': {'truth': '', 'predicted': '\"creation from nothing', 'question': 'Salam suggests physics and science be kept together from which topics which are more suited to religion?'}, '5ad2292fd7d075001a428580': {'truth': '', 'predicted': '\"creation from nothing', 'question': 'Salam suggests physics and science be kept separate from which topics which are less suited to religion?'}, '5727c25f2ca10214002d9592': {'truth': 'formalist', 'predicted': '', 'question': 'What is another word for a thin definition?'}, '5727c25f2ca10214002d9593': {'truth': 'substantive', 'predicted': '', 'question': 'What is another word for a thick definition?'}, '5727c25f2ca10214002d9595': {'truth': 'specific procedural attributes', 'predicted': 'procedural attributes', 'question': 'On what do aspects of the rule of law do formalist definitions focus?'}, '5727c25f2ca10214002d9596': {'truth': 'functional', 'predicted': '', 'question': 'What is the third and lesser referred to approach on defining the rule of law?'}, '56e10245e3433e1400422a98': {'truth': 'timing of 0.2 microseconds, and speed of 0.2 meters/second', 'predicted': '0.2 meters/second', 'question': 'What was the timing and speed that China promised to offer in 2008 with the BeiDou system?'}, '5728d9e23acd2414000e0032': {'truth': 'terrorism and building an ownership society', 'predicted': 'defending America against terrorism and building an ownership society', 'question': 'Which two topics did Bush remain steadfast on, during his campaign?'}, '5a7224040efcfe001a8afe47': {'truth': '', 'predicted': 'March 10, 2004', 'question': 'When did Dick Cheney get enough delegates to be nominated at the RNC?'}, '5a7224040efcfe001a8afe48': {'truth': '', 'predicted': 'September 2, 2004', 'question': 'When did Cheney accept the nomination?'}, '5a7224040efcfe001a8afe49': {'truth': '', 'predicted': 'New York City', 'question': 'In what city was Cheney nominated at the RNC in 2004?'}, '5a7224040efcfe001a8afe4a': {'truth': '', 'predicted': 'defending America against terrorism and building an ownership society', 'question': 'What two themes did Cheney speak about often during his campaign?'}, '5a7224040efcfe001a8afe4b': {'truth': '', 'predicted': 'defending America against terrorism and building an ownership society', 'question': 'What was one idea endorsed by Cheney in his 2004 speech?'}, '5732a7f71d5d2e14009ff87f': {'truth': 'the Himalayan orogeny', 'predicted': 'Himalayan orogeny', 'question': 'Which oregeny was created when India collided with Asia?'}, '5a4ebb46af0d07001ae8cc0a': {'truth': '', 'predicted': 'Europe, Greenland and North America', 'question': 'What three continent drifted together forming Laurasia during then Eocene?'}, '5a4ebb46af0d07001ae8cc0c': {'truth': '', 'predicted': 'the North Atlantic', 'question': 'What sea opened up?'}, '56e8648f37bdd419002c44eb': {'truth': 'Zytglogge', 'predicted': 'Zytglogge tower', 'question': 'What was the name of the tower that was the western boundary?'}, '56e8648f37bdd419002c44ec': {'truth': 'Käfigturm', 'predicted': '', 'question': 'What tower took over after Zytglogge?'}, '56e8648f37bdd419002c44ee': {'truth': 'the whole area of the peninsula', 'predicted': 'whole area of the peninsula', 'question': 'What did the big and small Schanze protect?'}, '5728ddb1ff5b5019007da885': {'truth': 'committed troops to the Korean War and attempted to ban the Communist Party of Australia in an unsuccessful referendum during the course of that war', 'predicted': 'troops to the Korean War', 'question': \"What actions showed Menzies' anti-Communist beliefs?\"}, '5728ddb1ff5b5019007da886': {'truth': 'concerns about the influence of the Communist Party over the Trade Union movement,', 'predicted': 'concerns about the influence of the Communist Party over the Trade Union movement', 'question': 'Over what did the Labor party divide?'}, '5a6a5f32a9e0c9001a4e9db6': {'truth': '', 'predicted': 'Anti-communism', 'question': 'What was a key political topic of the Democratic Labor Party?'}, '5a6a5f32a9e0c9001a4e9db7': {'truth': '', 'predicted': 'troops to the Korean War', 'question': 'What actions showed Stalin anti-Communist beliefs?'}, '5a6a5f32a9e0c9001a4e9db8': {'truth': '', 'predicted': 'Trade Union movement', 'question': 'Over what did the Soviet Union divide?'}, '5a6a5f32a9e0c9001a4e9dba': {'truth': '', 'predicted': \"to improve pit miners' working conditions\", 'question': 'Why did the Soviet Union lead a coal strike?'}, '572b85aef75d5e190021fe20': {'truth': 'plant sources', 'predicted': '', 'question': 'What is one way that vegetarians and vegans obtain zinc?'}, '572b85aef75d5e190021fe22': {'truth': 'seeds and cereal bran,', 'predicted': 'seeds and cereal bran', 'question': 'Where is zinc chelator phytate found?'}, '572b85aef75d5e190021fe23': {'truth': 'diet is high in phytates,', 'predicted': '', 'question': 'What kind of diet may require more than 15mg of zinc daily?'}, '5acfcdfc77cf76001a6860cd': {'truth': '', 'predicted': 'phytates', 'question': 'What is found in whole grains that can help with zinc absorption?'}, '570a5afd6d058f1900182d98': {'truth': 'unconscious', 'predicted': 'conscious or unconscious and may or may not take the form of conceptual processing', 'question': \"In Lazarus' view, what could the cognitive activity be if it was not conscious?\"}, '572acbfe111d821400f38d7f': {'truth': 'Tel Aviv', 'predicted': 'Tel Aviv and Florence', 'question': 'What city in Israel is a sister city to Philadelphia?'}, '572acbfe111d821400f38d81': {'truth': 'Copernicus monument', 'predicted': 'the Copernicus monument', 'question': 'What else does the Triangle contain?'}, '5726ba315951b619008f7c04': {'truth': 'too soft', 'predicted': '', 'question': 'Why was tin was rarely used for everyday use?'}, '5726ba315951b619008f7c07': {'truth': 'lead, antimony, bismuth or copper', 'predicted': '', 'question': 'What metals were alloyed with tin to make it stronger?'}, '57277d20f1498d1400e8f991': {'truth': 'Yale', 'predicted': 'Yale and Yale-New Haven', 'question': 'What entity serves as the largest employer in New Haven?'}, '57277d20f1498d1400e8f993': {'truth': 'Yale – New Haven Hospital', 'predicted': 'Yale', 'question': 'What is the second largest employer in New Haven?'}, '57277d20f1498d1400e8f994': {'truth': 'Alexion', 'predicted': 'Alexion Pharmaceuticals, Covidien and United Illuminating', 'question': 'What pharmaceutical company serves as a large employment provider for New Haven? '}, '5729778f6aef051400154f5a': {'truth': 'manufacturing', 'predicted': 'manufacturing, but the postwar period brought rapid industrial decline; the entire Northeast was affected, and medium-sized cities with large working-class populations, like New Haven, were hit particularly hard. Simultaneously, the growth and expansion of Yale University', 'question': 'New Haven relied on what in terms of growth and economy?'}, '5729778f6aef051400154f5c': {'truth': 'over half (56%)', 'predicted': 'over half (56%', 'question': 'In modern day how much does New Haven depend on blue collar jobs?'}, '5729778f6aef051400154f5d': {'truth': 'Yale', 'predicted': 'Yale – New Haven Hospital', 'question': \"What institution has largest impact on the city's job market?\"}, '57315327e6313a140071ce20': {'truth': 'Syria in the east, to Cyrene to the west, and south to the frontier with Nubia', 'predicted': 'southern Syria in the east, to Cyrene to the west, and south to the frontier with Nubia', 'question': 'What were the reaches of the Ptolemaic Kingdom?'}, '57315327e6313a140071ce22': {'truth': 'Pharaohs', 'predicted': 'successors to the Pharaohs', 'question': 'What were leaders known as during Ptolemaic Kingdom?'}, '5729e5501d0469140077965b': {'truth': 'Elastic energy in materials', 'predicted': 'Elastic energy', 'question': 'What is dependent upon electrical potential energy?'}, '5acd166907355d001abf341a': {'truth': '', 'predicted': 'kinetic', 'question': 'All types of energy are a varying mix of potential and what other kind of energy?'}, '56e1688fe3433e1400422eba': {'truth': 'every fourth year', 'predicted': 'fourth year', 'question': 'How often are elections for mayor held in Boston?'}, '56e1688fe3433e1400422ebb': {'truth': 'extensive executive power', 'predicted': 'executive power', 'question': 'What kind of power does the mayor have?'}, '56e1688fe3433e1400422ebe': {'truth': 'The School Committee', 'predicted': 'School Committee', 'question': 'Who oversees the Boston Public Schools?'}, '56f81fe6aef2371900625df8': {'truth': 'Paleolithic era', 'predicted': 'Paleolithic', 'question': 'Evidence of human habitation in the Alps goes as far back to what era? '}, '57268d16dd62a815002e8946': {'truth': 'Many Somerset soldiers died during the First World War, with the Somerset Light Infantry suffering nearly 5,000 casualties', 'predicted': '5,000', 'question': 'How many Somerset soldiers were killed in WW1'}, '57268d16dd62a815002e8947': {'truth': 'only nine, described as the Thankful Villages, had none of their residents killed', 'predicted': '', 'question': 'How many counties had no casualties in WW1 '}, '57268d16dd62a815002e8948': {'truth': 'for troops preparing for the D-Day landings', 'predicted': 'troops preparing for the D-Day landings', 'question': 'The county was base for what in WW2'}, '57268d16dd62a815002e8949': {'truth': 'The Taunton Stop Line was set up to repel a potential German invasion', 'predicted': 'Taunton Stop Line', 'question': 'What is the Tauton stop line '}, '5acf5b6977cf76001a684c18': {'truth': '', 'predicted': 'nine', 'question': 'What was one of the Thankful Villages?'}, '5acf5b6977cf76001a684c19': {'truth': '', 'predicted': 'Somerset Light Infantry', 'question': 'Which village suffered the most First World War casualties?'}, '5acf5b6977cf76001a684c1a': {'truth': '', 'predicted': '5,000', 'question': 'How many Somerset soldiers died in all in the First World War?'}, '5acf5b6977cf76001a684c1b': {'truth': '', 'predicted': '5,000', 'question': 'How many Somerset soldiers did in the Second World War?'}, '572950423f37b3190047822d': {'truth': 'May 1994.', 'predicted': '', 'question': 'When was homosexuality legalized in Bermuda?'}, '572950423f37b3190047822e': {'truth': 'The OBA government simultaneously introduced a bill to permit Civil Unions', 'predicted': '', 'question': 'What occurred in February of 2016?'}, '572950423f37b3190047822f': {'truth': 'same sex spouses of Bermuda citizens could not be denied basic Human Rights.', 'predicted': '', 'question': 'What did the Chief Justice decide?'}, '573149e7e6313a140071cdce': {'truth': 'Nintendo Wii', 'predicted': \"Nintendo Wii's sensor bar\", 'question': 'What video game console uses infrared LEDs?'}, '573149e7e6313a140071cdd0': {'truth': 'RGB LEDs', 'predicted': 'RGB', 'question': 'Some flatbed scanners use what type of LED?'}, '573149e7e6313a140071cdd1': {'truth': 'warm-up', 'predicted': '', 'question': 'By using LEDs, scanners do not have to what?'}, '573149e7e6313a140071cdd2': {'truth': 'increase photosynthesis in plants', 'predicted': 'increase photosynthesis', 'question': 'Grow lights use LEDs for what process?'}, '5ad1981f645df0001a2d20e2': {'truth': '', 'predicted': 'RGB LEDs', 'question': ' Some flatbed scanners use what type of what?'}, '57268054dd62a815002e8765': {'truth': 'Pesticides are substances meant for attracting, seducing, and then destroying any pest', 'predicted': 'attracting, seducing, and then destroying any pest', 'question': 'What is the purpose of a pesticide?'}, '57268054dd62a815002e8766': {'truth': 'The most common use of pesticides is as plant protection products', 'predicted': 'plant protection products', 'question': 'What are pesticides most commonly used for?'}, '57268054dd62a815002e8767': {'truth': 'protect plants from damaging influences such as weeds, fungi, or insects', 'predicted': 'damaging influences such as weeds, fungi, or insects', 'question': 'What can pesticides protect plants from?'}, '572781a5f1498d1400e8fa1f': {'truth': 'mayor', 'predicted': 'the mayor', 'question': 'Who is elected every even numbered year?'}, '572781a5f1498d1400e8fa20': {'truth': 'Two', 'predicted': \"Two council members are elected from each of the city's five\", 'question': \"How many council members are elected for the city's ward?\"}, '5ace433e32bba1001ae4a115': {'truth': '', 'predicted': 'Christopher Taylor', 'question': 'Who is the current Republican mayor of Ann Arbor?'}, '5727d0914b864d1900163dc2': {'truth': 'average 62', 'predicted': '62', 'question': 'How many tornadoes hit Oklahoma each year?'}, '5727d0914b864d1900163dc3': {'truth': 'severe thunderstorms, damaging thunderstorm winds, large hail and tornadoes', 'predicted': 'thunderstorms, damaging thunderstorm winds, large hail and tornadoes', 'question': 'What types of severe weather does Oklahoma get?'}, '570b4c3c6b8089140040f860': {'truth': 'The War on Terrorism', 'predicted': 'War on Terrorism', 'question': 'What term is given to the attempt by the US and her allies to fight global terrorist groups?'}, '570b4c3c6b8089140040f863': {'truth': 'the September 11, 2001 attacks', 'predicted': 'September 11, 2001', 'question': 'The War On Terrorism was caused by what event?'}, '5ad1787c645df0001a2d1d71': {'truth': '', 'predicted': 'Islamic Extremist', 'question': ' What non-religious groups are primarily targeted by this war?'}, '5ad1787c645df0001a2d1d72': {'truth': '', 'predicted': 'Islamic Extremist', 'question': 'What is one prominent, specific terrorist group targeted by the War on Terrorism?'}, '5ad1787c645df0001a2d1d74': {'truth': '', 'predicted': 'Arkansas and Texas', 'question': 'Since the start of the war on Terrorism, attacks on UK service members have occurred in which two UK states?'}, '57303157b2c2fd1400568a39': {'truth': '\"tweak\" the match', 'predicted': 'to cancel a certain amount of reactance in order to \"tweak\" the match', 'question': 'Why would a transmitter have additional adjustments?'}, '572810aa3acd2414000df394': {'truth': 'Church of England in the Island', 'predicted': 'Church of England', 'question': 'This body makes provisions in respect to matters concerning whom?'}, '572a9e1bf75d5e190021fba6': {'truth': '1994', 'predicted': '', 'question': 'Before what year was royal assent approved by Order in Council?'}, '572a9e1bf75d5e190021fba7': {'truth': 'the lieutenant governor', 'predicted': 'lieutenant governor', 'question': 'Who currently holds the power to grant royal assent to measures?'}, '572a9e1bf75d5e190021fba8': {'truth': 'Between 1979 and 1993', 'predicted': '1993', 'question': 'During which years did the Synod have power to enact measures?'}, '5acfb9fb77cf76001a685ab3': {'truth': '', 'predicted': 'Order in Council', 'question': 'Before 1994, royal dissent was granted by whom?'}, '5acfb9fb77cf76001a685ab4': {'truth': '', 'predicted': 'lieutenant governor', 'question': 'Currently, royal dissent has been delegated to whom?'}, '56ddc1d966d3e219004dacd3': {'truth': '2009', 'predicted': '', 'question': 'When did Internet Archive chance its platform for data storage?'}, '5a6b0974a9e0c9001a4e9e89': {'truth': '', 'predicted': 'California', 'question': 'Which campus did Internet Archive adopt 2009?'}, '5a6b0974a9e0c9001a4e9e8a': {'truth': '', 'predicted': \"Sun Modular Datacenter on Sun Microsystems' California campus\", 'question': 'Where does Internet archive run a sun storage center?'}, '5a6b0974a9e0c9001a4e9e8c': {'truth': '', 'predicted': 'California', 'question': 'What state is home to the first data center?'}, '5732836406a3a419008acaad': {'truth': 'Early humanists', 'predicted': '', 'question': 'Who was able to reconcile their religious beliefs with those of humanism?'}, '5732836406a3a419008acaae': {'truth': 'secular', 'predicted': '', 'question': 'What phrase that has come to be associated with a lack of faith was not seen as an issue for Christians?'}, '5732836406a3a419008acaaf': {'truth': 'Renaissance', 'predicted': '', 'question': 'During what time period did secular have a more neutral connotation?'}, '5a82062531013a001a335102': {'truth': '', 'predicted': 'secular', 'question': 'What phrase that has come to be associated with a lack of faith was seen as an issue for Christians?'}, '5731f24bb9d445190005e6d4': {'truth': 'sea', 'predicted': 'sea campaign', 'question': 'What type of campaign did Publius fight?'}, '5731f24bb9d445190005e6d6': {'truth': 'defeated', 'predicted': 'He was defeated', 'question': 'How did Publius fare in his battle?'}, '570e70c30b85d914000d7f01': {'truth': 'addressing and postal purposes', 'predicted': 'for addressing and postal purposes', 'question': 'Why is urban Melbourne divided into hundreds of suburbs?'}, '56cf7c394df3c31400b0d83b': {'truth': 'Kanye West Foundation', 'predicted': '\"Kanye West Foundation', 'question': 'With the help of his mom, what foundation did Kanye create early in his career?'}, '56cf7c394df3c31400b0d83c': {'truth': 'battle dropout and illiteracy rates, while partnering with community organizations to provide underprivileged youth access to music education', 'predicted': 'battle dropout and illiteracy rates', 'question': 'What is the goal of the Kanye West Foundation?'}, '56d11ed617492d1400aab9dd': {'truth': 'Kanye West Foundation', 'predicted': '\"Kanye West Foundation', 'question': 'What was founded by Kanye West and his mother?'}, '56d4647c2ccc5a1400d83132': {'truth': 'music education', 'predicted': 'provide underprivileged youth access to music education', 'question': 'What other mission besides dropout and illiteracy rates did the Kanye West Foundation seek to improve?'}, '56d4647c2ccc5a1400d83133': {'truth': '\"Ed in \\'08\"', 'predicted': 'Strong American Schools as part of their \"Ed in \\'08\"', 'question': 'What campaign did the Kanye West Foundation partner with in 2007?'}, '56de9a164396321400ee2a46': {'truth': 'Cruz Bustamante', 'predicted': 'Bustamante', 'question': \"Who was Schwarzenegger's closest rival in the gubernatorial race of 2003?\"}, '57279d0aff5b5019007d9106': {'truth': 'served as sailors', 'predicted': '', 'question': 'What did Aslations do to avoid conflict amongst themselves during the first World War?'}, '57279d0aff5b5019007d9107': {'truth': 'Jacques Peirotes', 'predicted': '', 'question': 'Who was the mayor that proclaimed independence from the German Empire for  Alsace-Lorraine?'}, '57279d0aff5b5019007d9108': {'truth': 'French', 'predicted': '', 'question': 'Who entered Alsace just two weeks after they declared independence? '}, '5a6fdd4c8abb0b001a675fd3': {'truth': '', 'predicted': 'First World War', 'question': 'During what war were Alsatians primarily ground units?'}, '5a6fdd4c8abb0b001a675fd7': {'truth': '', 'predicted': 'Treaty of Versailles', 'question': 'What treaty gave Alsace to Germany?'}, '570b421bec8fbc190045b924': {'truth': 'the Mason–Dixon line', 'predicted': '', 'question': 'What dividing line separated slave states from free states?'}, '570b421bec8fbc190045b926': {'truth': 'states in the South seceded from the United States', 'predicted': '', 'question': \"How did slave states react to Lincoln's election?\"}, '5ad17172645df0001a2d1bcb': {'truth': '', 'predicted': 'April 12, 1861', 'question': 'When did Confederate forces bombard Fort Pumter?'}, '570ba2b3ec8fbc190045baa0': {'truth': 'imbedded stars', 'predicted': '', 'question': 'What irradiates clouds of gas in the galaxy and makes them glow?'}, '570ba2b3ec8fbc190045baa2': {'truth': 'Stars', 'predicted': '', 'question': 'What objects emit less of their energy as infrared light versus visible light?'}, '5a07f8453fc87400182070cc': {'truth': '', 'predicted': 'Infrared', 'question': 'What can be used to detect protostars when they are cool?'}, '570b2dd76b8089140040f7d4': {'truth': '\"Old Style\" (OS) and \"New Style\"', 'predicted': '', 'question': 'What designation was added to British dates to differentiate them from countries not using the new calendar? '}, '5726b597f1498d1400e8e84e': {'truth': 'writing that possesses high quality or distinction', 'predicted': '', 'question': 'What is the main component of the qualitative judgment definition of literature?'}, '5726b597f1498d1400e8e850': {'truth': '\"the best expression of the best thought reduced to writing.\"', 'predicted': '', 'question': 'Encyclopedia Britannica defined literature in its 1911 editions how?'}, '5726b597f1498d1400e8e851': {'truth': 'anything which is universally regarded as literature has the potential to be excluded', 'predicted': '', 'question': 'What effect does the evolving definition of literature have?'}, '5a7a34d717ab25001a8a03a2': {'truth': '', 'predicted': 'fine writing', 'question': 'What is the meaning of the Spanish term, \"belles-lettres?\"'}, '5a7a34d717ab25001a8a03a4': {'truth': '', 'predicted': 'change', 'question': 'Value-decisions can do what over time?'}, '5a7a34d717ab25001a8a03a5': {'truth': '', 'predicted': '1910–11', 'question': 'What years does the Encyclopedia Brittanica Tenth Edition cover?'}, '5a7cfda9e8bc7e001a9e2108': {'truth': '', 'predicted': 'Encyclopædia Britannica Eleventh Edition', 'question': 'Which edition of the World Book Encyclopedia uses the value judgment definition of \"literature?\"'}, '5a7d25d970df9f001a874fe1': {'truth': '', 'predicted': '1910–11', 'question': 'When was the definition of value judgement first used?'}, '5a7d25d970df9f001a874fe3': {'truth': '', 'predicted': '1910–11', 'question': 'What year was the Eleventh Edition of Encyclopedia Britannica written?'}, '5a7d25d970df9f001a874fe4': {'truth': '', 'predicted': 'over time', 'question': 'How often do value-judgments change?'}, '57268e59708984140094c9f6': {'truth': '400,000 years ago', 'predicted': '', 'question': 'When was the extinct species believed to have lived in Myanmar?'}, '57268e59708984140094c9f7': {'truth': 'Anyathian', 'predicted': '', 'question': 'What is the name of the civilization that is believed to be one of the oldest. '}, '57268e59708984140094c9f8': {'truth': 'neolithic age domestication of plants and animals and the use of polished stone tools dating to sometime between 10,000 and 6,000 BC has been discovered', 'predicted': '', 'question': 'Did any other ancient cultures also leave behind evidence of existence in Myanmar?'}, '57268e59708984140094c9f9': {'truth': 'discovered in the form of cave paintings', 'predicted': 'cave paintings', 'question': 'What form was the evidence of ancient cultures discovered in ?'}, '5ad3ece8604f3c001a3ff78e': {'truth': '', 'predicted': '$70 million', 'question': 'How much did the 1954 class donate for their 40th reunion?'}, '57262d20271a42140099d706': {'truth': 'further strengthened and smoothed the filament', 'predicted': '', 'question': 'What properties of graphite improved the filament?'}, '57262d20271a42140099d707': {'truth': \"helped stabilize the lamp's power consumption, temperature and light output against minor variations in supply voltage\", 'predicted': \"stabilize the lamp's power consumption, temperature and light output against minor variations in supply voltage\", 'question': 'What are the effects of giving the filament a positive temperature coefficient?'}, '56d09354234ae51400d9c3aa': {'truth': 'driven by an expectation that coal would soon become scarce', 'predicted': 'an expectation that coal would soon become scarce', 'question': 'Why was solar technology developed in the 1860s?'}, '572efc3503f9891900756b14': {'truth': '319 million years ago', 'predicted': '319 million years ago and 192 million years ago', 'question': 'When did the first whole genome duplication event occur?'}, '572efc3503f9891900756b15': {'truth': 'whole genome duplication', 'predicted': 'whole genome duplication event', 'question': 'What type of event perhaps created the line which led to modern flowering plants?'}, '5a3acb0f3ff257001ab8428b': {'truth': '', 'predicted': 'whole genome duplication events', 'question': 'What appears to be the cause of duplicaton of seed plants?'}, '5a3acb0f3ff257001ab8428e': {'truth': '', 'predicted': 'sequencing the genome', 'question': 'How are modern plants studied?'}, '570888bf9928a814004714db': {'truth': 'red', 'predicted': 'red shirts, white shorts and red', 'question': \"What color are the socks traditionally worn in England's away kits?\"}, '59fb256bee36d60018400d49': {'truth': '', 'predicted': 'navy blue', 'question': \"What shade of blue was used in England's first away kits?\"}, '59fb256bee36d60018400d4b': {'truth': '', 'predicted': 'red', 'question': \"What shade of red is England's traditional away colour shirts?\"}, '572931841d04691400779146': {'truth': '2.6–7.8 million species', 'predicted': '2.6–7.8 million', 'question': 'How many insect species are estimated to exist?'}, '572931841d04691400779148': {'truth': 'less than 20%', 'predicted': '20%', 'question': 'Of all the species on earth, how much do insects make up?'}, '572a016b3f37b3190047863f': {'truth': 'transmission of electromagnetic energy via photons', 'predicted': 'the transmission of electromagnetic energy via photons', 'question': 'Give one example of how energy can be transferred between systems?'}, '5acd5d9e07355d001abf3efe': {'truth': '', 'predicted': 'Energy transfer', 'question': 'What can be considered for the typical case of systems which are opened to transfers of matter?'}, '5acd5d9e07355d001abf3eff': {'truth': '', 'predicted': 'heat', 'question': 'The portion of energy which works during the transfer is called what?'}, '5727bb132ca10214002d94f7': {'truth': 'Netherlands, France and England', 'predicted': '', 'question': 'Which other countries was Spain at war with during the 16 century?'}, '5a5447e0134fea001a0e16ea': {'truth': '', 'predicted': 'windmill', 'question': 'What technology was developed from advances in irrigation and farming?'}, '5a5447e0134fea001a0e16eb': {'truth': '', 'predicted': 'almonds and citrus fruit', 'question': 'What crops were brought from Europe?'}, '5a5447e0134fea001a0e16ed': {'truth': '', 'predicted': 'trade in the Indian Ocean', 'question': 'Where did Arab merchants dominate after the 16th century?'}, '57303b58947a6a140053d2e1': {'truth': 'as many as a thousand ships', 'predicted': 'as many as a thousand', 'question': 'Around how many ships were sent into service from Greek cities?'}, '57303b58947a6a140053d2e4': {'truth': 'Roman navy', 'predicted': 'the Roman navy', 'question': 'What aspect of the Roman military saw a decline in size after the subjugation of the Mediterranean?'}, '5719c6a94faf5e1900b8a7ea': {'truth': 'Denny Party', 'predicted': '', 'question': 'What group of settlers established a site at Pioneer Square?'}, '5719c6a94faf5e1900b8a7ec': {'truth': 'New York Alki', 'predicted': 'New York', 'question': \"What was the Chinook enhanced name of Terry and Low's settlement?\"}, '5719c6a94faf5e1900b8a7ed': {'truth': 'New York Alki', 'predicted': 'Alki', 'question': 'What site was eventually abandoned when the settlers moved back in with Denny?'}, '5719c6a94faf5e1900b8a7ee': {'truth': 'April 1853', 'predicted': '1853', 'question': 'When was New York Alki established?'}, '5725f7a389a1e219009ac116': {'truth': 'Dornier Do 17 bomber', 'predicted': '', 'question': 'What type of airplane was the German craft Holmes rammed into?'}, '5726245b89a1e219009ac2ee': {'truth': 'the Battle of Britain Day', 'predicted': 'Battle of Britain Day', 'question': 'What is September 15, 1940 known as?'}, '5726245b89a1e219009ac2ef': {'truth': 'the Palace', 'predicted': 'bomb the Palace', 'question': 'What did Ray Holmes believe the German pilot was targeting?'}, '5726245b89a1e219009ac2f1': {'truth': 'Dornier Do 17 bomber', 'predicted': '', 'question': 'What type of aircraft was the German Plane?'}, '5a7a513117ab25001a8a04ea': {'truth': '', 'predicted': '15 September 1940', 'question': 'What is the date of the celebration of Britain Day?'}, '5a7a513117ab25001a8a04eb': {'truth': '', 'predicted': 'bomb the Palace', 'question': 'What did Ray Holmes think the German pilot was going to restore?'}, '5a7a513117ab25001a8a04ed': {'truth': '', 'predicted': \"a King's Messenger\", 'question': 'What did Holmes become during the war?'}, '56d09a0e234ae51400d9c3c3': {'truth': 'could play a key role in de-carbonizing the global economy alongside improvements in energy efficiency and imposing costs on greenhouse gas emitters', 'predicted': 'de-carbonizing the global economy', 'question': \"What could the sun's energy do to help limit climate change?\"}, '56d8e18ddc89441400fdb38b': {'truth': 'the Eiffel Tower', 'predicted': 'scaling the Eiffel Tower', 'question': 'What did Reporters Without Borders scale in order to put a protest banner on it?'}, '56d8e18ddc89441400fdb38c': {'truth': 'Notre Dame cathedral', 'predicted': 'Notre Dame', 'question': 'Which cathedral did Reporters Without Borders hang another protest banner?'}, '56db1ba8e7c41114004b4d39': {'truth': 'banner', 'predicted': 'a protest banner', 'question': 'What was hung from the Eiffel Tower?'}, '56db1ba8e7c41114004b4d3b': {'truth': 'Notre Dame cathedral.', 'predicted': 'Notre Dame cathedral', 'question': 'Where else was a copy of the banner at Eiffel Tower hung?'}, '57305ed58ab72b1400f9c4a8': {'truth': 'around 500 BC', 'predicted': '500 BC', 'question': 'When did the Roman religion that is generally identified with the republic first established? '}, '57305ed58ab72b1400f9c4a9': {'truth': 'Greek', 'predicted': 'Greek culture', 'question': 'Who did the Romans adapt several of their religious convictions from?'}, '5730b55f396df919000962cc': {'truth': \"Android devices, Apple's iPhone and iPad\", 'predicted': 'Android', 'question': 'What smartphones have SNES emulators?'}, '5730b55f396df919000962ce': {'truth': \"Nintendo's Virtual Console service for the Wii\", 'predicted': '', 'question': \"What was Nintendo's first approved emulator?\"}, '5a43023e4a4859001aac73e7': {'truth': '', 'predicted': 'Android', 'question': 'What smartphones have Gamecube emulators?'}, '57304e618ab72b1400f9c412': {'truth': 'two', 'predicted': 'two periods of 45 minutes', 'question': 'How many periods are in a standard football match?'}, '57304e618ab72b1400f9c413': {'truth': '45', 'predicted': 'two periods of 45 minutes', 'question': 'How long are each period in a standard football match?'}, '57304e618ab72b1400f9c415': {'truth': 'referee', 'predicted': '', 'question': 'Who gets to decide how long stoppage time can go on for?'}, '56f9ebe18f12f3190062fffc': {'truth': 'develops and agrees upon specifications', 'predicted': 'develops and agrees upon specifications which are formally standardised by ETSI', 'question': \"What is the DVB's role?\"}, '5ad3b2f6604f3c001a3fed2d': {'truth': '', 'predicted': 'radio telecommunications sector', 'question': 'What does ETU-R stand for?'}, '5ad3b2f6604f3c001a3fed31': {'truth': '', 'predicted': 'ETSI', 'question': ' Who standardizes SDTV specifications?'}, '572ea993cb0c0d14000f1416': {'truth': 'palimpsests', 'predicted': '', 'question': 'What is the term for recycled parchments used in ancient manuscripts?'}, '572ea993cb0c0d14000f1417': {'truth': '671 AD', 'predicted': '', 'question': \"Before which year were the Sana'a manuscripts produced?\"}, '572ea993cb0c0d14000f1419': {'truth': 'Uthmanic', 'predicted': '', 'question': \"What version of the Quran was the scriptio superior of the Sana'a manuscripts?\"}, '5ad2183dd7d075001a4283f6': {'truth': '', 'predicted': '1972', 'question': \"After which year were the Sana'a manuscripts produced?\"}, '5729058baf94a219006a9f69': {'truth': 'Many religions are practised in Myanmar', 'predicted': 'Many religions', 'question': 'Does Burma have more than one religion ?'}, '5729058baf94a219006a9f6a': {'truth': 'Festivals can be held on a grand scale.', 'predicted': 'on a grand scale', 'question': 'Are public displays allowed for the celebration of religion in Myanmar ?'}, '5729058baf94a219006a9f6b': {'truth': 'Christian and Muslim populations do, however, face religious persecution', 'predicted': '', 'question': 'Are all welcomed to practice faith openly in Burma '}, '5729058baf94a219006a9f6c': {'truth': 'it is hard, if not impossible, for non-Buddhists to join the army', 'predicted': 'non-Buddhists', 'question': 'Can anyone in Burma Join the military forces in Burma ?'}, '571a8f6a4faf5e1900b8aa7b': {'truth': 'from just four women', 'predicted': 'four', 'question': 'A 2006 study by Behar et al, suggested that a large percentage of the current Ashkenazi population is descended matrilineally from how many women? '}, '56e7a21637bdd419002c42a1': {'truth': 'March 9, 2012', 'predicted': '', 'question': 'What was the first day of the 2015 AFL season?'}, '56e7a21637bdd419002c42a2': {'truth': 'San Antonio', 'predicted': 'San Antonio, Texas', 'question': 'To what city did the Tulsa Talons relocate?'}, '572fe399a23a5019007fcae3': {'truth': 'static charge', 'predicted': '', 'question': 'What force can easily change or even completely destroy an etch on a PCB?'}, '572fe399a23a5019007fcae4': {'truth': 'non-traditional', 'predicted': 'non-traditional PCBs such as MCMs and microwave PCBs', 'question': 'What class of PCBs are even more susceptible to static than standard ones?'}, '5ace846a32bba1001ae4a942': {'truth': '', 'predicted': 'an accumulated static charge', 'question': 'Proper handling techniques might transmit what?'}, '56e039947aa994140058e3d4': {'truth': 'comics', 'predicted': '', 'question': 'What German word is used for comics?'}, '56e039947aa994140058e3d6': {'truth': 'manhwa', 'predicted': '', 'question': 'What Korean word was derived from the Japanese word for manga?'}, '572817584b864d1900164462': {'truth': 'the Metropolitan Green Belt', 'predicted': 'Metropolitan Green Belt', 'question': 'What statutory policy minimizes outward expansion of urban London?'}, '572817584b864d1900164464': {'truth': 'Eleanor Cross at Charing Cross near the junction of Trafalgar Square and Whitehall', 'predicted': 'Eleanor Cross at Charing Cross', 'question': 'Where is the centre of London said to be located?'}, '5727b1f42ca10214002d941a': {'truth': '23.3%', 'predicted': '', 'question': 'In 2010, what percentage of the population was made up of foreigners?'}, '5727b1f42ca10214002d941b': {'truth': 'Italians', 'predicted': '', 'question': 'Who were the largest single group of foreigners in 2010?'}, '5727b1f42ca10214002d941e': {'truth': 'increase in xenophobia', 'predicted': '', 'question': \"In the 2000's, what was the concern perceived toward immigrants by institutions?\"}, '572fce07b2c2fd140056848d': {'truth': 'Catholic Saint Didacus', 'predicted': '', 'question': 'Who was the harbor named for?'}, '572fce07b2c2fd140056848f': {'truth': 'Navidad, New Spain', 'predicted': '', 'question': 'Where did Cabrillo leave from to embark on his journey to the West Coast?'}, '5727b2e53acd2414000dea18': {'truth': '2008', 'predicted': '', 'question': 'In what year did the risk of dying from TB reach half what it was in 1995?'}, '5727b2e53acd2414000dea19': {'truth': 'reinfection', 'predicted': '', 'question': 'New studies have found that half of reactivation cases of tuberculosis might actually be due to what other \"re-\" word?'}, '5a871df11d3cee001a6a10e2': {'truth': '', 'predicted': 'HIV', 'question': 'Reactivation is caused by what disease?'}, '5a871df11d3cee001a6a10e4': {'truth': '', 'predicted': '4%', 'question': 'What was the chance of death from HIV in 2008?'}, '5a871df11d3cee001a6a10e6': {'truth': '', 'predicted': 'DNA fingerprinting', 'question': 'What methods have recent studies used to examine HIV strains?'}, '5a3bf619cc5d22001a521c5b': {'truth': '', 'predicted': 'Protestants', 'question': 'What religion were most Prussian Lithuanians?'}, '56de471ccffd8e1900b4b76f': {'truth': 'Grover Cleveland', 'predicted': '', 'question': 'Who was the first president to veto over 400 bills?'}, '5ad3a323604f3c001a3fea4f': {'truth': '', 'predicted': 'damage', 'question': \"What did Cleveland's impeachment do to the presidency?\"}, '5ad3a323604f3c001a3fea50': {'truth': '', 'predicted': 'Johnson', 'question': \"Who's impeachment was perceived as having helped the presidency?\"}, '5ad3a323604f3c001a3fea51': {'truth': '', 'predicted': 'Congress', 'question': 'Which body of government because subordinate to the presidency?'}, '5ad3a323604f3c001a3fea52': {'truth': '', 'predicted': 'Johnson', 'question': 'After Grover Cleveland, who was the first Democratic President?'}, '5ad3a323604f3c001a3fea53': {'truth': '', 'predicted': 'Grover Cleveland', 'question': 'Who was the first Republican President after Johnson?'}, '56e11d89e3433e1400422c20': {'truth': 'December 2011', 'predicted': '', 'question': 'When did the Beidou system begin operating in China?'}, '56e11d89e3433e1400422c21': {'truth': 'by 2020', 'predicted': '2020', 'question': 'When is it projected that the global navigation system will be finished?'}, '56dfbe777aa994140058e0e1': {'truth': 'The Illuminating Engineering Society of North America', 'predicted': 'Illuminating Engineering Society of North America', 'question': 'What does the IESNA stand for? '}, '56dfbe777aa994140058e0e2': {'truth': 'ANSI and ASHRAE', 'predicted': '', 'question': 'Who else publishes along with IESNA? '}, '56dfbe777aa994140058e0e4': {'truth': 'distribution of light released', 'predicted': '', 'question': 'What defines photo metric data?'}, '5722d357f6b826140030fc68': {'truth': 'Her family and retainers', 'predicted': '', 'question': 'Who accused Karim of spying?'}, '5723e1e80dadf01500fa1f6e': {'truth': 'Abdul Karim', 'predicted': 'Abdul Karim. He was soon promoted to \"Munshi\": teaching her Hindustani, and acting as a clerk', 'question': 'What was the name of the waited that was promoted to Munshi?'}, '5723e1e80dadf01500fa1f70': {'truth': 'Equerry Frederick Ponsonby', 'predicted': 'Frederick Ponsonby', 'question': \"Who discovered that Victoria's new Munshi lied about his parentage?\"}, '5723e1e80dadf01500fa1f71': {'truth': 'Lord Elgin, Viceroy of India', 'predicted': 'Lord Elgin', 'question': 'Who did Ponsonby report the Munshis lies about his parentage to?'}, '5724e8960ba9f01400d97bbf': {'truth': 'Muslim Patriotic League', 'predicted': 'the Muslim Patriotic League', 'question': \"Who was Karim accused of spying for by Victoria's family? \"}, '5724e8960ba9f01400d97bc0': {'truth': 'Equerry Frederick Ponsonby', 'predicted': 'Frederick Ponsonby', 'question': 'Who discovered that Karim had lied about his parentage to Victoria? '}, '572635ccec44d21400f3dc42': {'truth': 'spying for the Muslim Patriotic League, and biasing the Queen against the Hindus', 'predicted': 'spying for the Muslim Patriotic League, and biasing the Queen against the Hindus. Equerry Frederick Ponsonby (the son of Sir Henry) discovered that the Munshi had lied about his parentage, and reported to Lord Elgin, Viceroy of India, \"the Munshi occupies very much the same position as John Brown used to do.\" Victoria dismissed their complaints as racial prejudice', 'question': \"Why did Victoria's family disapprove of Abdul Karim?\"}, '572635ccec44d21400f3dc43': {'truth': 'until he returned to India with a pension on her death', 'predicted': 'pension', 'question': \"How long was Karim in the Queen's employment?\"}, '5ad17cf2645df0001a2d1e13': {'truth': '', 'predicted': '1887', 'question': 'What year anniversary does the Silver Jubilee celebrate?'}, '5ad17cf2645df0001a2d1e16': {'truth': '', 'predicted': '1887', 'question': \"What year was Victoria's Silver Jubilee held?\"}, '57277778708984140094de58': {'truth': 'is, to the devout, taboo.', 'predicted': '', 'question': 'Why is there opposition to textual criticism of Jewish and Muslim religious books?'}, '57277778708984140094de59': {'truth': 'a period of about five millennia', 'predicted': 'five millennia', 'question': 'Over approximately what expanse of time can textual criticism be applied to written works?'}, '56f7f695aef2371900625cfe': {'truth': 'removing Lithuania from the names of the Gubernyas', 'predicted': '', 'question': 'What was an example of imposing sanctions on lithuanians?'}, '56f7f695aef2371900625cff': {'truth': '\"Lithuanians are Russians seduced by Poles and Catholicism\"', 'predicted': '', 'question': 'What did the russian officials announce as a sanction?'}, '56f7f695aef2371900625d00': {'truth': 'Lithuanian language.', 'predicted': 'Lithuanian', 'question': 'What language was banned from printing on books?'}, '572802332ca10214002d9b50': {'truth': 'direct manipulation', 'predicted': 'direct manipulation of the medium', 'question': 'What is a reason a DJ would prefer vinyl to CD?'}, '572802332ca10214002d9b54': {'truth': 'However, many CDJ and DJ advances, such as DJ software and time-encoded vinyl, now have these capabilities and more.', 'predicted': '', 'question': 'Had vinyl technology ceased expanding?'}, '572f7b31b2c2fd1400568181': {'truth': 'an exposition of social, artistic, and cultural dynamism.', 'predicted': 'social, artistic, and cultural dynamism', 'question': 'what did the \"Jazz Age\" usher in? '}, '5ad3c98e604f3c001a3ff09f': {'truth': '', 'predicted': 'British Marines', 'question': 'By who was the sailing ships occupied?'}, '5ad3c98e604f3c001a3ff0a0': {'truth': '', 'predicted': 'Whalers', 'question': 'Who used the islands as a base in the Northern Atlantic?'}, '5ad3c98e604f3c001a3ff0a2': {'truth': '', 'predicted': 'Suez Canal', 'question': 'What decreased isolation in the islands?'}, '56f9608b9b226e1400dd13de': {'truth': 'parliamentary-presidential system', 'predicted': 'parliamentary-presidential', 'question': 'What is the governing system of the Marshall Islands?'}, '570bd2ec6b8089140040fa68': {'truth': 'a Control-S', 'predicted': 'Control-S', 'question': 'What casued the automatic paper tape reader to stop?'}, '5a6513c9c2b11c001a425be3': {'truth': '', 'predicted': 'handshaking', 'question': 'What was the warning signal that warned TAPE that there was impending overflow?'}, '5ad3d84a604f3c001a3ff378': {'truth': '', 'predicted': 'William Glass', 'question': 'Who established a settlement based on inequality in 1817?'}, '5ad3d84a604f3c001a3ff379': {'truth': '', 'predicted': 'equality', 'question': 'On what did William Glass base his island on?'}, '5ad3d84a604f3c001a3ff37a': {'truth': '', 'predicted': 'it votes for a change in its law', 'question': 'What will not allow for a outsiders to buy land or settle on Tristan?'}, '5ad3d84a604f3c001a3ff37b': {'truth': '', 'predicted': '90', 'question': 'For how many months does the nominal fishing season last?'}, '572785c8dd62a815002e9f6d': {'truth': 'sell and carry drugs, guns, and other illegal substances', 'predicted': 'to sell and carry drugs, guns, and other illegal substances', 'question': 'What do drug cartels do with child workers?'}, '572785c8dd62a815002e9f6f': {'truth': 'an increase', 'predicted': 'there has been an increase', 'question': 'Has these dangers caused and increase or decrease in child labour with drug cartels?'}, '572842a0ff5b5019007da031': {'truth': 'bluffing', 'predicted': 'consulting his colleagues and made more and more of the decisions himself. Although Nasser repeatedly said that a war with Israel will start at a time of his, or Arab, choosing, on 1967 he started a bluffing game \"but a successful bluff means your opponent must not know which cards you are holding', 'question': 'What gambit did Nasser fail at in his bluster with Israel?'}, '5734465d879d6814001ca463': {'truth': 'Hunter-gathering', 'predicted': '', 'question': 'What type of lifestyle was prevalent in Siberia until the European Age of Discovery?'}, '5734465d879d6814001ca464': {'truth': 'some tribal societies', 'predicted': 'tribal societies', 'question': 'Where does the hunter-gathering lifestyle persist, though in decline?'}, '5735e8736c16ec1900b92889': {'truth': 'Sub-Saharan Africa, and Siberia, as well as all of Australia', 'predicted': 'Sub-Saharan Africa, and Siberia', 'question': 'What parts of the New World did the hunter-gathering lifestyles remain?'}, '5735e8736c16ec1900b9288a': {'truth': 'Hadza of Tanzania', 'predicted': 'the Hadza of Tanzania', 'question': 'Who are the only remaining full-time hunter-gatherers in Africa?'}, '5a39eb432f14dd001ac72647': {'truth': '', 'predicted': 'end of the 20th', 'question': 'Up until what century were cladistics universally taught?'}, '573192bca5e9cc1400cdc0e7': {'truth': 'high cost of labor in developed countries', 'predicted': 'With high cost of labor in developed countries', 'question': 'Why has production automation become popular?'}, '573192bca5e9cc1400cdc0e8': {'truth': 'CAD', 'predicted': 'computer aided design (CAD', 'question': 'Which software can aid in the design of robotically created mosaics?'}, '573192bca5e9cc1400cdc0e9': {'truth': '10 times faster', 'predicted': '', 'question': 'How much faster is automated creation over handmade?'}, '573192bca5e9cc1400cdc0ea': {'truth': 'a command file', 'predicted': 'individually according to a command file from the design software', 'question': 'How does the robot pick the tiles it places?'}, '573192bca5e9cc1400cdc0eb': {'truth': 'different look', 'predicted': '', 'question': 'What is not the same between hand made and robotic amde mosaics?'}, '5a82188631013a001a3351e5': {'truth': '', 'predicted': 'phonotactics', 'question': 'Under what topic is phonological alternation studied?'}, '5a82188631013a001a3351e7': {'truth': '', 'predicted': 'prosody', 'question': 'Phonotactics, phonological alternation and stress are topics contained in what discipline?'}, '56fb91df8ddada1400cd64f7': {'truth': 'scholasticism', 'predicted': '', 'question': 'What school of thought was Duns Scotus opposed to?'}, '56fb91df8ddada1400cd64f9': {'truth': 'England', 'predicted': '', 'question': 'What country did not see the increasing influence of Roman law?'}, '56fb91df8ddada1400cd64fb': {'truth': 'universals', 'predicted': '', 'question': 'What Platonic idea lost influence as a result of the work of Ockham and Duns Scotus?'}, '572eb63603f989190075697f': {'truth': 'few consultations result in jeopardy opinions', 'predicted': 'so few consultations result in jeopardy opinions', 'question': 'Why is this exemption provision often considered a nonfactor?'}, '572eb63603f9891900756980': {'truth': 'in the identification of reasonable and prudent alternatives to avoid jeopardy.', 'predicted': '', 'question': 'How can jeopardy opinions be dissuaded?'}, '5a848a6d7cf838001a46a8ea': {'truth': '', 'predicted': 'six', 'question': 'How many exemptions were granted in 2009?'}, '5a848a6d7cf838001a46a8eb': {'truth': '', 'predicted': 'nonfactor', 'question': 'What is a key factor for the ESA?'}, '5a848a6d7cf838001a46a8ed': {'truth': '', 'predicted': 'three', 'question': 'How many exemptions were withdrawn in 2009?'}, '5728cc17ff5b5019007da6e4': {'truth': 'two factions', 'predicted': 'two factions: The radicals, led by Tilak, advocated civil agitation and direct revolution to overthrow the British Empire and the abandonment of all things British', 'question': 'How was the Congress split in 1907?'}, '5728cc17ff5b5019007da6e5': {'truth': 'The radicals', 'predicted': 'radicals', 'question': 'Which faction of the Congress did Tilak lead?'}, '5728cc17ff5b5019007da6e6': {'truth': 'The moderates', 'predicted': 'moderates', 'question': 'Which faction wanted reform within British rule?'}, '572f7d6f04bcaa1900d76a1a': {'truth': 'tropical wet and dry climate', 'predicted': 'tropical wet and dry climate (Köppen Aw) bordering on a hot semi-arid', 'question': 'Köppen Aw refers to what kind of climate?'}, '572f7d6f04bcaa1900d76a1c': {'truth': '10 °C', 'predicted': '10 °C (50 °F)', 'question': 'What is the typical lowest temperature in Celsius during winter in Hyderabad?'}, '56e0ccb3231d4119001ac3b5': {'truth': 'Chrome', 'predicted': '', 'question': 'Which browser is the newest to enter the field?'}, '56e0ccb3231d4119001ac3b8': {'truth': 'May 2012', 'predicted': '', 'question': 'When did Chrome become more used than all versions of Internet Explorer?'}, '5a4d38437a6c4c001a2bbc5e': {'truth': '', 'predicted': 'December 2011', 'question': 'When did Internet Explorer 8 overtake Chrome as the more popular browser?'}, '5a4d38437a6c4c001a2bbc5f': {'truth': '', 'predicted': '16%', 'question': \"What was IE's increase in usage percentage in August 2011?\"}, '5726e8eb708984140094d58a': {'truth': 'species as a whole', 'predicted': 'While that particular prey organism may be killed, the coloring benefits the prey species as a whole', 'question': 'Does aposematism benefit only the organism ddirectly, or the entire population as a whole?'}, '5a6bdaf94eec6b001a80a5e9': {'truth': '', 'predicted': 'brightly colored', 'question': 'What do predators use to warn their prey?'}, '572f5d5eb2c2fd1400568083': {'truth': 'inability to damage industries', 'predicted': '', 'question': \"Why did Hitler feel bombing wasn't working?\"}, '572f5d5eb2c2fd1400568084': {'truth': 'the moment was right', 'predicted': 'when the moment was right', 'question': 'In 1939 Hitler said bombing of Britain would begin when?'}, '56e8dca40b45c0140094cd24': {'truth': 'the assessment attendant on the Dissolution of the Monasteries', 'predicted': 'Dissolution of the Monasteries', 'question': 'During what was the abbey made second in wealth?'}, '5ad3f4c7604f3c001a3ff959': {'truth': '', 'predicted': '1535', 'question': \"When was the abbey's annual income £2400–2700?\"}, '5ad3f4c7604f3c001a3ff95b': {'truth': '', 'predicted': 'Glastonbury Abbey', 'question': 'To which other abbey was Westminster Abbey first in wealth?'}, '573029cd947a6a140053d1f0': {'truth': 'indicating the level of resource usage', 'predicted': '', 'question': 'What is the point of using different colors on a heat map?'}, '573029cd947a6a140053d1f1': {'truth': 'a simpler and modern design with less technical information displayed', 'predicted': '', 'question': 'What changes were made to the BSoD?'}, '573029cd947a6a140053d1f2': {'truth': 'applications, background processes and Windows processes', 'predicted': '', 'question': 'What are some of the process type groups Windows 8 implemented?'}, '573029cd947a6a140053d1f3': {'truth': 'search the web', 'predicted': '', 'question': 'How can users find out more about obscure Windows 8 processes?'}, '570e25b30dc6ce1900204dfe': {'truth': '\"Pasta with tomato sauce and berbere\" (spice)', 'predicted': 'spice', 'question': \"How does 'Pasta al Sugo e Berbere' translate in English?\"}, '570e25b30dc6ce1900204dff': {'truth': 'pasta', 'predicted': '', 'question': \"What is the main food, influenced by Italy, eaten in Eritrea's capital today?\"}, '570624f252bb8914006898f7': {'truth': 'filter bank', 'predicted': '', 'question': 'What did the working group integrate their ideas with?'}, '570624f252bb8914006898fa': {'truth': 'MP2 at 192 kbit/s', 'predicted': '', 'question': 'What quality were they hoping to match at 128 kbit/s?'}, '5727ff0c2ca10214002d9ae8': {'truth': 'early 18th century', 'predicted': '18th century', 'question': 'During what century did the Mughal empire decline?'}, '572820842ca10214002d9e7c': {'truth': '€212.1bn', 'predicted': '', 'question': 'By 2012, how much did the ECB spend in covering bad debt?'}, '572820842ca10214002d9e7d': {'truth': 'Outright Monetary Transactions', 'predicted': '', 'question': 'How does the ECB plan to increase the available credit for businesses?'}, '572820842ca10214002d9e7e': {'truth': 'no ex-ante time or size limit', 'predicted': '', 'question': 'What is the duration of the Outright Monetary Transactions program?'}, '572820842ca10214002d9e80': {'truth': 'temporary', 'predicted': '', 'question': 'How long was the duration of the Securities Markets Programme to last?'}, '56d375b859d6e41400146468': {'truth': 'Polish refugees.', 'predicted': 'Polish refugees', 'question': 'Who were the beneficiaries of his last public concert?'}, '56d375b859d6e4140014646b': {'truth': \"London's Guildhall\", 'predicted': 'Guildhall', 'question': \"Where was Chopin's last public performance?\"}, '5731225ca5e9cc1400cdbc75': {'truth': 'Asia', 'predicted': 'Asia (in particular North Asia) to the Americas took place via Beringia', 'question': 'Where do most theories today attribute the settlement of the Americas as originating from?'}, '5731225ca5e9cc1400cdbc76': {'truth': 'Beringia', 'predicted': 'via Beringia', 'question': 'How did humans cross over to the Americas from Asia?'}, '5731225ca5e9cc1400cdbc77': {'truth': 'a land bridge which connected the two continents', 'predicted': 'a land bridge', 'question': 'What was Beringia?'}, '5731225ca5e9cc1400cdbc79': {'truth': 'a wide range of creation myths', 'predicted': 'oral histories of many of the indigenous peoples of the Americas, they have been living there since their genesis, described by a wide range of creation myths', 'question': 'How do the indigenous peoples explain how they came to live in the Americas?'}, '5731bfaae17f3d140042238d': {'truth': 'the mid-to-late 16th century', 'predicted': 'mid-to-late 16th century', 'question': 'When did Protestantism begin to split?'}, '5731bfaae17f3d140042238f': {'truth': 'transubstantiation', 'predicted': 'Roman Catholic dogma of transubstantiation', 'question': 'What Catholic belief did early Protestants not agree with?'}, '5727ff7c2ca10214002d9b02': {'truth': 'the Turcophile Pope', 'predicted': 'Turcophile Pope', 'question': 'What ws he known by in the Turkish community?'}, '5a60fc3ae9e1cc001a33cdfc': {'truth': '', 'predicted': '30 November 1934', 'question': 'When was Pope Pius XI appointed Apostolic Delegate to Turkey and Greece?'}, '5a60fc3ae9e1cc001a33cdfd': {'truth': '', 'predicted': 'Mesembria, Bulgaria', 'question': 'Where was Pope Pius XI named a titular archbishop?'}, '5a60fc3ae9e1cc001a33cdfe': {'truth': '', 'predicted': 'Turcophile Pope', 'question': 'What title is Pope Pius XI known by in Turkey?'}, '5a60fc3ae9e1cc001a33cdff': {'truth': '', 'predicted': '14 October', 'question': 'When did Pope Pius XI take his position in Turkey?'}, '5726909f5951b619008f76c6': {'truth': '€3 billion', 'predicted': '€3 billion worth of five-year government bonds at a yield of 4.95%', 'question': 'How much did Greece make from a bond sale in 2014?'}, '5726909f5951b619008f76c7': {'truth': 'six years', 'predicted': 'six', 'question': 'After how many years did it take for Greece to gain back growth in the economy?'}, '56f89a0c9e9bad19000a01b0': {'truth': 'Aeneas', 'predicted': 'Aeneas. As the protagonist of the poem, Aeneas', 'question': 'Who is the protagonist of the Aeneid?'}, '56f89a0c9e9bad19000a01b2': {'truth': 'Augustus', 'predicted': 'Augustus and Aeneas', 'question': 'Who is the founder of Rome which some scholars see strong associations with Aeneas?'}, '57305b0b069b5314008320a7': {'truth': 'context itself', 'predicted': 'the context itself', 'question': 'What have translators tried to preserve?'}, '57305b0b069b5314008320aa': {'truth': 'passive', 'predicted': '', 'question': 'What is the active voice sometimes shifted to when needed?'}, '570fc65b80d9841400ab366b': {'truth': 'consumer market', 'predicted': '', 'question': 'What market did Dell ignore at first?'}, '570fc65b80d9841400ab366c': {'truth': '1996', 'predicted': '1996 and 1997', 'question': \"When did Dell's internet site gain popularity?\"}, '570fc65b80d9841400ab366e': {'truth': '1997', 'predicted': 'early 1997', 'question': 'When did Dell create their internal marketing group?'}, '5a6bb3444eec6b001a80a4e9': {'truth': '', 'predicted': 'chloride', 'question': 'What is another name for thallium-201?'}, '5726f9d3f1498d1400e8f193': {'truth': 'UN Command', 'predicted': 'the UN Command', 'question': 'Who ignored the direct participation of the Soviet Union to prevent expanding the Korean War to the Soviet Union?'}, '5726f9d3f1498d1400e8f195': {'truth': 'dropping their code signals and speaking over the wireless in Russian', 'predicted': 'dropping their code signals', 'question': 'What did the Soviet pilots start doing when accused of playing an integral role in the Korean War?'}, '5726f9d3f1498d1400e8f196': {'truth': 'Soviet Union', 'predicted': 'the Soviet Union', 'question': 'Who feared engaging in direct conflict with the United States?'}, '572792f3f1498d1400e8fc8d': {'truth': 'end-of-year compilation', 'predicted': 'a longer end-of-year compilation', 'question': 'Along with individual tributes, what form did TCM Remembers occur in?'}, '572792f3f1498d1400e8fc8e': {'truth': 'December', 'predicted': '', 'question': 'In what month of the year does a longer version of TCM Remembers appear?'}, '5a826083e60761001a2eb222': {'truth': '', 'predicted': '2009', 'question': 'In what year did Steve Earle provide a soundtrack for TCM Remembers?'}, '5a826083e60761001a2eb223': {'truth': '', 'predicted': 'Steve Earle', 'question': 'Who provided the soundtrack for the longer Badly Drawn Boy episode in 2009?'}, '57269193f1498d1400e8e410': {'truth': 'in the late thirteenth century, but it declined in importance and the status of county town transferred to Taunton about 1366', 'predicted': 'late thirteenth century', 'question': 'When did Somerton take over from Ilchester as the county town '}, '57269193f1498d1400e8e411': {'truth': 'The county has two cities, Bath and Wells, and 30 towns', 'predicted': '30', 'question': 'How many cities and towns in Somerset '}, '57269193f1498d1400e8e412': {'truth': 'in terms of population are Bath, Weston-super-Mare, Taunton, Yeovil and Bridgwater', 'predicted': 'Bath, Weston-super-Mare, Taunton, Yeovil and Bridgwater', 'question': 'The largest populations of the county '}, '57269193f1498d1400e8e413': {'truth': 'strategic importance in relation to geographical features, such as river crossings or valleys in ranges of hills', 'predicted': 'river crossings or valleys in ranges of hills', 'question': 'What was the strategic purpose to settle this area '}, '57269193f1498d1400e8e414': {'truth': 'Chard is the most southerly town in Somerset, and at an altitude of 121 m (397 ft) it is also the highest', 'predicted': '', 'question': 'Most Southernly town of somerset'}, '5acf5f2877cf76001a684ca6': {'truth': '', 'predicted': 'Bath, Weston-super-Mare, Taunton, Yeovil and Bridgwater', 'question': 'What is the largest Urban area in Somerton?'}, '5acf5f2877cf76001a684ca7': {'truth': '', 'predicted': 'Chard', 'question': 'What is the most northerly town in Somerset?'}, '56ce4100aab44d1400b88615': {'truth': 'In 1642', 'predicted': '1642', 'question': 'When did the 5th Dalai Lama gain political control over Tibet?'}, '57268ac8dd62a815002e88d8': {'truth': '15.8', 'predicted': '15.8%', 'question': 'What percentage of the National GDP does the Federal District produce?'}, '57268ac8dd62a815002e88db': {'truth': '25.3', 'predicted': '25.3%', 'question': 'What percentage of service sector national GDP does Mexico City account for?'}, '5734009a4776f41900661692': {'truth': 'in a Shahmukhi script', 'predicted': 'Shahmukhi script', 'question': 'How is Punjabi written?'}, '5a68fabf8476ee001a58a965': {'truth': '', 'predicted': '89%', 'question': 'Punjabi is the mother-tongue of less than what percentage of the population?'}, '5727d9b5ff5b5019007d96d4': {'truth': 'medieval', 'predicted': 'medieval period', 'question': 'Albert Magnus studied Dionysus during what historical period?'}, '5727d9b5ff5b5019007d96d5': {'truth': 'his study of Dionysus the Areopagite', 'predicted': '', 'question': \"What was one of Albert the Great's biggest contributions during the medieval period?\"}, '5a611ac3e9e1cc001a33cf1c': {'truth': '', 'predicted': 'stone', 'question': 'What replaced metal objects?'}, '5a611ac3e9e1cc001a33cf20': {'truth': '', 'predicted': 'cattle', 'question': 'What diet did Egyptians begin to move away from?'}, '57344599acc1501500babd63': {'truth': 'were hunters', 'predicted': 'hunters', 'question': 'What is undisputed about early humans?'}, '57344599acc1501500babd64': {'truth': 'earlier Australopithecines', 'predicted': 'Australopithecines', 'question': 'Hunting was important for the emergence of the Homo genus from what?'}, '57344599acc1501500babd65': {'truth': 'hunting', 'predicted': 'hunting hypothesis', 'question': 'Production of stone tools and control of fire were also pushed forward by what?'}, '5735e8236c16ec1900b92882': {'truth': 'humans were hunters', 'predicted': '', 'question': 'What is undisputed about earlier humans?'}, '5ace521a32bba1001ae4a29d': {'truth': '', 'predicted': 'hunters', 'question': 'What kind of activity would require the use of fire for early humans?'}, '5ace521a32bba1001ae4a29e': {'truth': '', 'predicted': 'Australopithecines', 'question': 'Mating behavior was important for the emergence of Homo genus from what?'}, '572b7afb34ae481900deae3f': {'truth': 'infinite', 'predicted': '', 'question': 'According to Davidson, how many gods are there?'}, '572b7afb34ae481900deae40': {'truth': 'Aristotle', 'predicted': '', 'question': 'By whose philosophy was Davidson influenced?'}, '572b7afb34ae481900deae41': {'truth': 'rational thought', 'predicted': '', 'question': 'What did Davidson believe the God of Aristotle is synonymous with?'}, '5a7ca6d9e8bc7e001a9e1f49': {'truth': '', 'predicted': \"Aristotle's God with rational thought, Davidson argued, contrary to Aristotle\", 'question': \"Davidson's argument that God cannot exist apart from the world matched whose argument?\"}, '56e1586ee3433e1400422de8': {'truth': 'Big Five', 'predicted': 'the \"Big Five', 'question': 'The Boston Symphony Orchestra is a member of what?'}, '572fa925947a6a140053cb1c': {'truth': 'theatre and drama', 'predicted': '', 'question': 'What type of art might one encounter at Lalithakala Thoranam?'}, '572fa925947a6a140053cb1d': {'truth': 'annual exhibition of local and national consumer products', 'predicted': 'a popular annual exhibition of local and national consumer products', 'question': 'What is Numaish?'}, '572fa925947a6a140053cb20': {'truth': \"world's largest film studio\", 'predicted': '', 'question': 'What did Guinness World Records say of Ramoji Film City was in 20015?'}, '5706a52352bb891400689b10': {'truth': 'National Rail Museum', 'predicted': \"Indira Gandhi Memorial Museum, National Gallery of Modern Art, National Museum of Natural History, National Rail Museum, National Handicrafts and Handlooms Museum, National Philatelic Museum, Nehru Planetarium, Shankar's International Dolls Museum. and Supreme Court of India Museum\", 'question': 'What is the name of the major railroad related museum located in New Delhi?'}, '5706a52352bb891400689b11': {'truth': \"Shankar's International Dolls Museum\", 'predicted': \"Indira Gandhi Memorial Museum, National Gallery of Modern Art, National Museum of Natural History, National Rail Museum, National Handicrafts and Handlooms Museum, National Philatelic Museum, Nehru Planetarium, Shankar's International Dolls Museum\", 'question': 'What is the name of the doll themed museum located in New Delhi?'}, '572a34b83f37b319004787a7': {'truth': 'farmers', 'predicted': 'skilled farmers', 'question': 'What type of occupation were Neolithic people considered to be proficient at?'}, '572a34b83f37b319004787a8': {'truth': 'tending, harvesting and processing of crops', 'predicted': 'the tending, harvesting and processing of crops (such as sickle blades and grinding stones) and food production', 'question': 'What purposes were the production of farm tools used for?'}, '572a34b83f37b319004787aa': {'truth': 'pottery, bone implements)', 'predicted': 'pottery, bone implements', 'question': 'What types of tools did early farmers use for food production?'}, '572a34b83f37b319004787ab': {'truth': 'stone axe', 'predicted': 'polished stone axe', 'question': 'What tool allowed early farmers to convert forest into arable land?'}, '5a7d3e5c70df9f001a87503e': {'truth': '', 'predicted': 'wood', 'question': 'What types of tools did early farmers use for shelter?'}, '5a7d3e5c70df9f001a87503f': {'truth': '', 'predicted': 'the polished stone axe', 'question': 'What tool allowed early manufacturers to convert forest into arable land?'}, '5727c7ad3acd2414000dec38': {'truth': 'Strathmore', 'predicted': 'Wurlitzer Building and Strathmore Hotel', 'question': 'Which hotel is set to be renovated?'}, '5731c971e17f3d14004223e2': {'truth': 'the obduracy and the complacency of the Catholic establishment', 'predicted': 'complacency of the Catholic establishment', 'question': 'What did the French find alienating about Catholicism?'}, '5731c971e17f3d14004223e3': {'truth': 'the 1550s', 'predicted': '1550s', 'question': 'When were French nobles converted to Protestantism?'}, '5731c971e17f3d14004223e4': {'truth': 'Henry II', 'predicted': 'Henry II of France', 'question': 'Whose death caused an increase in the French civil wars?'}, '57264e34f1498d1400e8db96': {'truth': '50 °C (122 °F)', 'predicted': '122 °F', 'question': 'At what temperature does a typical 50-hour-life projection bulb operate?'}, '57264e34f1498d1400e8db98': {'truth': 'as short as two hours', 'predicted': 'two hours', 'question': 'What is the typical life of a P1 lamp?'}, '57264e34f1498d1400e8db99': {'truth': 'higher color temperature', 'predicted': '', 'question': 'How does the color temperature differ for photographic lighting?'}, '570acf964103511400d59a24': {'truth': '145 million to 100 million years', 'predicted': '145 million to 100 million years ago', 'question': 'What is the span of the Early Cretaceous?'}, '570acf964103511400d59a25': {'truth': 'expansion of seaways', 'predicted': '', 'question': 'What event of the Early Cretaceous caused the extinction of several species?'}, '5a2f4b02a83784001a7d26da': {'truth': '', 'predicted': 'Iguanodon', 'question': 'What dinosaur was particularly confined to one continent?'}, '572c99202babe914003c299d': {'truth': 'eastern', 'predicted': 'eastern third', 'question': 'Which part of Tennessee voted more Republican in the years following the Civil War?'}, '56f7d6d8aef2371900625c2d': {'truth': 'descent from the ancient Iranian tribes known as Sarmatians', 'predicted': \"descent from the ancient Iranian tribes known as Sarmatians or from Japheth, one of Noah's sons\", 'question': 'What was one historic theory of the szlachta origins?'}, '56f7d6d8aef2371900625c2f': {'truth': \"had not mixed their bloodlines with those of 'slaves, prisoners, and aliens\", 'predicted': '', 'question': 'What was important and unique about regional leaders?'}, '56e788ab37bdd419002c40d5': {'truth': 'no lines', 'predicted': 'there are no lines to see the individual documents', 'question': 'What is an aspect of a visit to the Rotunda for the Charters of Freedom?'}, '56e788ab37bdd419002c40d6': {'truth': 'advent of cameras with automatic flashes', 'predicted': '', 'question': 'What has made the no photography rule hard to enforce in the Rotunda for the Charters of Freedom?'}, '5a7b6b7321c2de001afe9ffb': {'truth': '', 'predicted': 'February 25, 2010', 'question': 'Since what date have visitors been allowed into the Rotunda?'}, '5a7b6b7321c2de001afe9ffe': {'truth': '', 'predicted': 'the advent of cameras with automatic flashes have made the rules increasingly difficult to enforce. As a result, all filming, photographing, and videotaping by the public in the exhibition areas has been prohibited', 'question': 'Was it difficult to enforce the ban on the public entering the Rotunda before February 25, 2010?'}, '56e8f62999e8941900975f30': {'truth': 'the Celestial Organ', 'predicted': 'Celestial Organ', 'question': 'What part of the organ is not connected or playable?'}, '5ad3fd89604f3c001a3ffbeb': {'truth': '', 'predicted': 'Celestial Organ', 'question': 'What part of the organ is connected or playable?'}, '5ad3fd89604f3c001a3ffbec': {'truth': '', 'predicted': 'Celestial Organ', 'question': 'What part of the organ is fully playable?'}, '5ad3fd89604f3c001a3ffbef': {'truth': '', 'predicted': 'Harrison and Harrison', 'question': 'In 2006, the console of the guitar was refurbished by who?'}, '57304e4b069b531400832039': {'truth': 'The Roman Catholic Diocese of Charleston Office of Education', 'predicted': 'Roman Catholic Diocese of Charleston Office of Education', 'question': 'Which organization oversees several K-8 Parochial Schools?'}, '5ad42525604f3c001a4008d5': {'truth': '', 'predicted': '150 years', 'question': 'Some of the oldest public schools in Charleston date back how long?'}, '5ad42525604f3c001a4008d7': {'truth': '', 'predicted': '150 years', 'question': \"How old are some of Charleston's public school?\"}, '5ad42525604f3c001a4008d9': {'truth': '', 'predicted': 'Roman Catholic Diocese of Charleston Office of Education', 'question': 'Which organization oversees several K-9 Parochial Schools?'}, '57293b691d0469140077918e': {'truth': 'dramatically lower solar prices', 'predicted': 'dramatically lower solar prices and weakened US and EU markets', 'question': 'Why did the total investment in renewable energy go down in 2012?'}, '57293b691d0469140077918f': {'truth': 'China, Germany, Spain, the United States, Italy, and Brazil', 'predicted': '', 'question': 'What  six were the top countries for investment in recent years?'}, '5ad123aa645df0001a2d0ee9': {'truth': '', 'predicted': 'dramatically lower solar prices and weakened US and EU markets', 'question': 'Why did the total investment in renewable energy go up in 2012?'}, '5ad37aaa604f3c001a3fe3cd': {'truth': '', 'predicted': 'Age of Enlightenment', 'question': 'During which age did Thomas Hobbes advocate the principle in his writing?'}, '5ad37aaa604f3c001a3fe3cf': {'truth': '', 'predicted': 'Thomas Hobbes', 'question': 'Who was at the forefront of the opposition towards separating the branches of government?'}, '56ddd3909a695914005b95fc': {'truth': 'The Geophysics Institute', 'predicted': 'Geophysics Institute', 'question': 'What institution is in charge of tracking volcanic activity in the Galápagos Islands?'}, '56cd7d3262d2951400fa6632': {'truth': 'third-party vendors', 'predicted': 'third-party vendors of iPod replacement batteries', 'question': 'Whose directions can be followed to interact with iPod batteries?'}, '56cd7d3262d2951400fa6633': {'truth': 'refurbished replacement iPod', 'predicted': '', 'question': 'What did Apple originally tell consumers to purchase when their iPod batteries no longer worked?'}, '56d131c817492d1400aabbe0': {'truth': 'batteries', 'predicted': '', 'question': 'Which iPod component did Apple somewhat inconveniently made non-replaceable?'}, '56d131c817492d1400aabbe1': {'truth': 'lithium-ion', 'predicted': '', 'question': 'What type of rechargeable battery does Apple use in its iPods?'}, '5726b75add62a815002e8dd0': {'truth': 'they must be retained by the organization for its self-preservation, expansion, or plans', 'predicted': 'they must be retained by the organization for its self-preservation, expansion, or plans. NPOs have controlling members or a board of directors', 'question': 'How does an NPO have to handle surplus money?'}, '5726b75add62a815002e8dd2': {'truth': 'Many have paid staff including management, whereas others employ unpaid volunteers and even executives who work with or without compensation', 'predicted': 'paid', 'question': 'How do NPOs handle staffing?'}, '5a45597219a820001a1eda2a': {'truth': '', 'predicted': 'controlling members or a board of directors', 'question': 'What control does management have in an NPO?'}, '5a45597219a820001a1eda2b': {'truth': '', 'predicted': 'to meet legal requirements for establishing a contract between the executive and the organization', 'question': \"What do NPO's use their surplus fees for?\"}, '573056f1069b531400832087': {'truth': 'Ni Guangnan', 'predicted': '', 'question': 'Who claimed that the Window 8 OS could gather sensitive user information?'}, '56dd2ff966d3e219004dac33': {'truth': 'cohabitation', 'predicted': 'political) cohabitation', 'question': 'What is the term for a situation in which the president and prime minister come from different political parties?'}, '56dd2ff966d3e219004dac34': {'truth': 'resignation of the government', 'predicted': 'force the resignation of the government', 'question': 'What can the French parliament cause in order to oust the prime minister?'}, '56e02cb57aa994140058e2f9': {'truth': 'Englishman Sir Francis Drake', 'predicted': 'Sir Francis Drake', 'question': 'Who probably located the island on their final leg of their circumnavigation trip?'}, '5ad3903d604f3c001a3fe64b': {'truth': '', 'predicted': 'MIA', 'question': \"What is Florida's busiest seaport?\"}, '5ad3903d604f3c001a3fe64e': {'truth': '', 'predicted': 'Brickell Avenue', 'question': 'What mountain is central to the financial district of Miami?'}, '5ad3903d604f3c001a3fe64f': {'truth': '', 'predicted': 'South America', 'question': 'Along with the Caribbean, from where does a significant amount of cargo enter MMI?'}, '572fe936b2c2fd14005685c0': {'truth': 'electrons', 'predicted': 'moving electrons', 'question': \"What particle's migration does the resonance principle rely on?\"}, '572fe936b2c2fd14005685c1': {'truth': 'reflective', 'predicted': 'reflective surface', 'question': 'What type of surface is formed by the tip of a conductor?'}, '5727b0563acd2414000de9cf': {'truth': 'the Tibetan Empire', 'predicted': 'Tibetan Empire', 'question': 'Which empire ravished the Chengdu region with constant warfare and economic distress?'}, '5a512d7dce860b001aa3fbf0': {'truth': '', 'predicted': 'Emperor Xuanzong of Tang', 'question': \"Who fled from Sichuan to Chang'an?\"}, '5a68e9e48476ee001a58a87e': {'truth': '', 'predicted': 'Tang dynasty', 'question': 'During which dynasty did Sichuan regain its political and cultural prominence for which it was known during the Tibetan Empire?'}, '5a68e9e48476ee001a58a87f': {'truth': '', 'predicted': 'Du Fu', 'question': \"Who was known as China's greatest poet and lived in Tang?\"}, '5a68e9e48476ee001a58a882': {'truth': '', 'predicted': 'Tibetan Empire', 'question': 'Which empire ravished the Lushan region with constant warfare and economic distress?'}, '572ec21cdfa6aa1500f8d34d': {'truth': \"They purged monarchists and members of Idris' Senussi clan from Libya's political world and armed forces\", 'predicted': \"purged monarchists and members of Idris' Senussi clan from Libya's political world and armed forces\", 'question': 'What did the RCC do with remnants of the monarchy?'}, '572ec21cdfa6aa1500f8d34e': {'truth': 'Idris was sentenced to execution in absentia.', 'predicted': 'sentenced to execution', 'question': 'What happened to Idris?'}, '572ec21cdfa6aa1500f8d34f': {'truth': 'Gaddafi believed this elite were opposed to the will of the Libyan people and had to be expunged.', 'predicted': 'opposed to the will of the Libyan people', 'question': 'How did Gaddafi view the elite?'}, '57317177a5e9cc1400cdbf6e': {'truth': 'monarchist politicians', 'predicted': 'monarchist politicians and journalists', 'question': \"Along with journalists, who was tried in the People's Courts?\"}, '57264c62dd62a815002e80d0': {'truth': 'rare', 'predicted': 'extremely rare', 'question': 'Are written records of Old Dutch rare or common?'}, '57264c62dd62a815002e80d1': {'truth': 'the Salic law', 'predicted': 'Salic law', 'question': 'What Frankish document contains the oldest recorded instance of Dutch?'}, '57264c62dd62a815002e80d2': {'truth': 'fish', 'predicted': 'A fish', 'question': 'What creature is swimming in the Dutch phrase \"Visc flot aftar themo uuatare\"?'}, '57264c62dd62a815002e80d4': {'truth': 'the Utrecht baptismal vow', 'predicted': 'Utrecht baptismal vow', 'question': 'What historically significant Dutch document begins with the phrase \"Forsachistu diobolae\"?'}, '5728b6bbff5b5019007da53e': {'truth': 'northern part', 'predicted': 'northern', 'question': 'What section of the temperate climate zone does Estonia reside?'}, '5728b6bbff5b5019007da540': {'truth': '16.3 °C (61.3 °F)', 'predicted': '16.3 °C', 'question': 'What is the average temperature of the Baltic Islands?'}, '56cf6057aab44d1400b89177': {'truth': 'local artists', 'predicted': 'burgeoning local artists', 'question': \"Who were the beats Kanye made in the 90's originally intended for?\"}, '56cf6057aab44d1400b89178': {'truth': 'classic soul records', 'predicted': 'classic soul', 'question': 'What types of records did Kanye sample in his early career.'}, '56cf6057aab44d1400b8917a': {'truth': 'Go-Getters', 'predicted': 'the Go-Getters', 'question': \"What music group did Kanye join when he couldn't release his solo album?\"}, '56d45f882ccc5a1400d830f1': {'truth': 'Go-Getters', 'predicted': 'the Go-Getters', 'question': 'What late 1990s Chicago rap group was Kanye West a member of?'}, '56dd37fe66d3e219004dac77': {'truth': 'non-Commonwealth countries', 'predicted': 'non-Commonwealth', 'question': 'In what kinds of nations can the head of government attain the title of Excellency?'}, '56dd37fe66d3e219004dac79': {'truth': \"Her Majesty's Most Honourable Privy Council\", 'predicted': '', 'question': 'What are British prime ministers part of that grants them the title Right Honourable?'}, '56dd37fe66d3e219004dac7a': {'truth': 'Canada', 'predicted': 'Commonwealth countries prime ministers and former prime ministers are styled Right Honourable due to their position, for example in the Prime Minister of Canada', 'question': 'What is an example of a country where prime ministers can be called Right Honourable solely because of their position?'}, '5acfc9ba77cf76001a685fb7': {'truth': '', 'predicted': 'prime ministers and former prime ministers', 'question': 'Who is called Honourable in non Commonwealth countries?'}, '5728171b4b864d1900164454': {'truth': '128 code points', 'predicted': '', 'question': 'What is the ISCII standard? '}, '5acd579107355d001abf3dee': {'truth': '', 'predicted': 'because the set of ligatures is font-dependent', 'question': 'Why are more ligatures in Unicode likely to happen?'}, '56fdee67761e401900d28c58': {'truth': 'an assembler.', 'predicted': 'an assembler', 'question': 'Programs that convert assembly language into machine language are called what?'}, '57288d3b3acd2414000dfae3': {'truth': 'College of Engineering', 'predicted': 'BYU College of Engineering', 'question': 'Which BYU college was founded by former alumnus Harvey Fletcher?'}, '57288d3b3acd2414000dfae4': {'truth': 'the electronic television', 'predicted': '', 'question': 'What did alumnus Philo T. Farnsworth invent before receiving his honorary degree from the college?'}, '57288d3b3acd2414000dfae5': {'truth': 'Tracy Hall', 'predicted': 'H. Tracy Hall', 'question': 'Which notable former BYU student invented the man-made diamond?'}, '57288d3b3acd2414000dfae6': {'truth': 'a new type of diamond press, the tetrahedral press', 'predicted': '', 'question': 'What did former student Tracy Hall invent as a BYU professor of chemistry and Director of Research?'}, '5acea46832bba1001ae4aeaf': {'truth': '', 'predicted': 'Robert Millikan', 'question': 'Who did Harvey Farnsworth carry out the oil-drop experiment with?'}, '5ad0124077cf76001a6868b0': {'truth': '', 'predicted': 'Eupraxia', 'question': 'Who was the daughter of Vladimir the Great?'}, '5727fb4eff5b5019007d99e4': {'truth': 'Muslim Brotherhood', 'predicted': 'The Muslim Brotherhood', 'question': 'What group supported the RCC?'}, '572ea6d5cb0c0d14000f13f0': {'truth': 'Through the force of sheer numbers, the English-speaking American settlers entering the Southwest established their language, culture, and law', 'predicted': '', 'question': \"Why isn't the southwest Spanish speaking?\"}, '572ea6d5cb0c0d14000f13f1': {'truth': 'United States never developed bilingualism as Canada did.', 'predicted': '', 'question': 'Is Canada bilingual?'}, '572ea6d5cb0c0d14000f13f3': {'truth': \"the convention's English-speaking participants felt that the state's remaining minority of Spanish-speakers should simply learn English\", 'predicted': '', 'question': \"Why didn't California officially become  bilingual?\"}, '572ea6d5cb0c0d14000f13f4': {'truth': 'the convention ultimately voted 46-39 to revise the earlier clause so that all official proceedings would henceforth be published only in English.', 'predicted': '', 'question': 'Was there a court ruling?'}, '570c5037b3d812140066d0bf': {'truth': '1207 and 1215', 'predicted': 'between at least 1207 and 1215', 'question': 'When did John have conjugal relationships with Isabella?'}, '570c5037b3d812140066d0c0': {'truth': 'five children', 'predicted': 'five', 'question': 'How many children did John and Isabella have?'}, '5727011b708984140094d843': {'truth': 'Heavy Brigade', 'predicted': 'the Heavy Brigade', 'question': \"Who countered the Russian cavalry's movement?\"}, '5727011b708984140094d844': {'truth': 'local commanders', 'predicted': 'the local commanders', 'question': 'Who failed to take advantage of the retreat?'}, '56e4cfd839bdeb14003479df': {'truth': 'solitary housing estates and suburban sprawl.', 'predicted': '', 'question': 'What kinds of buildings and building developments are the new movements not in favor of?'}, '56e4cfd839bdeb14003479e1': {'truth': 'modernist and globally uniform architecture', 'predicted': '', 'question': 'What older architectural movements do the newer movements not go along with?'}, '573428b44776f419006619c9': {'truth': 'in the western foothills of the Santa Catalina Mountains', 'predicted': 'western foothills of the Santa Catalina Mountains', 'question': 'Where is Oro Valley?'}, '573428b44776f419006619cd': {'truth': 'northwest', 'predicted': 'northwest of the city limits is diverse, ranging from the rural communities of Catalina and parts of the town of Marana, the small suburb of Picture Rocks, the affluent town of Oro Valley in the western foothills of the Santa Catalina Mountains', 'question': 'Which direction from Tucson is Picture Rocks?'}, '572823804b864d190016454f': {'truth': '1897', 'predicted': '', 'question': \"In what year was Queen Victoria's Diamond Jubilee?\"}, '5a6280f6f8d794001af1c070': {'truth': '', 'predicted': 'Signal Hill', 'question': \"What hill is St. John's located on?\"}, '5acfa94777cf76001a685795': {'truth': '', 'predicted': 'English Civil War', 'question': 'What war strenthened the monarchs position?'}, '5acfa94777cf76001a685797': {'truth': '', 'predicted': 'Bill of Rights', 'question': 'What bill was passed in the 17th century?'}, '5acfa94777cf76001a685798': {'truth': '', 'predicted': 'Bill of Rights', 'question': 'What bill gave the monarch the power to establish law and impose taxes?'}, '56f72fe03d8e2e1400e37403': {'truth': 'popular styles', 'predicted': 'most popular styles', 'question': 'What is usually written in song forms?'}, '56f72fe03d8e2e1400e37404': {'truth': 'sophisticated', 'predicted': 'highly sophisticated', 'question': 'The concerto, symphony, sonata and opera are examples of what type of musical forms?'}, '572b6f49111d821400f38e9d': {'truth': 'Sense without Matter', 'predicted': '', 'question': 'What book was written by A.A. Luce?'}, '572b6f49111d821400f38ea0': {'truth': 'Berkeley', 'predicted': '', 'question': 'Whose work is Sense without Matter regarded as updating?'}, '5a7c7516e8bc7e001a9e1e25': {'truth': '', 'predicted': 'vocabulary', 'question': \"What aspect of Berkeley's writing did Foster modernize?\"}, '57268c78708984140094c9bb': {'truth': 'Clifford and Angela Levin', 'predicted': 'Angela Levin', 'question': 'Who wrote Max Clifford: Read All About It?'}, '57268c78708984140094c9bc': {'truth': 'Starr', 'predicted': '', 'question': 'Who was writing a book with McCaffrey?'}, '57268c78708984140094c9be': {'truth': 'the attention helped to revive his career', 'predicted': 'helped to revive his career', 'question': \"How did attention from the story impact Starr's career?\"}, '56dfc0ae231d4119001abd96': {'truth': 'An upstream ISP usually has a larger network than the contracting ISP', 'predicted': '', 'question': 'Why does an ISP need to pay an upstream ISP?'}, '56dfc0ae231d4119001abd97': {'truth': 'access to parts of the Internet the contracting ISP by itself has no access to', 'predicted': 'access to parts of the Internet', 'question': 'What does an upstream ISP provide for an ISP?'}, '5a10dee906e79900185c343b': {'truth': '', 'predicted': 'Internet access', 'question': 'What do ISPs pay customers for?'}, '572ed3f503f9891900756a6b': {'truth': 'common divine source', 'predicted': 'their common divine source', 'question': 'What do devout Muslims believe is the reason for the overlap of events and characters in the Bible and Quran?'}, '56f739203d8e2e1400e3749d': {'truth': 'to create an obligation under international law', 'predicted': 'an obligation under international law', 'question': 'North Korea and the United States have been characterized by a disagreement over one parties desire to create what with respect to security guarantees and nuclear proliferation?'}, '56d632371c85041400946fe3': {'truth': 'the face or neck.', 'predicted': 'face or neck', 'question': 'Children are often bit where by dogs?'}, '572f068ccb0c0d14000f1718': {'truth': 'most US and Canadian jurisdictions', 'predicted': 'US and Canadian', 'question': 'In what neighboring countries are passenger elevators required to adhere to Standard A17.1?'}, '57303bb004bcaa1900d773f1': {'truth': 'The Northern Ireland Peace Process', 'predicted': '', 'question': 'What has caused several uncommon arrangements between the various states in the United Kingdom?'}, '57303bb004bcaa1900d773f2': {'truth': 'choice of Irish or British citizenship or both', 'predicted': 'Irish or British citizenship', 'question': 'What type of citizenship can Northern Ireland people have?'}, '57303bb004bcaa1900d773f3': {'truth': 'policies common across the island of Ireland', 'predicted': '', 'question': 'The 1998 Good Friday Agreement resulted in what arrangement?'}, '5acda46b07355d001abf48a2': {'truth': '', 'predicted': 'Republic of Ireland, Northern Ireland and the United Kingdom', 'question': 'The Northern Atlantic Peace Process involves arrangements between which kingdoms?'}, '5733b2e14776f41900661086': {'truth': 'cultural and material lives of past societies', 'predicted': 'the cultural and material lives of past societies', 'question': 'What are artifacts, faunal remains and human altered landscapes the evidence of?'}, '5733b2e14776f41900661087': {'truth': 'human behavior and cultural practices', 'predicted': 'patterns of past human behavior and cultural practices', 'question': 'What can archaeologists deduce from material remains?'}, '5733b2e14776f41900661088': {'truth': 'past human groups', 'predicted': '', 'question': 'What do Ethnoarchaeologists gain a better understanding of by studying living human groups?'}, '5733b2e14776f41900661089': {'truth': 'in similar ways', 'predicted': '', 'question': 'How are long dead human groups presumed to have lived and behaved as compared to still living populations?'}, '5ad2d6a8d7d075001a42a420': {'truth': '', 'predicted': 'Archaeology', 'question': 'What is the study of humanity?'}, '57318ae9e6313a140071d07f': {'truth': '30', 'predicted': 'over 30', 'question': 'About how many governments recognized the legitimacy of the NTC at a meeting on July 15, 2011?'}, '572fb5b0b2c2fd1400568396': {'truth': 'is not a reproductive process', 'predicted': 'not a reproductive process', 'question': 'Is creating endospore a reproductive process?'}, '572fb5b0b2c2fd1400568397': {'truth': 'cortex layer', 'predicted': 'a cortex layer', 'question': 'What are ribosomes in endospores are enclosed in?'}, '57278133dd62a815002e9eec': {'truth': 'form roots and shoots', 'predicted': '', 'question': 'What can plant callus be coaxed into doing?'}, '572a7b02111d821400f38b53': {'truth': 'El Paso', 'predicted': '', 'question': 'What was the second poorest US city in 2004?'}, '572a7b02111d821400f38b56': {'truth': 'fifth', 'predicted': 'fifth-richest', 'question': 'In terms of purchasing power, where did Miami rank among world cities in a 2009 UBS study?'}, '5ad39216604f3c001a3fe693': {'truth': '', 'predicted': 'fifth-richest', 'question': 'In terms of purchasing power, where did Miami rank among world cities in a 2008 UBS study?'}, '572f8622947a6a140053ca19': {'truth': 'nationalist demagogues', 'predicted': 'nationalist demagogues—the most infamous being Adolf Hitler', 'question': 'In some world states who did the people turn to? '}, '572f8622947a6a140053ca1b': {'truth': 'the rise of Nazism', 'predicted': 'Nazism', 'question': 'What did the Convulsion caused by the global depression resul in?'}, '570c27686b8089140040fb9b': {'truth': 'communist ties', 'predicted': 'communist', 'question': 'What ties did the FBI believe civil rights leaders had?'}, '570c27686b8089140040fb9d': {'truth': 'FBI', 'predicted': 'FBI inaction', 'question': 'What agency had Dr. T.R.M. Howard criticized?'}, '5ad37d89604f3c001a3fe418': {'truth': '', 'predicted': 'communist', 'question': 'What ties did civil rights leaders believe the FBI had?'}, '5ad37d89604f3c001a3fe419': {'truth': '', 'predicted': '1956', 'question': 'When did Dr. T.R.M. Howard send an open letter denouncing Hoover?'}, '57096fa9ed30961900e84123': {'truth': 'animal emotions, animal culture and learning', 'predicted': 'personal symbolic name use, animal emotions, animal culture and learning, and even sexual conduct', 'question': 'What are some fields of knowledge concerning the animal world that have been revolutionizes in the 21st century?'}, '59fc23cca9fb160018f10da3': {'truth': '', 'predicted': 'human communication', 'question': 'Anthroposemiotics is the study of animal what?'}, '59fc23cca9fb160018f10da4': {'truth': '', 'predicted': 'animal communication', 'question': 'Zoo semiotics is the study of human what?'}, '59fc23cca9fb160018f10da6': {'truth': '', 'predicted': 'Animal communication', 'question': 'What is defined as any one behavior of all animals that affects the current or future behavior of another animal?'}, '59fc23cca9fb160018f10da7': {'truth': '', 'predicted': '21st century', 'question': 'Human and animal communication is a rapidly growing field of study in what century.'}, '5726dde0f1498d1400e8edf5': {'truth': 'the worst record of labor tension of any university in the U.S.', 'predicted': '', 'question': \"What are The New York Times' views on Yale's labor tension?\"}, '5726dde0f1498d1400e8edf6': {'truth': 'Professor David Graeber', 'predicted': 'David Graeber', 'question': 'What professor was retired in a 2003 labor strike?'}, '5726dde0f1498d1400e8edf7': {'truth': 'he came to the defense of a student who was involved in campus labor issues.', 'predicted': 'he came to the defense of a student who was involved in campus labor issues', 'question': 'Why was Professor David Graeber retired during the strike?'}, '5726dde0f1498d1400e8edf8': {'truth': \"Yale's unusually large endowment\", 'predicted': 'endowment', 'question': 'What adds to the tensions during wage considerations?'}, '5ad3e20e604f3c001a3ff524': {'truth': '', 'predicted': 'he came to the defense of a student who was involved in campus labor issues', 'question': 'Why was Professor David Graeber hired during the strike?'}, '5ad3e20e604f3c001a3ff525': {'truth': '', 'predicted': \"Yale's unusually large endowment\", 'question': 'What eases the tensions during wage considerations?'}, '56f986ed9b226e1400dd1511': {'truth': 'the hypothalamus,', 'predicted': 'hypothalamus', 'question': 'In vertebrates, the most important part of the brain is what?'}, '56f986ed9b226e1400dd1512': {'truth': 'the hypothalamus,', 'predicted': 'hypothalamus', 'question': 'A collection of small nuclei at the base of the forebrain is called what?'}, '56f986ed9b226e1400dd1513': {'truth': 'the pituitary gland', 'predicted': 'pituitary', 'question': 'The gland directly underneath the hypothalamus is which gland?'}, '56f986ed9b226e1400dd1514': {'truth': 'the bloodstream', 'predicted': 'bloodstream', 'question': 'The pituitary gland sends hormones through what in the body?'}, '57324359b9d445190005e949': {'truth': 'Presbyterian', 'predicted': 'Presbyterian Church', 'question': 'What church did Eisenhower join in 1953?'}, '573106b7497a881900248b07': {'truth': 'only France and the United Kingdom', 'predicted': 'France and the United Kingdom', 'question': 'What 2 powers named in the 5 orignal great powers of the congress of vienna have maintained that status?'}, '573106b7497a881900248b09': {'truth': 'British Empire', 'predicted': 'the British Empire', 'question': 'What country emerged as the pre-eminent power, due to its navy and the extent of its territories?'}, '5a149f88a54d4200185292b1': {'truth': '', 'predicted': 'France and the United Kingdom', 'question': 'What two powers involved in the Franco-Prussian war have the same status today?'}, '5a149f88a54d4200185292b2': {'truth': '', 'predicted': 'British Empire', 'question': 'After WWII which country was the pre-eminent power?'}, '5a149f88a54d4200185292b3': {'truth': '', 'predicted': 'The balance of power between the Great Powers', 'question': 'What became a main influence for the Congress of Vienna?'}, '5a149f88a54d4200185292b4': {'truth': '', 'predicted': 'All politics', 'question': 'By what formula are all great powers reduced?'}, '5a149f88a54d4200185292b5': {'truth': '', 'predicted': 'France', 'question': 'What pre-eminent power lost the Franco-Prussian war?'}, '5727c1123acd2414000debb5': {'truth': 'Portugal and Catalonia', 'predicted': 'Portugal and Catalonia, the Junta changed its attitude, this time due to the exhaustion of Galicia, now involved not just in naval or oversea operations, but also in an exhausting war with the Portuguese', 'question': 'War broke out with which other countries?'}, '5727c1123acd2414000debb6': {'truth': 'second half of the 17th century', 'predicted': '17th century', 'question': 'When did the Galician Junta more often stand up to requests from the monarch?'}, '5727c1123acd2414000debb7': {'truth': 'there were frequent urban mutinies', 'predicted': '', 'question': 'In what way was the tension between the monarch and Galicia similar to the wars it was fighting?'}, '572a23643f37b31900478729': {'truth': 'Archaic Era', 'predicted': '', 'question': 'What term is used to describe the Early Neolithic era in American education?'}, '572a23643f37b3190047872a': {'truth': 'bow and arrow', 'predicted': '', 'question': 'What hunting weapon was found in the Southwestern US during  500 to 1200 C.E.?'}, '56fa5493f34c681400b0c088': {'truth': 'beds', 'predicted': 'chairs and beds. It is also used for tool handles and cutlery, such as chopsticks, toothpicks, and other utensils, like the wooden spoon', 'question': 'What pieces of furniture that most people use every night can be made out of wood?'}, '570b6593ec8fbc190045b9d4': {'truth': 'Adrenalize', 'predicted': 'Hysteria with Adrenalize', 'question': \"What was the title of Def Leppard's 1992 album?\"}, '570b6593ec8fbc190045b9d5': {'truth': 'Hysteria', 'predicted': '', 'question': 'Adrenalize followed what 1987 Def Leppard record?'}, '5a5a45ab9c0277001abe70e9': {'truth': '', 'predicted': '1991', 'question': 'What year did Ozzy Osbourne release Unlawful Carnal Knowledge?'}, '5a5a45ab9c0277001abe70ea': {'truth': '', 'predicted': 'Shake Your Money Maker', 'question': 'What was The Black Crowes second album?'}, '572849ebff5b5019007da0fc': {'truth': 'drones', 'predicted': '', 'question': 'What controversial technology did the US use in Pakistan?'}, '572849ebff5b5019007da0fd': {'truth': 'the Central Intelligence Agency', 'predicted': 'Central Intelligence Agency', 'question': 'Which US agency runs its drones in Pakistan?'}, '5a85e80ab4e223001a8e72c2': {'truth': '', 'predicted': 'a report on drone warfare and aerial sovereignty', 'question': 'What did the United States criticize?'}, '5a85e80ab4e223001a8e72c4': {'truth': '', 'predicted': 'Global War on Terror', 'question': 'What term was Pakistan accused of abusing?'}, '56cd8ffa62d2951400fa6723': {'truth': 'Japan', 'predicted': 'Japanese', 'question': 'What country does Akiko Komoto come from?'}, '5a8d914edf8bba001a0f9b0b': {'truth': '', 'predicted': 'nods and facial expressions', 'question': \"Through what can Link's verbalizations be discerned?\"}, '5a8d914edf8bba001a0f9b0d': {'truth': '', 'predicted': 'Akiko Kōmoto', 'question': \"Who provided the basis for Zelda's voice?\"}, '5a8d914edf8bba001a0f9b0f': {'truth': '', 'predicted': 'grunts', 'question': 'What does Zelda say when attacking?'}, '57096446ed30961900e8406d': {'truth': 'Lahaul Spiti', 'predicted': '', 'question': 'Who was last in population strength?'}, '5a3625b0788daf001a5f875a': {'truth': '', 'predicted': '21st', 'question': 'Where on the population chart is the Shimla district census wise, after being followed by Tripura?'}, '5727ae3e2ca10214002d9380': {'truth': 'Upanishads and Brahma', 'predicted': 'Brahma', 'question': 'On which sutras did the Vedanta school focus?'}, '5727ae3e2ca10214002d9381': {'truth': 'first millennium BCE', 'predicted': '', 'question': 'In what time did the Vedanta school become active?'}, '5727ae3e2ca10214002d9382': {'truth': 'Vedānta', 'predicted': 'Vedānta school', 'question': 'Which is the most developed and well known of the Hindu schools?'}, '5727ae3e2ca10214002d9383': {'truth': 'five or six methods', 'predicted': 'five or six', 'question': 'How many ways did the Vedantins have of gaining knowledge?'}, '5727ae3e2ca10214002d9384': {'truth': 'sub-school', 'predicted': '', 'question': 'On what was dependent for the choice of methods in gaining knowledge?'}, '57315bfaa5e9cc1400cdbf01': {'truth': 'Bastille', 'predicted': 'fall of Bastille', 'question': 'After the demise of what was the red flag linked to the French Revolution?'}, '5ad4fc2f5b96ef001a10a88f': {'truth': '', 'predicted': '\"Against us they have raised the bloody flag of tyranny!\"', 'question': 'What did Lisle de Rouget write?'}, '572fffeb947a6a140053cf3a': {'truth': 'ball grid array', 'predicted': 'ball grid array (BGA) and naked die technologies', 'question': 'To what type of packaging is thermal expansion particularly critical?'}, '572fffeb947a6a140053cf3c': {'truth': 'FR-6', 'predicted': '', 'question': 'What dielectric is matte glass and polyester?'}, '5ace942432bba1001ae4aace': {'truth': '', 'predicted': 'Thermal expansion', 'question': 'The ball gasket array (BGA) is an important what?'}, '573035c8b2c2fd1400568a7b': {'truth': 'San Diego International Airport (SAN)', 'predicted': 'San Diego International Airport', 'question': 'What is the more popular name of Lindbergh Field?'}, '572f83ae947a6a140053c9fa': {'truth': 'the onset of the Great Depression', 'predicted': '', 'question': 'What changed worldwide property drasrically?'}, '572f83ae947a6a140053c9fd': {'truth': '1930s or early 1940s', 'predicted': '1929. The Wall Street Crash of 1929 served to punctuate the end of the previous era, as The Great Depression set in. The Great Depression was a worldwide economic downturn starting in most places in 1929 and ending at different times in the 1930s or early 1940s', 'question': 'When did the great Depression end?'}, '572f83ae947a6a140053c9fe': {'truth': '20th century', 'predicted': '', 'question': 'The great depression is the worst economic downturn of what century?'}, '57109aada58dae1900cd6ac2': {'truth': 'the Parisian lodge', 'predicted': 'Parisian', 'question': 'Which lodge that met in the mid 1720s was composed of English Jacobite exiles?'}, '5730c888aca1c71400fe5aba': {'truth': 'Hewlett Packard (HP)', 'predicted': 'Hewlett Packard', 'question': 'What modern company introduced LEDs in 1968?'}, '57282ec23acd2414000df67d': {'truth': 'Ukrainian', 'predicted': 'Ukrainian independence in 1918', 'question': \"What country's independence were the chain members celebrating?\"}, '57282ec23acd2414000df67f': {'truth': 'the Democratic Bloc', 'predicted': 'Democratic Bloc', 'question': 'In 1990 which party had most of the election victories?'}, '572848e94b864d19001648c4': {'truth': 'LaserDisc', 'predicted': '', 'question': 'Which format is considered to look most realistic, LaserDisc or DVD?'}, '572848e94b864d19001648c6': {'truth': 'video signal-to-noise ratio and bandwidth', 'predicted': '', 'question': 'What features do LaserDiscs lack in, causing DVDs to appear sharper and clearer?'}, '5728b383ff5b5019007da4de': {'truth': 'Half the deported perished', 'predicted': 'Half', 'question': 'What percentage of Estonians died after deporation?'}, '5728b383ff5b5019007da4df': {'truth': 'the early 1960s', 'predicted': 'early 1960s', 'question': 'When were the deported Estonians allowed to return?'}, '5728b383ff5b5019007da4e1': {'truth': 'the Forest Brothers', 'predicted': 'Forest Brothers', 'question': 'Who fought a guerrilla war against the Soviets?'}, '572a3b486aef0514001553b8': {'truth': 'scholars', 'predicted': 'scholars. Franz von Juraschek was a leading economist in Austria-Hungary and a close friend of Eugen Böhm von Bawerk, one of the founders of the Austrian School of Economics. Von Juraschek was a statistician', 'question': \"What occupation did Hayek's grandfather's have?\"}, '572a3b486aef0514001553ba': {'truth': 'systematic works in biology', 'predicted': '', 'question': \"What did August von Hayek's father write?\"}, '56d109f317492d1400aab7cc': {'truth': 'Glow in the Dark Tour', 'predicted': 'Glow in the Dark', 'question': \"What was the name of Kanye's 2008 music tour?\"}, '56d109f317492d1400aab7cd': {'truth': 'Hawaii', 'predicted': 'Honolulu, Hawaii', 'question': 'In what state did Kanye West record them majority of his fourth album?'}, '56d109f317492d1400aab7ce': {'truth': '\"Love Lockdown\"', 'predicted': 'Love Lockdown', 'question': \"What was the first song released off of Kanye's fourth album?\"}, '572f8892b2c2fd14005681c8': {'truth': 'four-engined bombers', 'predicted': 'four-engined', 'question': 'What kind of bombers was Germany not able to build?'}, '56e08070231d4119001ac201': {'truth': '5', 'predicted': '4 to 5', 'question': 'How many digits did Saint Helena change their phone numbers to?'}, '5730878f2461fd1900a9ce8f': {'truth': 'Slavic tribes', 'predicted': '', 'question': 'Who populated the area between the Baltic Sea and the Black sea before Kievan Rus?'}, '5730878f2461fd1900a9ce90': {'truth': 'Novgorod', 'predicted': '', 'question': 'Where were the Limen Slavs located before Kievan Rus?'}, '56d2581659d6e41400145edc': {'truth': 'Atticus', 'predicted': '', 'question': 'Who was the only non-abusive father mentioned?'}, '572fc229b2c2fd1400568404': {'truth': 'Air Staff', 'predicted': 'The Air Staff', 'question': \"Who thought Dowding was stubborn and didn't like to cooperate?\"}, '572fc229b2c2fd1400568405': {'truth': 'Battle of Britain Day', 'predicted': 'Battle of Britain Day and the Big Wing', 'question': 'The Air Ministry was critical of Dowding after which battle?'}, '572fc229b2c2fd1400568406': {'truth': 'attack him and his abilities', 'predicted': 'a cudgel with which to attack him and his abilities', 'question': 'What did the Air Ministry plan to do with his failures?'}, '571de3ebb64a571400c71ddb': {'truth': 'elder family members', 'predicted': '', 'question': 'Who will not reveal full ancestral data to mixed race people?'}, '5ad2b36bd7d075001a429f92': {'truth': '', 'predicted': 'censuses', 'question': 'What identified slaves by name before the American Civil War?'}, '5ad2b36bd7d075001a429f93': {'truth': '', 'predicted': \"Thomas Jefferson's with Sally Hemings\", 'question': 'Who acknowledged their mixed-race slave children in records?'}, '572759cb5951b619008f888f': {'truth': 'chemicals industry', 'predicted': 'chemicals', 'question': 'What type of industry produced a growing chain of synthetic fibers?'}, '572759cb5951b619008f8890': {'truth': 'Nylon', 'predicted': '', 'question': 'What was manufactured completely from petrochemicals?'}, '5a8192e331013a001a334ccd': {'truth': '', 'predicted': 'Nylon', 'question': 'What was the first polyester fiber?'}, '5a8192e331013a001a334cd1': {'truth': '', 'predicted': 'DuPont', 'question': 'What company produced polyester and nylon in the 1930s and 1940s?'}, '5732a3dfcc179a14009dabc2': {'truth': '66 million years', 'predicted': '66 million', 'question': 'How many years long was the Cenozoic Era?'}, '5732a3dfcc179a14009dabc3': {'truth': 'Cretaceous–Paleogene extinction event', 'predicted': 'Paleogene', 'question': 'Which extinction marked the beginning of the Cenozoic Era?'}, '5732a3dfcc179a14009dabc4': {'truth': 'Cenozoic Era', 'predicted': 'The Cenozoic Era', 'question': 'What geologic period are we in currently?'}, '5732a3dfcc179a14009dabc6': {'truth': 'the Himalayas', 'predicted': 'Himalayas', 'question': 'The collision of the Indian sub continent and the Asian plate created which mountain range?'}, '56d8dc9cdc89441400fdb353': {'truth': 'Uyghurs living in Turkey', 'predicted': 'Uyghurs', 'question': 'Who protested for their compatriots who were in Xinjiang?'}, '56db0a87e7c41114004b4cb3': {'truth': 'Taksim Square.', 'predicted': 'Taksim Square', 'question': 'Where did the torch relay finish in Turkey?'}, '56db0a87e7c41114004b4cb5': {'truth': 'arrested', 'predicted': 'arrested by the police', 'question': 'What happened to protesters who tried to interrupt the carrying of the torch?'}, '571a993510f8ca140030518e': {'truth': 'modern-day Italians', 'predicted': 'Italians', 'question': 'The 2010 study found that what modern population is most closely related to Ashkenazi Jews?'}, '571a993510f8ca140030518f': {'truth': 'inter-marriage and conversions in the time of the Roman Empire', 'predicted': 'inter-marriage and conversions', 'question': 'Ashkenazi Jews and Italians may be genetically similar due to what two factors?'}, '56e168ebe3433e1400422ec7': {'truth': 'its stake in DuPont', 'predicted': '', 'question': 'What did Seagram sell to finance their purchase of a share in MCA/Universal?'}, '5ad15e76645df0001a2d18e2': {'truth': '', 'predicted': '80%', 'question': 'What percent stake did MCA/Univeral have in Matsushita?'}, '5ad15e76645df0001a2d18e5': {'truth': '', 'predicted': '1999', 'question': 'What year did PolyGram buy Seagram?'}, '56d0051e234ae51400d9c270': {'truth': 'algae may produce toxic chemicals', 'predicted': 'toxic chemicals', 'question': 'What is a reason why the water from a water stabilisation pond may be unusable?'}, '56ce451caab44d1400b88640': {'truth': 'he died', 'predicted': '', 'question': \"Why didn't Yonten Gyatso make it to Beijing?\"}, '571aeaf69499d21900609baa': {'truth': 'supported the tenets', 'predicted': '', 'question': 'Were Arians also Origenists?'}, '571aeaf69499d21900609bab': {'truth': 'bishops disagreed', 'predicted': '', 'question': 'Did the bishops consider themselves Arians?'}, '571aeaf69499d21900609bac': {'truth': 'a real theological ideology', 'predicted': '', 'question': 'What did the Council of Nicaea decide about Arianism?'}, '56defb12c65bf219000b3e73': {'truth': 'the Commander of the Royal Canadian Air Force', 'predicted': 'Commander', 'question': 'Who heads the Royal Canadian Air Force?'}, '56defb12c65bf219000b3e74': {'truth': 'Winnipeg', 'predicted': '', 'question': 'Where is the commander based out of?'}, '56defb12c65bf219000b3e77': {'truth': 'tactical commander', 'predicted': '', 'question': 'Who reports to the operational commander about the wings?'}, '5726da10dd62a815002e929d': {'truth': 'fall of 1939 till spring of 1940', 'predicted': 'lasting from fall of 1939 till spring of 1940', 'question': 'How long did liquidation occur?'}, '5726da10dd62a815002e929e': {'truth': '16,000 members', 'predicted': '16,000', 'question': ' How many intelligentia were killed during operation AB-Akiton?'}, '56cbd2356d243a140015ed66': {'truth': 'Polish and French', 'predicted': 'Polish', 'question': \"What was Frédéric's nationalities?\"}, '56cbd2356d243a140015ed67': {'truth': 'Romantic era', 'predicted': 'Romantic', 'question': 'In what era was Frédéric active in?'}, '56ce0a3762d2951400fa69d7': {'truth': 'Romantic era', 'predicted': 'Romantic', 'question': 'What era was Chopin active during?'}, '56cf54a2aab44d1400b8900a': {'truth': 'Romantic era', 'predicted': 'Romantic', 'question': 'Chopin was active during what era?'}, '56d1ca30e7d4791d009021a8': {'truth': 'Warsaw', 'predicted': 'Duchy of Warsaw', 'question': 'In what city was Chopin born and raised?'}, '5727a8874b864d19001639bb': {'truth': 'has since been ratified', 'predicted': 'ratified', 'question': 'What has since happened to the second series of bilateral agreements?'}, '5727a8874b864d19001639bc': {'truth': \"Switzerland's isolation from the rest of Europe\", 'predicted': 'isolation from the rest of Europe', 'question': 'What were the original bilateral agreements meant to minimize the negative consequences of?'}, '570a5e0e6d058f1900182dba': {'truth': 'one third', 'predicted': '', 'question': 'What portion of females reported that they were held back by managers?'}, '56df631296943c1400a5d4ac': {'truth': 'November', 'predicted': 'November to March have the highest mean wind speeds, with June to August', 'question': 'Along with March, what month has the fastest winds on average?'}, '572e7f0adfa6aa1500f8d047': {'truth': 'late Bronze Age', 'predicted': 'Bronze Age', 'question': 'During what Age did Cyprus experience two waves of Greek settlement?'}, '572e7f0adfa6aa1500f8d049': {'truth': '1400 BC', 'predicted': '', 'question': 'What year did Turkish Republic of Northern Cyprus begin visiting Cyprus?'}, '57285eda3acd2414000df970': {'truth': 'it makes those aspects more familiar', 'predicted': 'because it makes those aspects more familiar', 'question': 'Why does Stewart Guthrie believe that people project their human features onto non human things?'}, '57285eda3acd2414000df971': {'truth': \"god concepts are projections of one's father\", 'predicted': '', 'question': 'What did Frued believe about the belief in God?'}, '57285eda3acd2414000df972': {'truth': 'The construction of gods and spirits like persons', 'predicted': 'construction of gods and spirits like persons', 'question': 'What is one of the most common traits of religion according to Boyer?'}, '572c0480dfb02c14005c6b60': {'truth': 'much like people', 'predicted': 'behave much like people', 'question': 'How do supernatural entities act?'}, '570b2117ec8fbc190045b843': {'truth': 'financing', 'predicted': 'lacking the legislation or the financing for large-scale VRS services, and to provide the necessary telecommunication equipment to deaf users', 'question': 'What is one of the reasons why VRS services are not in most European countries?'}, '570b2117ec8fbc190045b844': {'truth': 'Germany', 'predicted': 'Germany and the Nordic countries are among the other leaders in Europe, while the United States', 'question': 'What European country is a leader in providing VRS services to its citizens?'}, '56e180f5e3433e1400422f96': {'truth': 'uniformity', 'predicted': 'relative uniformity', 'question': 'What do the dialects of Catalan feature?'}, '56e180f5e3433e1400422f97': {'truth': 'other Romance languages', 'predicted': '', 'question': 'In comparison to what are the dialects uniform?'}, '56e180f5e3433e1400422f98': {'truth': 'intelligibility', 'predicted': 'Mutual intelligibility', 'question': 'What is high among dialects?'}, '56e180f5e3433e1400422f9a': {'truth': 'Alguerese', 'predicted': 'Alguerese dialect', 'question': 'What dialect is the exception to intelligibility?'}, '56e0cfce7aa994140058e737': {'truth': 'Chrome', 'predicted': 'Google Chrome', 'question': 'What other browser has Google as the default search engine?'}, '56e0cfce7aa994140058e739': {'truth': 'Google Chrome', 'predicted': 'development at Google and of Google Chrome', 'question': 'The increased revenue funds what, in addition to Google?'}, '5a4d40ce7a6c4c001a2bbc84': {'truth': '', 'predicted': 'to make their engine default', 'question': 'Why do web browsers pay search engine companies?'}, '5726ee155951b619008f82ae': {'truth': 'carrying capacity', 'predicted': '', 'question': 'Prey that is eaten is simply replaced by anohter when the population is close to what?'}, '5728d8e3ff5b5019007da80f': {'truth': 'University of Detroit Mercy', 'predicted': 'The University of Detroit Mercy', 'question': 'What Catholic university is in Detroit?'}, '5ad269fad7d075001a4292fc': {'truth': '', 'predicted': 'Synthesizing: Ten Ragas to a Disco Beat', 'question': \"What was the name of Charanjit Singh's 1981 album?\"}, '5ad269fad7d075001a4292fd': {'truth': '', 'predicted': \"electronic instrumentation and minimal arrangement of Charanjit Singh's Synthesizing: Ten Ragas to a Disco Beat (1982), an album of Indian ragas\", 'question': \"What did Beat's album contains?\"}, '5ad269fad7d075001a4292ff': {'truth': '', 'predicted': 'electronic instrumentation and minimal', 'question': 'What sort of arrangement did Charanjit Singh use on his 1981 album?'}, '572b8fff111d821400f38f1b': {'truth': 'a Less Commonly Taught Language', 'predicted': 'Less Commonly Taught Language', 'question': 'What kind of language is Czech in U.S. schools?'}, '572b8fff111d821400f38f1d': {'truth': 'Spanish', 'predicted': '', 'question': 'What is the most commonly spoken non-English language at homes nationwide in the U.S.?'}, '5ad13a0b645df0001a2d12a6': {'truth': '', 'predicted': 'returned to opposition', 'question': 'What did Labour do after winning the 1970 general election?'}, '5ad13a0b645df0001a2d12a7': {'truth': '', 'predicted': 'Labour returned to opposition, but retained Harold Wilson', 'question': 'Who won the 1970 general election?'}, '5ad13a0b645df0001a2d12a9': {'truth': '', 'predicted': 'Ulster Unionists', 'question': 'What group of people supported the Conservatives? '}, '5ad13a0b645df0001a2d12aa': {'truth': '', 'predicted': 'they had fewer seats', 'question': 'Why were the Conservatives able to form a government alone?'}, '57266a17708984140094c531': {'truth': 'Federal law and treaties', 'predicted': '', 'question': 'What comes before state and territorial laws in the 50 U.S states?'}, '57266a17708984140094c533': {'truth': 'federal constitutional rights', 'predicted': 'any federal constitutional rights', 'question': 'States may grant their citizens border rights as long as they do not infringe on what?'}, '57266a17708984140094c535': {'truth': 'vary greatly from one state to the next.', 'predicted': 'state law, which can and does vary greatly from one state to the next.', 'question': 'Does every state have the same laws?'}, '5726d83d708984140094d331': {'truth': 'conflicting state and territorial laws', 'predicted': '', 'question': 'Federal law overrides what laws?'}, '572c9ab3dfb02c14005c6bab': {'truth': 'Federal law and treaties', 'predicted': '', 'question': 'Is there anything that trumps state law?'}, '572c9ab3dfb02c14005c6bad': {'truth': 'as long as they do not infringe on any federal constitutional rights', 'predicted': '', 'question': 'Can states grant rights to citizens that are not defined by the constitution?'}, '572c9ab3dfb02c14005c6bae': {'truth': '\"living law\"', 'predicted': '', 'question': 'What is day-to-day, operational law considered?'}, '572c9ab3dfb02c14005c6baf': {'truth': 'state law', 'predicted': '', 'question': 'What is living law mostly made up of?'}, '5a79baa717ab25001a8a0001': {'truth': '', 'predicted': 'conflicting', 'question': 'State and territorial laws preempt what kind of laws and treaties?'}, '5a79baa717ab25001a8a0003': {'truth': '', 'predicted': 'dual-sovereign system of American federalism', 'question': 'What type of government system do Indian reservations have? '}, '5728b8862ca10214002da659': {'truth': 'East India Company', 'predicted': 'Bengal Army of the East India Company', 'question': \"What British company was heavily involved in the defeat of the Nawab's forces?\"}, '5728b8862ca10214002da65a': {'truth': 'Robert Clive', 'predicted': \"Robert Clive, defeated the French-supported Nawab's forces. This was the first real political foothold with territorial implications that the British acquired in India. Clive\", 'question': 'Who did the East India Company appoint as Governor of Bengal?'}, '57283abb4b864d19001647b0': {'truth': 'St. George Cathedral.', 'predicted': '', 'question': 'Where was the August 9 liturgy held?'}, '57283abb4b864d19001647b1': {'truth': '1933', 'predicted': '', 'question': 'Prior to the September 8th rally when was the last Youth for Christ rally held?'}, '57296cccaf94a219006aa3e0': {'truth': 'United States', 'predicted': 'the United States', 'question': 'What country has been increasing its support for the fossil fuel and nuclear industries?'}, '5ad13603645df0001a2d1188': {'truth': '', 'predicted': 'Germany', 'question': 'What country is building its coal subsidy?'}, '57342bbc4776f419006619f1': {'truth': 'Artur Pizarro, Maria João Pires, Sequeira Costa', 'predicted': 'Artur Pizarro', 'question': 'What are some examples of classical pianists from Portugal?'}, '57342bbc4776f419006619f2': {'truth': 'Carlos Damas, Gerardo Ribeiro', 'predicted': 'Carlos Damas', 'question': 'What are some examples of classical violinists from Portugal?'}, '57342bbc4776f419006619f3': {'truth': 'José Vianna da Motta, Carlos Seixas, João Domingos Bomtempo, João de Sousa Carvalho, Luís de Freitas Branco and his student Joly Braga Santos', 'predicted': 'José Vianna da Motta, Carlos Seixas', 'question': 'Who are some notable musical composers from Portugal?'}, '5726d331f1498d1400e8ec6e': {'truth': 'the Berlin Conference', 'predicted': 'Berlin Conference', 'question': \"Where was Britain's claim to West Africa recognized in 1885?\"}, '570b09a96b8089140040f700': {'truth': 'STOBAR', 'predicted': 'STOBAR carrier: Admiral Flota Sovetskovo Soyuza Kuznetsov: 55,000 tonne Admiral Kuznetsov-class STOBAR aircraft', 'question': 'What type of carrier is Admiral Flota Sovetskovo Soyuza Kuznetsov?'}, '570b09a96b8089140040f701': {'truth': 'in 1985', 'predicted': '1985', 'question': 'When was Admiral Flota Sovetskovo Soyuza Kuznetsov first launched?'}, '570b09a96b8089140040f702': {'truth': 'Tbilisi', 'predicted': '', 'question': 'What was Admiral Flota Sovetskovo Soyuza Kuznetsov renamed?'}, '570b09a96b8089140040f704': {'truth': 'The P-700 systems', 'predicted': 'P-700 systems', 'question': 'What will be removed from Tbilisi in order to enlarge her below decks aviation facilities?'}, '5acd884007355d001abf4604': {'truth': '', 'predicted': '55,000 tonne', 'question': 'What type of airplane is Admiral Flota Sovetskovo Soyuza Kuznetsov?'}, '5acd884007355d001abf4608': {'truth': '', 'predicted': 'P-700 systems', 'question': 'What will be added from Tbilisi in order to enlarge her below decks aviation facilities?'}, '5709828fed30961900e84244': {'truth': '3rd–2nd century BC', 'predicted': 'around the 3rd–2nd century BC', 'question': 'When did Britain first use brass?'}, '5a836aeee60761001a2eb68c': {'truth': '', 'predicted': 'Native Americans', 'question': 'Who were the only copper miners in North America?'}, '5a836aeee60761001a2eb68f': {'truth': '', 'predicted': 'early 20th century', 'question': 'When did commercial production of copper end?'}, '56e163afe3433e1400422e62': {'truth': 'the National Football League', 'predicted': 'National Football League', 'question': 'What league do the new England patriots belong to?'}, '56e163afe3433e1400422e63': {'truth': '1960', 'predicted': '', 'question': 'What year were the new England patriots founded in?'}, '56e163afe3433e1400422e66': {'truth': 'Gillette Stadium', 'predicted': '', 'question': 'What stadium do the patriots play in?'}, '56f8d3179b226e1400dd1095': {'truth': 'post-World War I', 'predicted': 'post-World War I period', 'question': 'When were ski-lifts built in Swiss and Austrian towns?'}, '56f8d3179b226e1400dd1096': {'truth': 'the 1970s', 'predicted': '1970s', 'question': 'When were several new villages built in France almost exclusively for skiing?'}, '57303e5ea23a5019007fcffe': {'truth': 'through a Control Panel applet called \"Windows 7 File Recovery\"', 'predicted': '', 'question': 'How is Backup and Restore opened?'}, '57303e5ea23a5019007fd000': {'truth': 'other software', 'predicted': '', 'question': 'What is :74 used for? '}, '5726adf0dd62a815002e8cc5': {'truth': 'the landmark building', 'predicted': 'landmark building', 'question': 'Prior to the 20th century, a Gothic cathedral was considered to be what type of building in the town in which it was constructed?'}, '5726adf0dd62a815002e8cc6': {'truth': 'the ogival', 'predicted': 'ogival', 'question': 'What is another name for the pointed arch?'}, '5726adf0dd62a815002e8cc7': {'truth': 'the ribbed vault', 'predicted': 'the ogival or pointed arch, the ribbed vault, and the buttress', 'question': 'What is an example of architectural technology that is seen in Gothic construction?'}, '5726adf0dd62a815002e8cc8': {'truth': 'the buttress', 'predicted': 'the ogival or pointed arch, the ribbed vault, and the buttress', 'question': 'What is another example of architectural technology that is seen in Gothic construction?'}, '56df778a5ca0a614008f9adf': {'truth': '1898', 'predicted': '1898 until 1903', 'question': 'What year did Bell become President of the National Geographic magazine?'}, '5731dd950fdd8d15006c65b3': {'truth': 'British', 'predicted': '', 'question': 'What nationality will the new international school in Brasilia be?'}, '56f8cf869b226e1400dd1053': {'truth': 'Access courses', 'predicted': 'Access', 'question': 'What courses does Southampton City College offer to adult students?'}, '56f8cf869b226e1400dd1055': {'truth': 'Richard Taunton Sixth Form College', 'predicted': 'Itchen College and Richard Taunton Sixth Form College', 'question': \"What's the sixth form college named after a person?\"}, '572846473acd2414000df84e': {'truth': 'violently anti-communist, and much more militaristic than the norm', 'predicted': 'violently anti-communist', 'question': 'How did von Neumaan describe his political ideology?'}, '56dfb89e7aa994140058e06f': {'truth': 'accommodation', 'predicted': '', 'question': 'What amenity does an inn offer that pubs, alehouses and taverns usually do not?'}, '56dfb89e7aa994140058e070': {'truth': 'the UK', 'predicted': 'UK', 'question': \"In what nation's pubs is food often served?\"}, '5731eaa7e17f3d140042254a': {'truth': 'shipbuilding', 'predicted': 'Industrial activity, (including heavy industry like shipbuilding)', 'question': 'What type of industrial activity was evident in Greece in the period researched?'}, '5a7b2ba921c2de001afe9d82': {'truth': '', 'predicted': 'slightly lower', 'question': \"What was Greece's GDP decline between 1833 and 1911 compared with other Western European nations?\"}, '5a7b2ba921c2de001afe9d84': {'truth': '', 'predicted': 'defaulted on its external loans', 'question': 'What did Greece avoid in 1826, 1843, 1860 and 1894?'}, '572786b5dd62a815002e9f88': {'truth': \"Darwin's contemporaries thought the time he took was reasonable\", 'predicted': 'reasonable', 'question': \"What did Darwin's contemporaries think of the long delays on his publishing?\"}, '56e837f437bdd419002c44b3': {'truth': 'Ancient Greek', 'predicted': '', 'question': 'What do many historical linguists consider modern Greek to be a dialect of?'}, '5ad27318d7d075001a4294b0': {'truth': '', 'predicted': 'modern Greek', 'question': 'According to this few, what language is Ancient Greek a dialect of?'}, '56e09c507aa994140058e64d': {'truth': 'electrons', 'predicted': 'electron', 'question': 'When hydrogen oxidates, what is it removing?'}, '56e09c507aa994140058e651': {'truth': 'Bronsted-Lowry', 'predicted': 'Bronsted-Lowry theory', 'question': 'What theory suggests that acids are proton donors?'}, '56e790e237bdd419002c414f': {'truth': 'national universities', 'predicted': '', 'question': \"Against what other kinds of institutions was KU's engineering school compared?\"}, '57284dd0ff5b5019007da13f': {'truth': 'First Amendment rights, Fourth Amendment rights, and right to due process', 'predicted': 'First Amendment', 'question': 'What rights did the ACLU say the Patriot Act violated?'}, '57284dd0ff5b5019007da140': {'truth': \"a person's business, bookstore, and library records\", 'predicted': '', 'question': 'What did Section 215 of the Patriot Act allow the FBI to search?'}, '57284dd0ff5b5019007da141': {'truth': 'governing bodies in a number of communities', 'predicted': 'governing bodies', 'question': 'Who passed symbolic resolutions against the Patriot Act?'}, '5a86027fb4e223001a8e7383': {'truth': '', 'predicted': 'American Civil Liberties Union', 'question': 'Who sued the ACLU?'}, '5726e302f1498d1400e8eea9': {'truth': 'academic journals', 'predicted': '', 'question': 'Most serious studies in philosophy are segregated to what publications?'}, '5726e302f1498d1400e8eeab': {'truth': 'logic', 'predicted': '', 'question': 'What aspect of modern academic philosophy is less literary than technical in nature?'}, '5a7a553c21c2de001afe9b73': {'truth': '', 'predicted': 'academic journals', 'question': 'Most new pychologic work appears where?'}, '5a7a553c21c2de001afe9b74': {'truth': '', 'predicted': 'Descartes, Kierkegaard, Nietzsche', 'question': 'Besides Plato, Achilles, Socrates, and Augustine, who are other noted major philosophers in history?'}, '5a7e59cc48f7d9001a063517': {'truth': '', 'predicted': 'mathematics', 'question': 'Logic is technical and similar to what field of study?'}, '5726e48b708984140094d4ff': {'truth': 'glass', 'predicted': '', 'question': 'Besides porcelain, paper and mica, what other non conductive material was used as an insulator? '}, '5726e48b708984140094d501': {'truth': 'strip of impregnated paper', 'predicted': 'impregnated paper', 'question': 'What was layered between strips of metal in order to create paper capacitors?'}, '5726e48b708984140094d502': {'truth': 'in 1876', 'predicted': '1876', 'question': 'When were paper capacitors first manufactured?'}, '5acf588577cf76001a684bd2': {'truth': '', 'predicted': 'non conductive materials', 'question': 'Besides porcelain, paper and mica, what other conductive material was used as an insulator?'}, '5acf588577cf76001a684bd3': {'truth': '', 'predicted': 'the dielectric', 'question': 'For what use were conductive materials used in the first capacitors?'}, '5acf588577cf76001a684bd6': {'truth': '', 'predicted': 'decoupling', 'question': ' What other use did metal capacitors serve in the telecommunications industry?'}, '5730c30bf6cb411900e24460': {'truth': 'mid-19th', 'predicted': '19th', 'question': 'In what century did trading companies arrive in Tuvalu?'}, '57304c5e8ab72b1400f9c3ff': {'truth': '40,000 people', 'predicted': '40,000', 'question': 'How many people per week were losing housing?'}, '57304c5e8ab72b1400f9c400': {'truth': 'because of its vulnerable position on the south coast', 'predicted': 'because of its vulnerable position on the south coast and close proximity to German air bases', 'question': 'Why was Plymouth targeted the most?'}, '57304c5e8ab72b1400f9c402': {'truth': 'Over 2,000', 'predicted': '2,000', 'question': 'How many AAA shells were fired?'}, '5727b735ff5b5019007d933e': {'truth': 'ISO-8859-1', 'predicted': 'the content of ISO-8859-1', 'question': 'What were the first 256 code points of Unicode made identical to? '}, '5727b735ff5b5019007d9340': {'truth': 'a full Latin alphabet that is separate from the main Latin alphabet', 'predicted': 'a full Latin alphabet', 'question': 'What does the \"fullwidth forms\" section of code points encompass?'}, '5727b735ff5b5019007d9341': {'truth': 'Chinese, Japanese, and Korean', 'predicted': '', 'question': 'What are the CJK languages referenced?'}, '5acd10bc07355d001abf32f6': {'truth': '', 'predicted': 'CJK ideographs', 'question': 'What are Latin characters called when they are half width?'}, '5acd10bc07355d001abf32f8': {'truth': '', 'predicted': 'western', 'question': 'The 256 initial points make it difficult to translate what kind of text?'}, '57322fcce17f3d14004226ef': {'truth': 'Democratic Congressional Campaign Committee', 'predicted': 'minority leader may call meetings of the Democratic Caucus. He or she is a member of the Democratic Congressional Campaign Committee', 'question': 'According to democratic rules of the 106th congress what campaign membership do they have?'}, '57322fcce17f3d14004226f0': {'truth': 'Democratic Leadership Council', 'predicted': '', 'question': 'According to democratic rules of the 106th congress what leadership members do they appoint?'}, '57317c50a5e9cc1400cdbfc2': {'truth': 'Transfiguration', 'predicted': 'the Transfiguration', 'question': \"What was the name of the painting used to represent raphael in St. Peter's?\"}, '5726e7d3f1498d1400e8ef87': {'truth': 'British Ambassador and the French Emperor', 'predicted': 'the British Ambassador and the French Emperor', 'question': 'Who was Burgoyne visiting in Paris?'}, '5726e7d3f1498d1400e8ef88': {'truth': 'Lord Cowley', 'predicted': 'The Lord Cowley', 'question': 'Who wrote to Burgoyne on February 8th?'}, '56fad599f34c681400b0c147': {'truth': '2006', 'predicted': '', 'question': 'When did Richards publish his mtDNA research?'}, '56fad599f34c681400b0c148': {'truth': 'Ethiopians', 'predicted': 'Ethiopians and North Africans', 'question': 'Along with Egyptians, Algerians and Somalis, what people commonly possess the M1 haplogroup?'}, '5733e8ccd058e614000b656e': {'truth': 'active in the allied war effort', 'predicted': '', 'question': \"What were Boas' peers doing in the 1940s?\"}, '5733e8ccd058e614000b6570': {'truth': 'the armed forces', 'predicted': 'armed forces', 'question': 'What did many anthropologists serve in?'}, '5733e8ccd058e614000b6572': {'truth': 'communist sympathies.', 'predicted': 'communist sympathies', 'question': 'Why are several anthropologists dismissed from their jobs, according to David H. Price?'}, '572a3a0b6aef0514001553a4': {'truth': 'the A-series', 'predicted': 'A-series', 'question': \"What was McTaggart's first series called?\"}, '5a42d2aa4a4859001aac734b': {'truth': '', 'predicted': 'J. M. E. McTaggart', 'question': 'Who created a problem with the flow of time?'}, '56e79b2d00c9c71400d77385': {'truth': 'Zhong Mountain', 'predicted': 'Ningzhen range is in Nanjing; the Loong-like Zhong Mountain', 'question': 'What mountain is in the East are of Nanjing?'}, '56e79b2d00c9c71400d77386': {'truth': 'Stone Mountain', 'predicted': 'Ningzhen range is in Nanjing; the Loong-like Zhong Mountain is curling in the east of the city; the tiger-like Stone Mountain', 'question': 'What mountain lies in the west of Nanjing?'}, '5730ef4105b4da19006bcc60': {'truth': 'St Bernard of Clairvaux', 'predicted': '', 'question': 'Who began to query the position of the conception of Mary following the 11th century ?'}, '5730ef4205b4da19006bcc61': {'truth': 'St Bernard blames the canons of the metropolitan church of Lyon for instituting such a festival without the permission of the Holy See.', 'predicted': 'the canons of the metropolitan church of Lyon', 'question': \"Who did the query starter lay blame upon for the festivals that surrounded Mary's inception ?\"}, '5730ef4205b4da19006bcc63': {'truth': \"conception in the active sense of the mother's cooperation\", 'predicted': '', 'question': \"What did the query starter believe had been done by Mary's direct  maternal line that contradict the conception theory of immaculate for Mary ?\"}, '5a273563c93d92001a400427': {'truth': '', 'predicted': 'St Bernard blames the canons of the metropolitan church of Lyon', 'question': 'Who instituted a feast of the conception of the Blessed Virgin with the permission of the Holy See?'}, '5a273563c93d92001a400428': {'truth': '', 'predicted': 'Mary', 'question': 'Whose conception does St. Bernard say was sinless?'}, '5726c3b1f1498d1400e8ea97': {'truth': 'Secretariat for non-Christians', 'predicted': 'Secretariat for non-Christians, later renamed the Pontifical Council for Interreligious Dialogue', 'question': 'What organization did Paul VI create to address non believers by the church?'}, '5726c3b1f1498d1400e8ea98': {'truth': 'Pontifical Council for Interreligious Dialogue', 'predicted': '', 'question': 'What was the Secretariat for non-believers eventually renamed?'}, '5a6bb40d4eec6b001a80a4fc': {'truth': '', 'predicted': '300', 'question': \"How many mg of fat is suggested for a healthy person's diet?\"}, '56ce81bdaab44d1400b88817': {'truth': 'bass', 'predicted': '', 'question': 'What part of audio output was substandard on 3rd generation iPods?'}, '56ce81bdaab44d1400b88818': {'truth': 'undersized DC-blocking capacitors', 'predicted': 'DC-blocking capacitors', 'question': 'What component was to blame for the weak bass of the 3rd generation iPod?'}, '571a10584faf5e1900b8a881': {'truth': '5th Avenue Theatre', 'predicted': 'The 5th Avenue Theatre', 'question': 'What Seattle theater was built in 1926?'}, '5726638e708984140094c490': {'truth': 'modernizing factories', 'predicted': 'modernizing factories, houses (decline of coal heating) and cars', 'question': 'What is one thing that helped to improve condition of forests, rivers and air?'}, '5726638e708984140094c491': {'truth': 'Uranium surface mines around Ronneburg have been remediated', 'predicted': 'contaminated areas such as the former Uranium surface mines around Ronneburg have been remediated', 'question': 'What has been done to former Uranium surface mines around Ronneburg?'}, '5726638e708984140094c493': {'truth': 'discharges of K+S salt mines around Unterbreizbach', 'predicted': 'discharges of K+S salt mines', 'question': 'What is causing the salination of the Werra river?'}, '5a7cc42be8bc7e001a9e1fd9': {'truth': '', 'predicted': '1990', 'question': 'When has environmental damage in Thuringia been increased since?'}, '56e19df2e3433e1400423035': {'truth': 'Standard Catalan', 'predicted': '', 'question': 'What form is excepted by most speakers?'}, '56e19df2e3433e1400423037': {'truth': 'traditional ones', 'predicted': 'traditional', 'question': 'What language forms are not now used in eastern Catalonia?'}, '570c6506fed7b91900d45981': {'truth': 'dictatorships', 'predicted': 'the dictatorships of Miguel Primo de Rivera (1923–1930) and especially of Francisco Franco (1939–1975)', 'question': 'What caused the suppression of regional cultures?'}, '570c6506fed7b91900d45982': {'truth': 'Spanish (Castilian)', 'predicted': 'Spanish (Castilian', 'question': 'Of the languages of the are, what was the only approved language?'}, '570c6506fed7b91900d45984': {'truth': 'blaugrana', 'predicted': 'blaugrana team', 'question': 'What team was awarded by Franco for having a good relationship?'}, '570c6506fed7b91900d45985': {'truth': \"'More than a club'\", 'predicted': \"More than a club'\", 'question': 'What motto of the team Barcelona appealed to the Catalans?'}, '5726086889a1e219009ac16f': {'truth': 'By careful selection of which electron energy level transitions are used, and fluorescent coatings which modify the spectral distribution', 'predicted': '', 'question': 'How can luminescent light sources be modified to resemble the appearance of incandescents?'}, '572844c5ff5b5019007da066': {'truth': 'unitary state', 'predicted': 'a unitary state', 'question': 'How has United Kingdom been governed?'}, '572844c5ff5b5019007da067': {'truth': 'UK has relied on gradual devolution to decentralise political power', 'predicted': 'gradual devolution to decentralise political power', 'question': 'Instead of the UK adopting the federalist model, what did they do?'}, '572844c5ff5b5019007da068': {'truth': '1914', 'predicted': 'with the Government of Ireland Act 1914', 'question': 'When did devolution in the UK begin?'}, '572844c5ff5b5019007da069': {'truth': 'which granted home rule to Ireland as a constituent country of the former United Kingdom of Great Britain and Ireland', 'predicted': 'home rule', 'question': 'What is Ireland Act 1914?'}, '572844c5ff5b5019007da06a': {'truth': 'eventually evolved into the modern day Republic of Ireland', 'predicted': 'Republic of Ireland', 'question': 'What is Irish Free State?'}, '5acfbe5777cf76001a685c00': {'truth': '', 'predicted': 'sovereign Irish Free State', 'question': 'What is Irish Cost State?'}, '572ea043c246551400ce4425': {'truth': 'English', 'predicted': '', 'question': 'Are there any Western languages spoken in Cyprus?'}, '572ea043c246551400ce4426': {'truth': 'Russian', 'predicted': '', 'question': 'Are there any Eastern languages spoken in Cyprus?'}, '56fdc60e19033b140034cd67': {'truth': 'a form of tally stick', 'predicted': 'tally stick', 'question': 'The earliest device to help count was what?'}, '56cfef3c234ae51400d9c10d': {'truth': 'the Revolutionary Étude', 'predicted': 'Revolutionary Étude', 'question': 'What is another title Op. 10, No. 12 has garnered? '}, '56cfef3c234ae51400d9c10f': {'truth': 'Sonata No. 2', 'predicted': 'Sonata No. 2 (Op. 35), the one case where he did give a title, was written before the rest of the sonata', 'question': 'The Funeral March was written as part of what piece?'}, '56d3913859d6e41400146793': {'truth': 'one', 'predicted': '', 'question': 'How many instrumental works did Chopin give a descriptive name to?'}, '570d6de5fed7b91900d460b7': {'truth': 'Charles I', 'predicted': 'Habsburg king Charles I', 'question': 'Whose government did the guilds rebel against?'}, '5acec3f332bba1001ae4b339': {'truth': '', 'predicted': 'Domestic Organization', 'question': 'What organization did Brigham Young create?'}, '5acec3f332bba1001ae4b33a': {'truth': '', 'predicted': 'group of teachers', 'question': \"What did Young's Domestic Organization do?\"}, '5acec3f332bba1001ae4b33d': {'truth': '', 'predicted': '3rd', 'question': 'What did the Princeton Review rank BUY for LGBT-friendliness? '}, '57291a3e1d04691400779037': {'truth': 'neither a Western German state nor part of one', 'predicted': '', 'question': 'Which state was West Berlin apart of?'}, '5a4745ff5fd40d001a27dd9d': {'truth': '', 'predicted': 'nine', 'question': 'When South Baden was formed in 1952, how many states were there?'}, '5a4745ff5fd40d001a27dda1': {'truth': '', 'predicted': 'special status', 'question': 'What kind of status did Saarland have when integrated with West Germany?'}, '5a514b63ce860b001aa3fcbd': {'truth': '', 'predicted': 'nine', 'question': \"How many states did West Germany add after it's founding in 1952?\"}, '5a514b63ce860b001aa3fcc1': {'truth': '', 'predicted': 'Western Allies', 'question': 'What German state did West Berlin fall under?'}, '5728f761af94a219006a9e86': {'truth': 'shameful, if not criminal', 'predicted': '', 'question': 'How did the samurai view kidnapping concubines?'}, '5728f761af94a219006a9e87': {'truth': 'many wealthy merchants', 'predicted': 'wealthy merchants', 'question': 'Who thought being a concubine was better than being a wife?'}, '5728f761af94a219006a9e88': {'truth': \"her family's money erased the samurai's debts\", 'predicted': '', 'question': 'Why did merchants prefer that their daughters not marry samurai?'}, '5728f761af94a219006a9e89': {'truth': \"the son could inherit his father's social status\", 'predicted': \"inherit his father's social status\", 'question': 'What happened if a commoner concubine had a son?'}, '5730ed3ea5e9cc1400cdbaf1': {'truth': '10 years', 'predicted': '', 'question': 'What is the required education for males on Tuvalu?'}, '57060fde52bb891400689838': {'truth': 'foreign policy', 'predicted': 'foreign', 'question': 'What kind of policy did The Times express reservation for when concerning the political stances of Barack Obama?'}, '572a518ab8ce0319002e2a94': {'truth': 'interests of the state', 'predicted': 'state', 'question': 'What dominated all economic and political interests?'}, '572a518ab8ce0319002e2a95': {'truth': 'capitalist and mercantile economies', 'predicted': 'capitalist and mercantile', 'question': 'What were the types of economies that were being developed in western Europe?'}, '56f8c9d29e9bad19000a04f1': {'truth': 'identical daughter cells', 'predicted': 'daughter cells', 'question': 'In cell division, what two cells are created? '}, '5726e07a5951b619008f8106': {'truth': 'the PVA', 'predicted': 'PVA', 'question': \"Who's attack resulted in victory at Hoengseong?\"}, '5726e07a5951b619008f8109': {'truth': 'more than 25,000', 'predicted': '', 'question': 'How many PVA soldiers fought in this battle and lost?'}, '57266c865951b619008f7251': {'truth': \"Hispanic or Latino ancestry ancestry accounted for 22.5% (4,223,806) of Florida's population\", 'predicted': '22.5%', 'question': 'What percentage of the Florida population in 2010 was Hispanic '}, '57266c865951b619008f7253': {'truth': 'Nearly 80% of Cuban Americans live in Florida, especially South Florida', 'predicted': '80%', 'question': 'What percentage of Cuban Americans live in Florida '}, '5acd969f07355d001abf47af': {'truth': '', 'predicted': 'New York', 'question': 'What area is the third largest Puerto Rican population in the US?'}, '5acd969f07355d001abf47b2': {'truth': '', 'predicted': '1.6%', 'question': 'What percentage of the Florida population in 2001 was Colombian?'}, '56e0cdc37aa994140058e722': {'truth': 'sales of Windows to computer manufacturers and direct to users', 'predicted': 'the sales of Windows to computer manufacturers and direct to users', 'question': 'Internet Explorer was partially funded in what two ways?'}, '57302bbda23a5019007fcee4': {'truth': 'were believed to be funding rebels in Sierra Leone.', 'predicted': 'funding rebels in Sierra Leone', 'question': 'Why were sanctions place on Liberian timber exports?'}, '57302bbda23a5019007fcee5': {'truth': 'in 2006', 'predicted': '2006', 'question': 'When were the timber export sanctions lifted for Liberia?'}, '5a62be8af8d794001af1c1fc': {'truth': '', 'predicted': '2006', 'question': 'In what year did UN sanctions prohibit membership in the World Trade Organization?'}, '5a62be8af8d794001af1c1ff': {'truth': '', 'predicted': '2006', 'question': 'In what year were World Trade Organization sanctions lifted?'}, '5733b195d058e614000b6083': {'truth': 'that Islamic militarism in the east of the country was on the rise following the escape of 25 militants from a Tajik prison in August', 'predicted': 'Islamic militarism in the east of the country', 'question': 'Why was there concerns in 2010?'}, '5aced84b32bba1001ae4b716': {'truth': '', 'predicted': '3', 'question': 'Fighting outside Guam left how many militants dead?'}, '56e7b1e900c9c71400d77506': {'truth': 'Double Summer Time', 'predicted': '', 'question': 'What term was used in Britain for double daylight saving time?'}, '56e7b1e900c9c71400d77507': {'truth': 'Central European Midsummer Time', 'predicted': '', 'question': \"What's a third name for double daylight saving time or Double Summer Time used in Europe?\"}, '57278cb9f1498d1400e8fbb2': {'truth': 'Supreme Federal Tribunal (Supremo Tribunal Federal)', 'predicted': 'Supreme Federal Tribunal', 'question': 'What is the highest court in Brazil?'}, '57278cb9f1498d1400e8fbb3': {'truth': 'cases that may be unconstitutional or final habeas corpus pleads for criminal cases', 'predicted': '', 'question': 'What are the two areas this court has supremacy over?'}, '57278cb9f1498d1400e8fbb5': {'truth': 'ministers of state, members of the high courts and the President and Vice-President of the Republic', 'predicted': 'members of congress, senators, ministers of state, members of the high courts and the President and Vice-President of the Republic', 'question': \"What other government officials are subject to judgments of Brazil's highest court?\"}, '57278cb9f1498d1400e8fbb6': {'truth': 'The Superior Labour Tribunal (Tribunal Superior do Trabalho)', 'predicted': 'Superior Labour Tribunal', 'question': \"What is Brazil's high court for labor law?\"}, '5a7fb0f18f0597001ac0007b': {'truth': '', 'predicted': 'general', 'question': 'The Superior Labour Tribunal oversees what type of elections?'}, '5a7fb0f18f0597001ac0007e': {'truth': '', 'predicted': 'Supreme Federal Tribunal', 'question': 'What is the Brazilian name for the Supreme Court?'}, '56d9ddf4dc89441400fdb86f': {'truth': 'anxiety', 'predicted': '', 'question': 'What is lessened when people are with their pet dogs?'}, '5731450de6313a140071cda0': {'truth': 'deny the opponent an advantage in the EMS', 'predicted': 'to deny the opponent an advantage in the EMS and ensure friendly, unimpeded access to the EM spectrum portion of the information environment', 'question': 'What is the purpose of electronic warfare?'}, '5731450de6313a140071cda2': {'truth': 'to keep airspaces friendly, and send critical information to anyone who needs it', 'predicted': 'to keep airspaces friendly', 'question': 'What does the USAF use Electronic warfare aircraft for?'}, '5731450de6313a140071cda4': {'truth': 'Airborne Command Post', 'predicted': '', 'question': 'What does the USAF use the E-4B aircraft for?'}, '5726f50add62a815002e9631': {'truth': 'in Western calendars', 'predicted': 'Western calendars', 'question': 'Where are Mesopotamian astronomical periods still used?'}, '5726f50add62a815002e9633': {'truth': 'A. Aaboe', 'predicted': '', 'question': 'Who believes that the Hellenistic world relies on Babylonian astronomy?'}, '571021d7a58dae1900cd68ce': {'truth': 'James W. Rodgers', 'predicted': 'Utah shot James W. Rodgers', 'question': 'Who was executed on March 30, 1960?'}, '571021d7a58dae1900cd68cf': {'truth': 'firing squad', 'predicted': '', 'question': 'What method of execution was used on James W. Rodgers?'}, '5ad3f93e604f3c001a3ffaa7': {'truth': '', 'predicted': 'Utah', 'question': 'In what state was Rodgers born?'}, '5ad3f93e604f3c001a3ffaa9': {'truth': '', 'predicted': 'Oklahoma', 'question': ' Which state freed James French?'}, '5733e5704776f41900661453': {'truth': 'number of other disciplines', 'predicted': '', 'question': 'What does the field of anthrozoology overlap with?'}, '5ad2f672604f3c001a3fda6b': {'truth': '', 'predicted': 'Anthrozoology', 'question': 'What is the study of animals?'}, '5ad2f672604f3c001a3fda6d': {'truth': '', 'predicted': 'anthropology, ethology, medicine, psychology, veterinary medicine and zoology', 'question': 'What fields developed from Anthrozoology?'}, '57302c9aa23a5019007fcefd': {'truth': 'imaginary plane', 'predicted': '', 'question': 'What are electrical fields projected on to?'}, '57302c9aa23a5019007fcefe': {'truth': 'radio wave', 'predicted': '', 'question': 'What is the imagenary plane perpindicular to?'}, '5a839cf3e60761001a2eb82a': {'truth': '', 'predicted': 'sheik commander', 'question': 'What was the title for Saleh Nabhan Saleh Ali?'}, '5a839cf3e60761001a2eb82b': {'truth': '', 'predicted': 'Nabhan', 'question': 'Who was wanted in connection with the 2002 Kenyan attacks?'}, '5a839cf3e60761001a2eb82c': {'truth': '', 'predicted': 'Baarawe', 'question': 'Witnesses from what village claim they saw French-flagged warships?'}, '5a85b7e3b4e223001a8e71b9': {'truth': '', 'predicted': 'two', 'question': 'How many US Special Forces were killed on 14 September 2009?'}, '5a85b7e3b4e223001a8e71bb': {'truth': '', 'predicted': 'Baarawe', 'question': 'What was the name of the village attacked in Mombasa?'}, '57266a2c5951b619008f7206': {'truth': 'salt', 'predicted': 'the saltpetre made', 'question': 'One of the governor of the company said that he would rather have saltpetre then____ in its raw form?'}, '5a8464dd7cf838001a46a7da': {'truth': '', 'predicted': '1673', 'question': 'What year did Banks negotiate between the princess and the North India company for 600 tons of salt?'}, '570b00026b8089140040f69e': {'truth': 'telemedicine', 'predicted': 'telemedicine, distance education, and business meetings', 'question': 'What is one area where teleconferencing could not be used?'}, '570b00026b8089140040f6a0': {'truth': 'the 1950s', 'predicted': '1950s', 'question': 'When was the first slow-scan video systems researched?'}, '5a1f22043de3f40018b2650e': {'truth': '', 'predicted': 'AT&T Corporation', 'question': 'Who developed the first system to transmit distance education?'}, '5a1f22043de3f40018b2650f': {'truth': '', 'predicted': 'telemedicine, distance education', 'question': 'In what area could business meetings not be used?'}, '5a1f22043de3f40018b26510': {'truth': '', 'predicted': 'poor picture quality and the lack of efficient video compression techniques', 'question': 'Why did using business meetings to transmit slow-scan video fail?'}, '5a1f22043de3f40018b26512': {'truth': '', 'predicted': 'a few hundred', 'question': 'How many telephony networks were there in the world?'}, '572a8012be1ee31400cb8045': {'truth': 'Baghdad', 'predicted': '', 'question': 'Where did Shuhda attend school?'}, '5ace804f32bba1001ae4a879': {'truth': '', 'predicted': 'music, dancing and poetry', 'question': 'What disciplines were women trained in during the second century?'}, '5ace804f32bba1001ae4a87b': {'truth': '', 'predicted': 'Baghdad', 'question': ' Where did Shuhda work?'}, '5ace804f32bba1001ae4a87c': {'truth': '', 'predicted': 'around 750', 'question': ' When was formal education for Islamic women encouraged?'}, '5ace804f32bba1001ae4a87d': {'truth': '', 'predicted': 'Abbasid', 'question': 'During which Caliphate did Islamic women stop going to formal school?'}, '572b883cf75d5e190021fe32': {'truth': 'teaching, research, and social services activities,', 'predicted': 'teaching, research, and social services activities', 'question': 'What does University education include?'}, '572b883cf75d5e190021fe34': {'truth': 'independent', 'predicted': '', 'question': 'What type of University would Yale fall under?'}, '5731a21de17f3d1400422297': {'truth': 'an alternative to traditional universities', 'predicted': '', 'question': ' Governments created universities to serve as what?'}, '5a77ae98b73996001af5a4fa': {'truth': '', 'predicted': 'free', 'question': 'What type of education did scientists benefactors hope to provide to the public?'}, '5a77ae98b73996001af5a4fd': {'truth': '', 'predicted': 'the resources available through private benefactors', 'question': 'What could scientists not compete with?'}, '5ad37275604f3c001a3fe2b5': {'truth': '', 'predicted': 'the Primary Reserve, Supplementary Reserve, Cadet Organizations Administration and Training Service, and the Canadian Rangers', 'question': 'What four components make up the Force Reserve?'}, '5ad37275604f3c001a3fe2b6': {'truth': '', 'predicted': 'Department of National Defence', 'question': 'What is the Canadian Armed Forces associated with under the National Defence Act?'}, '5709b165ed30961900e84426': {'truth': 'deceased', 'predicted': '', 'question': 'What condition does a person have to meet to  be allowed by law on a coin?'}, '5709b165ed30961900e84428': {'truth': '20th century', 'predicted': 'after the early 20th century', 'question': 'When did modern day currency start getting the faces that they have?'}, '5709b165ed30961900e84429': {'truth': 'composite Native Americans', 'predicted': 'Native Americans', 'question': 'Other than Greek and Roman mythology, who else was featured on the \"heads\" side of past coins?'}, '5709b165ed30961900e8442a': {'truth': 'Dollar', 'predicted': '', 'question': 'What was the last coin to be converted to the modern day style of having historic Americans on the face?'}, '5a8cd753fd22b3001a8d8f31': {'truth': '', 'predicted': 'Native Americans', 'question': 'Other than Greek and Roman mythology, who else was featured on the \"heads\" of past monarchs?'}, '56e14538e3433e1400422d21': {'truth': 'General Council of the Pyrénées-Orientales', 'predicted': 'the General Council of the Pyrénées-Orientales', 'question': 'Who recognized Catalan as a departmental language?'}, '56dfdbee7aa994140058e1c7': {'truth': 'March 2006', 'predicted': '2006', 'question': 'In what month and year was smoking banned in public places in Scotland?'}, '56dfdbee7aa994140058e1c8': {'truth': 'April 2007', 'predicted': '', 'question': 'When did Wales outlaw smoking in public?'}, '56dfdbee7aa994140058e1cb': {'truth': 'Wetherspoon', 'predicted': 'The Wetherspoon', 'question': 'What chain of pubs reported favorable profits in June 2009?'}, '572827843acd2414000df5ae': {'truth': 'hair-forming', 'predicted': '', 'question': 'What do chetoblast cells do?'}, '5728b201ff5b5019007da4a3': {'truth': '40 or 50 members', 'predicted': '40 or 50', 'question': 'How many people compose the Japanese mandolin orchestras?'}, '5728b201ff5b5019007da4a4': {'truth': 'include woodwind, percussion, and brass sections.', 'predicted': '', 'question': 'What other instruments do the Japanese madnolin orchestras play?'}, '5728b201ff5b5019007da4a5': {'truth': \"20th Century mandolin music from Europe and one of the most complete collections of mandolin magazines from mandolin's golden age\", 'predicted': '20th Century mandolin music', 'question': 'Japan hold and extensive collection of what? '}, '5728b201ff5b5019007da4a6': {'truth': 'Morishige Takei.', 'predicted': 'Morishige Takei', 'question': 'Who purhcased one of the collections of mandolin magazines? '}, '5728dbd84b864d1900164faa': {'truth': 'Commune of Paris', 'predicted': 'the Commune of Paris', 'question': 'What was the most populated city in the EU in 2012?'}, '5ace3ada32bba1001ae49f8c': {'truth': '', 'predicted': 'one', 'question': \"How many women's tennis NCAA Division III championships have they won?\"}, '5ace3ada32bba1001ae49f8d': {'truth': '', 'predicted': '19', 'question': \"How many men's cross country NCAA Division III championships have they won?\"}, '5ace3ada32bba1001ae49f8f': {'truth': '', 'predicted': 'ten', 'question': \"how many men's volleyball NCAA Division III championships have they won?\"}, '56e7ac6c37bdd419002c430f': {'truth': '85 routes', 'predicted': '85', 'question': \"How many routes does Nanjing's airport run?\"}, '56e7ac6c37bdd419002c4310': {'truth': '15,011,792 passengers', 'predicted': '15,011,792', 'question': 'How many passengers did the airport service in 2013?'}, '56e7ac6c37bdd419002c4312': {'truth': 'Nanjing Dajiaochang Airport', 'predicted': '', 'question': 'What airport was the primary airport before Lukou?'}, '57325124e17f3d140042285d': {'truth': 'between New England and New York', 'predicted': 'The development of the Bronx is directly connected to its strategic location between New England and New York (Manhattan)', 'question': \"What strategic advantage did the Bronx's location have?\"}, '5727a1eeff5b5019007d9160': {'truth': 'Guaranda (Bolivar province) and Ambato (Tungurahua province', 'predicted': 'Guaranda', 'question': 'Where are the most famed Carnival festivities?'}, '5727a1eeff5b5019007d9163': {'truth': 'afro-Ecuadorian', 'predicted': '', 'question': 'What is there a large population of in the Chota Valley?'}, '5727a1eeff5b5019007d9164': {'truth': 'bomba del chota', 'predicted': 'bomba del chota music', 'question': 'What music is the Carnival celebrated with in the Chota Valley?'}, '57276683dd62a815002e9c35': {'truth': 'they are exposed to this heat', 'predicted': 'When the boys are at work, they are exposed to this heat. This could cause eye trouble, lung ailments, heat exhaustion, cut, and burns. Since workers were paid by the piece, they had to work productively for hours without a break. Since furnaces had to be constantly burning', 'question': 'Under glass making conditions were the children exposed to any heating elements?'}, '56e032247aa994140058e34b': {'truth': 'Ninth Art', 'predicted': '', 'question': 'Comics for adults began to be called what?'}, '56e032247aa994140058e34d': {'truth': 'Adventures of Asterix', 'predicted': 'The Adventures of Asterix', 'question': 'What became a best-seller comic in the French language?'}, '57321963e99e3014001e650e': {'truth': 'mythical chullumpi bird', 'predicted': 'chullumpi bird', 'question': 'What is said to mark the existence of a portal between such worlds, and to transform itself into a llama?'}, '572e9c96cb0c0d14000f135c': {'truth': 'After the Mexican War of Independence from Spain', 'predicted': '', 'question': 'When did other states become part of Mexico?'}, '572e9c96cb0c0d14000f135d': {'truth': 'California, Nevada, Arizona, Utah, western Colorado and southwestern Wyoming became part of the Mexican territory of Alta California', 'predicted': '', 'question': 'What states made up of Alta California'}, '572e9c96cb0c0d14000f135e': {'truth': 'most of New Mexico, western Texas, southern Colorado, southwestern Kansas, and Oklahoma panhandle were part of the territory of Santa Fe de Nuevo México', 'predicted': 'New Mexico, western Texas, southern Colorado, southwestern Kansas, and Oklahoma panhandle', 'question': 'What states were part of Santa Fe de Nuevo'}, '572e9c96cb0c0d14000f135f': {'truth': 'The geographical isolation and unique political history of this territory', 'predicted': '', 'question': 'Why is  there still Bilingual  spoken in these states?'}, '5730a9732461fd1900a9cf63': {'truth': 'Ubaid', 'predicted': 'The Ubaid period', 'question': 'Fine quality painted pottery is a distinctive style of what period in Sumerian history?'}, '5730a9732461fd1900a9cf66': {'truth': 'irrigation', 'predicted': 'irrigation agriculture', 'question': 'What type of agriculture did the farmers settling at Eridu bring with them?'}, '5a650f65c2b11c001a425bc7': {'truth': '', 'predicted': 'Hadji Muhammed culture', 'question': 'What culture adopted and improved on irrigation agriculture?'}, '5a650f65c2b11c001a425bc8': {'truth': '', 'predicted': 'Uruk', 'question': 'What city became the new religious center after Eridu?'}, '57310f4ae6313a140071cbca': {'truth': 'United Kingdom, France, Italy, and Japan', 'predicted': 'the United Kingdom, France, Italy, and Japan', 'question': 'Who were the four permanent members of the League of Nations Council?'}, '5a14a7c7a54d4200185292fd': {'truth': '', 'predicted': '19 March 1920', 'question': 'When did the League of Nations council vote against the Treaty of Versailles ratification?'}, '5a14a7c7a54d4200185292ff': {'truth': '', 'predicted': 'the United Kingdom, France, Italy, and Japan', 'question': 'What four countries were permanent members of the Senate?'}, '5a14a7c7a54d420018529300': {'truth': '', 'predicted': 'US Senate', 'question': 'What council voted against ratification of the Treaty of Versailles?'}, '5732ae23cc179a14009dabfa': {'truth': 'the Isthmus of Panama', 'predicted': 'Isthmus of Panama', 'question': 'What is the link between North and South America called?'}, '5732ae23cc179a14009dabfc': {'truth': 'marsupial faunas', 'predicted': 'marsupial', 'question': 'The Pliocene saw the end of what fauna in South America?'}, '5732ae23cc179a14009dabfd': {'truth': 'Africa', 'predicted': '', 'question': 'The Mediterranean was created by the collision of Europe and what?'}, '5732ae23cc179a14009dabfe': {'truth': 'the Quaternary Period', 'predicted': '', 'question': 'What period came after the Pliocene?'}, '5a4ebec8af0d07001ae8cc27': {'truth': '', 'predicted': 'Isthmus of Panama', 'question': 'What formed to linked North and South America in the Quaternary Period?'}, '5a4ebec8af0d07001ae8cc29': {'truth': '', 'predicted': 'every 40,000–100,000 years', 'question': 'How often does the cycle of rising sea levels repeat?'}, '572bc28a111d821400f38f76': {'truth': 'poor countries should grow faster than rich countries', 'predicted': '', 'question': 'What is the theory behind Empirical analyses?'}, '5acd857607355d001abf4562': {'truth': '', 'predicted': 'they can adopt cutting edge technologies already tried and tested by rich countries', 'question': \"Why don't poor countries go quickly?\"}, '57337cc94776f41900660ba7': {'truth': 'Claremont and a select number of liberal graduate-level theology and philosophy programs', 'predicted': 'Claremont', 'question': \"Where are Whitehead's works primarily studied in English-speaking countries?\"}, '57337cc94776f41900660ba8': {'truth': 'through the work of his students and admirers rather', 'predicted': '', 'question': 'Where has interest outside of those areas mainly come from?'}, '57337cc94776f41900660ba9': {'truth': 'Bertrand Russell, and he also taught and supervised the dissertation of Willard Van Orman Quine', 'predicted': 'Willard Van Orman Quine', 'question': \"Who are two of Whitehead's students that have gone on to become renowned in the field of analytic philosophy?\"}, '5723fc250dadf01500fa1fe2': {'truth': 'London', 'predicted': 'Prince Frederick William of Prussia in London', 'question': \"Where did Victoria's oldest daughter get married?\"}, '572501720ba9f01400d97c29': {'truth': 'last German Kaiser', 'predicted': 'Kaiser', 'question': 'What position did Wilhelm later hold in Germany? '}, '57266b70708984140094c567': {'truth': \"Eleven days after Orsini's assassination attempt\", 'predicted': \"Eleven days after Orsini's assassination attempt in France, Victoria's eldest daughter married Prince Frederick William of Prussia in London. They had been betrothed since September 1855\", 'question': \"When was Victoria's oldest daughter married?\"}, '57266b70708984140094c569': {'truth': 'liberalising influence in the enlarging Prussian state', 'predicted': 'would be a liberalising influence in the enlarging Prussian state', 'question': 'What did Queen Victoria hope for the marriage between her daughter and Prince Frederick William?'}, '57266b70708984140094c56b': {'truth': 'the last German Kaiser', 'predicted': 'Wilhelm, who would become the last German Kaiser', 'question': 'What future awaited the first grandson of Queen Victoria?'}, '5ad17607645df0001a2d1cee': {'truth': '', 'predicted': 'Prince Frederick William of Prussia', 'question': 'Who did Victorias youngest daughter marry?'}, '5ad17607645df0001a2d1cef': {'truth': '', 'predicted': 'Prince Frederick William of Prussia in London', 'question': \"Where did Victoria's youngest daughter get married?\"}, '5ad17607645df0001a2d1cf0': {'truth': '', 'predicted': '14', 'question': 'How old was Princess Victoria when she agreed to divorce the Prince?'}, '56e7b1d437bdd419002c4378': {'truth': 'May', 'predicted': 'May to February', 'question': 'In what month did the AFL season originally begin?'}, '56e7b1d437bdd419002c4379': {'truth': 'February', 'predicted': 'May to February', 'question': 'After the TV deal, when was the start of the AFL season moved to?'}, '572fd6a6b2c2fd14005684f3': {'truth': 'One Laptopschool Per child', 'predicted': '', 'question': 'What does OLPC stand for?'}, '572fd6a6b2c2fd14005684f4': {'truth': 'American University of Armenia and the QSI International School of Yerevan', 'predicted': 'American University of Armenia', 'question': 'What are the names of some of the other higher education organizations in Armenia?'}, '573271ece17f3d1400422982': {'truth': 'death of Stalin', 'predicted': '', 'question': 'What event led to decreased Russian support for China?'}, '5706b5fa0eeca41400aa0d71': {'truth': 'One Voice Records', 'predicted': '', 'question': 'what label did robert ozn start in 1989?'}, '5ad2932ad7d075001a429adc': {'truth': '', 'predicted': \"Los Angeles saw a huge explosion of underground raves and DJs, notably DJs Marques Wyatt and Billy Long, who spun at Jewel's Catch One\", 'question': 'Where did DJs Marques, Wyatt, and Mike Wilson become successful?'}, '5ad2932ad7d075001a429add': {'truth': '', 'predicted': \"Jewel's Catch One\", 'question': 'What is the oldest dance club in San Deigo?'}, '5ad2932ad7d075001a429ae0': {'truth': '', 'predicted': 'Dada Nada', 'question': \"David Morales was the moniker for what artist's solo act?\"}, '56f96a3c9e9bad19000a08ef': {'truth': 'the Amended Compact of Free Association', 'predicted': 'Amended Compact of Free Association', 'question': 'What document defines how much money is transferred from the United States to the Marshall Islands?'}, '56f96a3c9e9bad19000a08f0': {'truth': 'a trust fund', 'predicted': '', 'question': 'What will be established in 2023?'}, '572966e11d046914007793a5': {'truth': 'A unique green', 'predicted': '', 'question': 'What is produced on a computer display when light from the green primary is mixed with some light from the blue primary?'}, '572966e11d046914007793a6': {'truth': '~550 nm', 'predicted': '550 nm', 'question': 'At what wavelength is green on computer displays?'}, '5a74beb142eae6001a389a58': {'truth': '', 'predicted': '550 nm', 'question': 'What is the wavelength of red?'}, '5a74beb142eae6001a389a59': {'truth': '', 'predicted': 'by mixing light from the green primary with some light from the blue primary', 'question': 'How is a unique blue made?'}, '5a74beb142eae6001a389a5b': {'truth': '', 'predicted': '550 nm', 'question': \"What is unique green's wavelength?\"}, '56e0c956231d4119001ac391': {'truth': 'the World Wide Web', 'predicted': 'World Wide Web', 'question': 'The primary function of a browser is to use what?'}, '56e0c956231d4119001ac393': {'truth': 'file systems', 'predicted': 'private networks or files in file systems', 'question': 'A browser can also access files where?'}, '5a4d2f747a6c4c001a2bbc16': {'truth': '', 'predicted': 'web servers in private networks or files', 'question': 'What can information access when used?'}, '5a4d2f747a6c4c001a2bbc18': {'truth': '', 'predicted': 'World Wide Web', 'question': 'Where are browsers located?'}, '5a4d2f747a6c4c001a2bbc19': {'truth': '', 'predicted': 'World Wide Web', 'question': 'Where is a browser so you are able to find it later?'}, '57294fa6af94a219006aa286': {'truth': 'refugees, who were captured in 2001 in Pakistan', 'predicted': 'refugees', 'question': 'What were the Uyghurs claiming to be?'}, '57294fa6af94a219006aa287': {'truth': \"training to assist the Taliban's military.\", 'predicted': \"training to assist the Taliban's military\", 'question': 'What were the Ugyhurs accused of?'}, '57294fa6af94a219006aa288': {'truth': 'the US government determined that China was likely to violate their human rights.', 'predicted': '', 'question': \"Why weren't the Ugyhurs deported back to China?\"}, '5ad417d2604f3c001a4003b9': {'truth': '', 'predicted': '2001', 'question': 'What year were the Uyghurs captured fleeing America?'}, '5726b8addd62a815002e8e28': {'truth': '29 September', 'predicted': '29 September 1963', 'question': 'On what date was Vatican II re convened?'}, '5729145a3f37b31900477fff': {'truth': 'delimitation of the federal territory', 'predicted': 'A new delimitation of the federal territory', 'question': 'What keeps being debated in Germany?'}, '5729145a3f37b31900478000': {'truth': 'pay for it from own source revenues', 'predicted': '', 'question': 'What does Gunlick remark that the German System of  dual federalism requires strong Länder to have other than the capacity to implement legislation?'}, '5a47387c5fd40d001a27dd81': {'truth': '', 'predicted': 'territorial reform', 'question': 'What has failed so far in America?'}, '5a5140ccce860b001aa3fc87': {'truth': '', 'predicted': 'federal territory', 'question': 'What is a settled question in Germany?'}, '56dfe86b7aa994140058e251': {'truth': 'John Manners, Marquess of Granby', 'predicted': 'John Manners', 'question': 'After whom was the Marquis of Granby pub named?'}, '56dfe86b7aa994140058e254': {'truth': '18th', 'predicted': '', 'question': 'In what century did the 3rd Duke of Rutland live?'}, '572fa6bca23a5019007fc834': {'truth': 'Russian Army', 'predicted': 'Imperial Russian Army', 'question': 'Which army had a group of Armenian volunteers fighting for them?'}, '57290895af94a219006a9fae': {'truth': 'Agence', 'predicted': 'Agence France-Presse', 'question': \"What is France's oldest operating news agency?\"}, '57290895af94a219006a9faf': {'truth': '1835', 'predicted': '', 'question': 'When did Agence open in Paris?'}, '56e162a3e3433e1400422e44': {'truth': 'the FleetCenter', 'predicted': 'FleetCenter', 'question': 'What is the TD Gardens former name?'}, '5726b8be708984140094cf15': {'truth': 'Anglican', 'predicted': 'Anglican Church', 'question': 'Which church did Burke most defend?'}, '5726b8be708984140094cf16': {'truth': 'religion', 'predicted': '', 'question': 'What did Burke think was the foundation of society?'}, '5726b8be708984140094cf19': {'truth': 'political arrangements', 'predicted': '', 'question': 'Burke thought religion was beneficial to what besides souls?'}, '5ad11411645df0001a2d0c94': {'truth': '', 'predicted': 'civil society', 'question': 'What did Burke believe was the foundation of religion?'}, '5731bcdc0fdd8d15006c64c4': {'truth': 'to appease the Anti-Federalists', 'predicted': '', 'question': 'Why was a more general \"religion\" used in the language of the First Amendment?'}, '5731bcdc0fdd8d15006c64c5': {'truth': 'because of the experience under the British crown', 'predicted': 'the experience under the British crown', 'question': 'Why was the word \"national\" a cause for alarm to both Federalists and Anti-Federalists?'}, '5731bcdc0fdd8d15006c64c6': {'truth': 'Elbridge Gerry', 'predicted': 'Rep. Elbridge Gerry of Massachusetts', 'question': \"Who took issue with Madison's language during the debate over the establishment clause?\"}, '5ad1423f645df0001a2d1422': {'truth': '', 'predicted': 'because of the experience under the British crown', 'question': 'Why was the word \"national\" a cause for alarm to neither Federalists and Anti-Federalists?'}, '5728116c3acd2414000df3a5': {'truth': 'bound by the treaties', 'predicted': '', 'question': 'Why do EU institutions and national governments have to respect the independence of the ECB?'}, '5728116c3acd2414000df3a6': {'truth': 'bound to publish reports on its activities and has to address its annual report to the European Parliament', 'predicted': 'publish reports', 'question': \"How is the ECB held accountable for it's actions?\"}, '5726134f89a1e219009ac1fc': {'truth': 'better', 'predicted': 'much better', 'question': 'Is the environmental performance of CFLs better or worse than that of incandescents?'}, '5730a045069b5314008321cf': {'truth': 'Greek surnames are most commonly patronymics', 'predicted': '', 'question': 'What names are used that are typically from  a father and have usually added a suffix or prefix ?'}, '5730a045069b5314008321d0': {'truth': 'Greek male surnames end in -s, which is the common ending', 'predicted': '', 'question': 'What letter of the alphabet do most of the last names the men of Greecs end with ?'}, '5730a045069b5314008321d1': {'truth': 'many have Latin, Turkish and Italian origin.', 'predicted': '', 'question': 'What other beginnings of origination do some of the last names of the Greeks share ?'}, '5730a045069b5314008321d2': {'truth': 'some end in -ou, indicating the genitive case of this proper noun for patronymic reasons.', 'predicted': '', 'question': 'What does it mean to have the letters OU added to the ending of a males last name ?'}, '56e6df336fe0821900b8ec13': {'truth': 'soft rock', 'predicted': 'adult-oriented soft rock', 'question': 'What genre of music is played by The Eagles?'}, '5710f431b654c5140001fa3f': {'truth': 'Thomas Jefferson', 'predicted': 'Jefferson', 'question': 'John Locke, Francis Bacon, and Isaac Newton where considered the the greatest men who ever lived by which American colonist?'}, '5710f431b654c5140001fa40': {'truth': 'the United States Constitution', 'predicted': 'United States Constitution', 'question': 'Religious tolerance and the importance of individual conscience was particularly influential in the drafting of which American document?'}, '57294c7f3f37b31900478215': {'truth': '17 percent', 'predicted': '17', 'question': 'In the Phillipines, geothermal represented what percentage of the total power mix at the end of 2008?'}, '570a5d534103511400d59674': {'truth': 'Imperial College Healthcare NHS Trust', 'predicted': '', 'question': 'What was formed on the 1st October 2007?'}, '570a5d534103511400d59676': {'truth': 'an academic health science centre', 'predicted': '', 'question': 'What is it considered to be?'}, '5a48747484b8a4001a7e7888': {'truth': '', 'predicted': 'Hammersmith Hospitals NHS Trust', 'question': 'what  hospitals were formed from Imperial College Healthcare?'}, '56defd9bc65bf219000b3e9b': {'truth': 'The Canadian Special Operations Forces Command', 'predicted': 'Canadian Special Operations Forces Command', 'question': 'what does CANSOFCOM stand for?'}, '56defd9bc65bf219000b3e9c': {'truth': 'generating special operations forces', 'predicted': 'generating special operations forces (SOF) elements to support CJOC', 'question': 'What is the CANSOFCOM focussed on?'}, '5ad3ec3b604f3c001a3ff759': {'truth': '', 'predicted': 'Canadian Special Operations Forces Command', 'question': 'what does CANSOFCORN stand for?'}, '570b28ab6b8089140040f7a4': {'truth': 'system of weeks', 'predicted': '', 'question': 'What other system of calculations are inherent in the Gregorian calendar?'}, '570b28ab6b8089140040f7a5': {'truth': 'irregularities', 'predicted': 'irregularities in the Gregorian system', 'question': 'Why is calculating the days of the Gregorian calendar not simple?'}, '57301c4fb2c2fd140056888f': {'truth': 'a military coup led by Master Sergeant Samuel Doe of the Krahn ethnic group', 'predicted': 'Master Sergeant Samuel Doe', 'question': 'Who was responsible for the death of William R. Tolbert?'}, '57301c4fb2c2fd1400568890': {'truth': 'April 12, 1980', 'predicted': '', 'question': 'On what date was William R. Tolbert killed?'}, '5a62a976f8d794001af1c198': {'truth': '', 'predicted': 'Master Sergeant Samuel Doe', 'question': 'Who led a military coup that killed the American ambassador?'}, '5a62a976f8d794001af1c19a': {'truth': '', 'predicted': 'Master Sergeant Samuel Doe', 'question': \"Who tried to stop President Tolbert's assasination?\"}, '5a62a976f8d794001af1c19b': {'truth': '', 'predicted': 'corruption and political repression', 'question': 'What did the United States criticize the PRC for?'}, '5a62a976f8d794001af1c19c': {'truth': '', 'predicted': 'Doe', 'question': 'Who provided financial backing to the United States?'}, '56bfb8dca10cfb1400551279': {'truth': 'female-empowerment', 'predicted': 'personally driven and female-empowerment', 'question': \"What theme was Beyonce's early music?\"}, '56bfb8dca10cfb140055127d': {'truth': 'melodies', 'predicted': 'comes up with melodies and ideas', 'question': 'What part of production does she do?'}, '56d4dd502ccc5a1400d832ae': {'truth': 'Women', 'predicted': 'female-empowerment themed compositions like \"Independent Women\" and \"Survivor\", but after the start of her relationship with Jay Z', 'question': \"Beyoncé's early recordings empowered who?\"}, '56d4dd502ccc5a1400d832b0': {'truth': 'co-producing', 'predicted': 'producing', 'question': 'In addition to co-writing credits, Beyoncé also got what credits for most of her albums?'}, '572a982b34ae481900deaba6': {'truth': 'the Silver Star Medal, Bronze Star Medal, and three Purple Heart Medals', 'predicted': 'Silver Star Medal, Bronze Star Medal, and three Purple Heart Medals', 'question': 'What medals did Kerry win?'}, '572956496aef051400154d14': {'truth': 'Brigadier Harvey', 'predicted': '', 'question': 'Who was the youngest Royal Marine Brigadier?'}, '5ad423a1604f3c001a40084b': {'truth': '', 'predicted': 'highest-ranking soldier', 'question': 'Who was Major-General Charles Glyn Gilbert Anglim?'}, '5ad423a1604f3c001a40084c': {'truth': '', 'predicted': 'developing the Bermuda Regiment', 'question': 'What was Major-General Charles Glyn Gilbert Anglim instrumental in?'}, '572b7eb8be1ee31400cb83ee': {'truth': 'intense gamma radiation', 'predicted': 'gamma radiation', 'question': 'What does 65Zn produce?'}, '572b7eb8be1ee31400cb83ef': {'truth': 'anti-corrosion agent', 'predicted': '', 'question': 'Why is zinc oxide used in nuclear reactors?'}, '5acfc84c77cf76001a685f43': {'truth': '', 'predicted': \"radioactivity of the weapon's fallout\", 'question': 'What does 65Zn reduce?'}, '5a6246a3f8d794001af1bf2c': {'truth': '', 'predicted': 'headhunting', 'question': 'What ritual did Robert Lewis Stevenson say that the Samoans engaged in?'}, '5a6246a3f8d794001af1bf2d': {'truth': '', 'predicted': '1894', 'question': 'What year did John Williams die?'}, '570e38eb0dc6ce1900204ea7': {'truth': 'Italy', 'predicted': '', 'question': 'In what county was glass with uranium oxide content found?'}, '570e38eb0dc6ce1900204ea9': {'truth': 'the University of Oxford', 'predicted': 'University of Oxford', 'question': 'What institution did R.T. Gunther belong to?'}, '5ad1167a645df0001a2d0d14': {'truth': '', 'predicted': '79 CE', 'question': 'What was the latest year in recorded history that uranium oxide was used?'}, '5ad1167a645df0001a2d0d18': {'truth': '', 'predicted': 'University of Oxford', 'question': 'What institution did T.T. Gunther belong to?'}, '570b26c7ec8fbc190045b888': {'truth': 'the Xbox Guide button on the gamepad', 'predicted': 'Xbox Guide button', 'question': 'The simple dashboard could be accessed by pressing what controller button?'}, '570b26c7ec8fbc190045b889': {'truth': 'five', 'predicted': '', 'question': 'How many tabs were on the 360 dashboard interface?'}, '5a70c2358abb0b001a676177': {'truth': '', 'predicted': 'four', 'question': 'How many blades did the simplified version on the guide have?'}, '56dfa2414a1a83140091ebde': {'truth': 'sexual division', 'predicted': '', 'question': 'How is labor often divided in these groups?'}, '56dfa2414a1a83140091ebe2': {'truth': '17%', 'predicted': '', 'question': 'What is the success rate for male Aeta hunters?'}, '5acd54e907355d001abf3d74': {'truth': '', 'predicted': 'the same kind of quarry', 'question': 'In a majority of cases, women hunt what?'}, '5acd54e907355d001abf3d76': {'truth': '', 'predicted': 'dogs', 'question': \"The Ju'/hoansi women typically hunt in groups and with what?\"}, '5acd54e907355d001abf3d77': {'truth': '', 'predicted': 'quarry', 'question': \"Among the Ju'/hoansi women of China, women help men track down what?\"}, '5acd54e907355d001abf3d78': {'truth': '', 'predicted': 'lizards', 'question': 'Men in the Austrailian Martu primarily hunt small animals like what?'}, '57264dcd708984140094c1d8': {'truth': 'central part of the Aegean Sea', 'predicted': 'the central part of the Aegean Sea', 'question': 'The Cyclade islands are located where?'}, '57264dcd708984140094c1da': {'truth': 'between Crete and Turkey', 'predicted': 'the southeast between Crete and Turkey', 'question': 'The Dodecanese islands are located where?'}, '571ae53e9499d21900609b97': {'truth': 'murder', 'predicted': '', 'question': 'What happened to Bishop George?'}, '571ae53e9499d21900609b98': {'truth': 'severe penance', 'predicted': '', 'question': 'What happened to Bishop leaders who did not agree with the doctrine?'}, '56dfc2b77aa994140058e157': {'truth': 'domesticated food', 'predicted': 'domesticated food sources', 'question': 'What do modern hunter-gatherers depend on at least somewhat?'}, '5acd607f07355d001abf3fbc': {'truth': '', 'predicted': 'the landscape', 'question': 'Many hunter-gatherers unconsciously manipulate what?'}, '5acd607f07355d001abf3fbe': {'truth': '', 'predicted': 'hunter-gatherers consciously manipulate the landscape through cutting or burning undesirable plants while encouraging desirable ones, some even going to the extent of slash-and-burn to create habitat for game animals', 'question': 'Which group uses a slash-and-burn technique to create habitats for humans?'}, '5acd607f07355d001abf3fbf': {'truth': '', 'predicted': 'hunter-gatherers', 'question': 'Which group burns desirable plants while encouraging undesirable ones?'}, '572fffb8a23a5019007fcc29': {'truth': 'Second Triumvirate of Octavian', 'predicted': 'the Second Triumvirate of Octavian, Lepidus and Mark Antony failed', 'question': 'What failure caused the the flares of civil war to spark up again?'}, '573367eed058e614000b5a5b': {'truth': 'Colombier Bay', 'predicted': '', 'question': 'What is the name of the deepest bay at St Barts?'}, '573367eed058e614000b5a5c': {'truth': 'small', 'predicted': 'small vessels', 'question': 'Grande Saline Bay provides docking for what kind of boats?'}, '573367eed058e614000b5a5d': {'truth': 'visible coral reef', 'predicted': 'coral reef', 'question': 'The North and East sides of St. Barts are fringed by what?'}, '5a39963a2f14dd001ac72435': {'truth': '', 'predicted': 'shallow', 'question': 'How deep do most of the coral reefs lie?'}, '5a39963a2f14dd001ac72436': {'truth': '', 'predicted': 'northwest', 'question': 'In what direction does St. Jean Bay lie from Colombier Bay?'}, '5a39963a2f14dd001ac72437': {'truth': '', 'predicted': 'northwest', 'question': 'In what direction does Grande Saline Bay lie from St. Jean Bay?'}, '56e75e1200c9c71400d77019': {'truth': 'Laidlines', 'predicted': '', 'question': 'What are small regular lines left on paper when handmade in a mould?'}, '56e75e1200c9c71400d7701b': {'truth': 'chainlines', 'predicted': '', 'question': 'What runs perpendicular to the laidlines?'}, '56e75e1200c9c71400d7701c': {'truth': 'Laidlines', 'predicted': '', 'question': 'Which is lines are commonly higher in density, laidlines or chainlines?'}, '5ad504cb5b96ef001a10a9d3': {'truth': '', 'predicted': 'Wove paper', 'question': 'What type of paper exhibits laidlines?'}, '5731c1260fdd8d15006c6503': {'truth': 'mechanism of action', 'predicted': 'based on their mechanism of action, chemical structure, or spectrum of activity', 'question': 'Besides sprectrum of activity and chemical structure, how can antibacterial antibiotics classified?'}, '5733b4cf4776f419006610c9': {'truth': 'mechanism of action, chemical structure, or spectrum of activity', 'predicted': '', 'question': 'What three ways are antibiotics classified?'}, '5733b4cf4776f419006610cd': {'truth': '(macrolides, lincosamides and tetracyclines', 'predicted': '', 'question': 'What 3 types go after protein synthesis?'}, '5a65cfcfc2b11c001a425d57': {'truth': '', 'predicted': 'bacteriostatic', 'question': 'Besides spectrum of activity and chemical structure, how can protein synthesis be classified?'}, '56e79ed037bdd419002c426a': {'truth': '1987', 'predicted': '1987–2006 and post-2006', 'question': 'If located in Canada somewhere where DST is observed, a system running Vista might mishandle time stamps that are older than what year?'}, '56e79ed037bdd419002c426b': {'truth': '2006', 'predicted': 'post-2006', 'question': 'On a system running Windows older than Vista, locations in Canada observing DST would only reliably support time stamps from after what year?'}, '5725f1dc271a42140099d354': {'truth': 'the Royal Mews', 'predicted': 'Royal Mews', 'question': 'Where is the Gold State Coach housed?'}, '5725f1dc271a42140099d357': {'truth': 'horses', 'predicted': 'coach horses', 'question': 'The Royal Mews houses which type of animal?'}, '57261ed8ec44d21400f3d922': {'truth': 'the Royal Mews', 'predicted': 'Royal Mews', 'question': 'Where is the Gold State Coach kept?'}, '57261ed8ec44d21400f3d925': {'truth': 'horses', 'predicted': 'coach horses', 'question': 'What animals are kept in the mews?'}, '5a7a4c3d17ab25001a8a0493': {'truth': '', 'predicted': 'Golden Jubilee of Elizabeth II', 'question': 'When was the Gold State Coach last used?'}, '572d3b0d8351f81400e9d37d': {'truth': 'music that emerged from the cultural milieu of punk rock in the late 1970s', 'predicted': 'music that emerged from the cultural milieu of punk rock', 'question': 'What is post-punk?'}, '572d3b0d8351f81400e9d37e': {'truth': 'new wave music', 'predicted': '', 'question': 'What else was music incorrectly catagorized into before post-punk?'}, '572d3b0d8351f81400e9d37f': {'truth': 'various groups commonly labeled post-punk in fact predate the punk rock movement', 'predicted': 'the accuracy of the term\\'s chronological prefix \"post\" has been disputed, as various groups commonly labeled post-punk in fact predate the punk rock movement', 'question': 'Why is the term post-punk sometimes disputed?'}, '572d3b0d8351f81400e9d380': {'truth': 'between 1978 and 1984', 'predicted': '1978 and 1984', 'question': 'What is the acctepted era of post-punk?'}, '572e6bacc246551400ce422d': {'truth': 'punk or new wave music', 'predicted': 'new wave music', 'question': 'What were many groups now labeled as post-punk initially categorized as?'}, '5a270daac93d92001a4003a2': {'truth': '', 'predicted': '1978 and 1984', 'question': 'Between what years did the punk movement occur?'}, '5a270daac93d92001a4003a3': {'truth': '', 'predicted': 'punk or new wave music', 'question': 'What were groups categorized as post-punk later changed to?'}, '5a282e9bd1a287001a6d0ac7': {'truth': '', 'predicted': 'new wave music', 'question': 'What else was music correctly catagorized into before post-punk?'}, '5a282e9bd1a287001a6d0ac8': {'truth': '', 'predicted': '1978 and 1984', 'question': 'Which years was the punk era between?'}, '5a4e8143755ab9001a10f4a2': {'truth': '', 'predicted': 'rapid transit, light rail lines or other non-road public transport systems', 'question': 'What commonly connects a city with a departure loop?'}, '5a4e8143755ab9001a10f4a3': {'truth': '', 'predicted': 'AirTrain JFK', 'question': 'What is an example of an arrival loop?'}, '5a4e8143755ab9001a10f4a4': {'truth': '', 'predicted': 'motor vehicles', 'question': 'What does a departure loop help do?'}, '5a4e8143755ab9001a10f4a5': {'truth': '', 'predicted': 'controlled-access highways', 'question': 'What does Seattle allow access to the AirTrain through?'}, '57271739f1498d1400e8f388': {'truth': 'marked insulin resistance', 'predicted': 'insulin resistance', 'question': 'Almost all individuals who suffer from type 2 diabetes and/or obesity are found to have which trait?'}, '57301d1da23a5019007fcda7': {'truth': 'the San Ysidro neighborhood at the San Ysidro Port of Entry', 'predicted': 'San Ysidro neighborhood', 'question': \"Where is San Diego's border crossing?\"}, '57301d1da23a5019007fcda8': {'truth': '15-mile (24 km)', 'predicted': '15-mile', 'question': 'How long is the border that San Diego shares with Mexico?'}, '57301d1da23a5019007fcda9': {'truth': 'Otay Mesa', 'predicted': 'Otay Mesa area; it is the largest commercial crossing on the California-Baja California border', 'question': 'Where is the next nearest commercial crossing at the border?'}, '57301d1da23a5019007fcdaa': {'truth': 'the third-highest volume of trucks and dollar value of trade among all United States-Mexico land crossings.', 'predicted': 'third-highest volume', 'question': 'What is the volumte of trucks handled at the Otay Mesa crossing?'}, '5ad4d50b5b96ef001a10a258': {'truth': '', 'predicted': 'Otay Mesa area; it is the largest commercial crossing on the California-Baja California border', 'question': 'Where is the farthest commercial crossing at the border?'}, '57295a38af94a219006aa309': {'truth': 'electronic', 'predicted': 'electronic music', 'question': 'What type of music does Kraftwerk make?'}, '57295a38af94a219006aa30a': {'truth': 'Rock am Ring', 'predicted': 'Rock am Ring festival', 'question': 'What is the largest music festival in Germany?'}, '57295a38af94a219006aa30b': {'truth': 'up to 30,000', 'predicted': '30,000', 'question': \"How many people does M'era Luna Festival attract?\"}, '5a63e5537f3c80001a150b85': {'truth': '', 'predicted': 'Germany', 'question': 'What is the fourth largest music market in Europe?'}, '5a63e5537f3c80001a150b86': {'truth': '', 'predicted': 'trance music', 'question': 'What type of dance was pioneered in Germany?'}, '57317628e6313a140071cf61': {'truth': 'inseparable from', 'predicted': 'inseparable', 'question': 'What role did music play in the religious festivities? '}, '57317628e6313a140071cf62': {'truth': 'large variety of percussion and wind', 'predicted': '', 'question': 'What instruments were used to make music by the Central Americans?'}, '57317628e6313a140071cf63': {'truth': 'a jar in Guatemala', 'predicted': 'Guatemala', 'question': 'Where did archaeologists find a depiction of a Mayan stringed instrument?'}, '57317628e6313a140071cf64': {'truth': \"a jaguar's growl\", 'predicted': \"jaguar's growl\", 'question': \"What did the Mayan's stringed instrument sound like when played?\"}, '56cee23caab44d1400b88be6': {'truth': 'that China formally requested the support of the international community', 'predicted': '', 'question': 'What did UNICEF report?'}, '56d660e91c850414009470d3': {'truth': 'condolences and assistance', 'predicted': 'offering condolences and assistance', 'question': 'What did foreign nations offer China because of the severity of the quake?'}, '56d660e91c850414009470d6': {'truth': 'magnitude of the quake', 'predicted': 'Because of the magnitude of the quake, and the media attention on China', 'question': 'Why did the world community notice the need for help?'}, '5727865cf1498d1400e8fad2': {'truth': 'raise pupil achievement, improve pupil self-esteem, raise pupil aspirations and improve professional practice across the schools', 'predicted': 'to raise pupil achievement, improve pupil self-esteem, raise pupil aspirations and improve professional practice across the schools', 'question': 'What are the goals of the Independent and State School Partnership?'}, '5ad2043fd7d075001a4281fa': {'truth': '', 'predicted': '63', 'question': 'How many Eton students attended Eton free of charge in 2011?'}, '5ad2043fd7d075001a4281fb': {'truth': '', 'predicted': 'Tony Little', 'question': 'Who was the Head Master of Eton in 1982?'}, '5ad2043fd7d075001a4281fe': {'truth': '', 'predicted': 'Tony Little', 'question': 'Who was the Head Master of the Eton, Slough, Windsor and Hounslow Independent and State School Partnership in 2008?'}, '5ad3d325604f3c001a3ff26c': {'truth': '', 'predicted': 'Edinburgh of the Seven Seas', 'question': 'What is the name for the settlement located in the south-west coast?'}, '5ad3d325604f3c001a3ff26f': {'truth': '', 'predicted': 'Gough Island', 'question': 'Where is the weather station with a staff of 2,062 currently located?'}, '56e0a2a3231d4119001ac2f3': {'truth': 'the Chechen-Ingush ASSR', 'predicted': '', 'question': 'What state was dissolved on March 3, 1944?'}, '5ace04ac32bba1001ae4996e': {'truth': '', 'predicted': 'Stalin', 'question': 'Who ordered the naturalization of the residents of the Chechen-Ingush ASSR?'}, '5ad02a8077cf76001a686c56': {'truth': '', 'predicted': 'Tuscan', 'question': 'Which dialect became the norm for the ancient Italian language?'}, '5726083a89a1e219009ac167': {'truth': 'Premier League title', 'predicted': '', 'question': 'What competition sparked the rivalry with Manchester?'}, '5acd176407355d001abf3441': {'truth': '', 'predicted': 'Chelsea, Fulham', 'question': \"A 2008 poll showed which team is West Ham United's biggest rival?\"}, '5acd176407355d001abf3442': {'truth': '', 'predicted': 'Manchester United', 'question': \"According to a 2003 online poll who is Fulham's biggest rival club?\"}, '5acd176407355d001abf3444': {'truth': '', 'predicted': 'North London derbies', 'question': 'What are matches between Chelsea and Manchester United called?'}, '56cd81df62d2951400fa6666': {'truth': 'Verité', 'predicted': '', 'question': 'Who did Apple partner with to monitor its labor policies?'}, '56cd81df62d2951400fa6669': {'truth': 'Longhua, Shenzhen', 'predicted': 'Longhua', 'question': 'Where was the Foxconn plant located?'}, '56e10dbdcd28a01900c674e1': {'truth': 'the CDMA principle', 'predicted': 'CDMA principle', 'question': 'What are the ranging signals of the BeiDou system based on?'}, '56e10dbdcd28a01900c674e2': {'truth': 'open and restricted (military)', 'predicted': '', 'question': 'What positioning levels will the BeiDou system offer?'}, '56e10dbdcd28a01900c674e3': {'truth': 'globally to general users', 'predicted': 'globally', 'question': 'Where will the public service for the BeiDou system be available?'}, '56e10dbdcd28a01900c674e5': {'truth': '75+ satellites', 'predicted': '75+', 'question': 'How many satellites will the COMPASS navigation system use?'}, '5acd455207355d001abf3b89': {'truth': '', 'predicted': 'CDMA', 'question': 'The ranging signals have a simple structure and are based on which principle? '}, '5acd455207355d001abf3b8a': {'truth': '', 'predicted': 'complex', 'question': 'The ranging signals, based on the Galileo principle, have what type of structure?'}, '5acd455207355d001abf3b8c': {'truth': '', 'predicted': 'BeiDou navigation system', 'question': 'Which system is the predecessor of the GNSS system?'}, '572e9d6d03f9891900756837': {'truth': 'road, sea and air', 'predicted': 'by road, sea and air', 'question': 'What are the three modes of transport available on Cyprus?'}, '572e9d6d03f9891900756838': {'truth': '6,249 km (3,883 mi)', 'predicted': '3,883', 'question': 'How many miles of roads are paved on Cyprus?'}, '572e9d6d03f989190075683a': {'truth': 'left-hand', 'predicted': 'left-hand side', 'question': 'Which side of the road do vehicles on Cyprus drive on?'}, '5727b808ff5b5019007d935a': {'truth': 'Law should govern', 'predicted': '', 'question': 'What phrase Ariostle also use which is closely related to \"the rule of law\"?'}, '5727b808ff5b5019007d935b': {'truth': 'against the divine right of kings', 'predicted': 'divine right of kings', 'question': 'Samuel Rutherford used the principle of the rule of law to argue what point?'}, '5a3af1c53ff257001ab84351': {'truth': '', 'predicted': 'rule of law', 'question': 'What type of rule says a nation should be governed by the decisions of government officials?'}, '5a3af1c53ff257001ab84354': {'truth': '', 'predicted': 'Samuel Rutherford', 'question': 'What British theologian used the rule of law in his argument against the divine rights of kings?'}, '5ad26126d7d075001a429005': {'truth': '', 'predicted': 'changes in pulse rate', 'question': ' Aside from decreased perspiration, what is a physiological change related to emotions?'}, '57293faa6aef051400154be7': {'truth': 'South-West Africa', 'predicted': '', 'question': 'Where did the Spanish and Portugese enslave most of their black people from?'}, '5ad4005e604f3c001a3ffccc': {'truth': '', 'predicted': 'Spanish', 'question': 'What language did the first blacks to arrive in Bermuda speak?'}, '56e19b21e3433e140042300a': {'truth': 'use of carbon monoxide through the water gas shift reaction', 'predicted': '', 'question': 'How can it be recovered through steam?'}, '5726cb515951b619008f7e4d': {'truth': '50', 'predicted': '50%', 'question': 'What percent of scientific research is done at UNAM?'}, '572f9c99a23a5019007fc7d6': {'truth': 'a newer variant', 'predicted': '', 'question': 'What does a letter at the end of a device number mean?'}, '5a7b83a521c2de001afea0e4': {'truth': '', 'predicted': 'germanium', 'question': 'What were most early devices made from?'}, '5a7b83a521c2de001afea0e5': {'truth': '', 'predicted': '2, 3 or 4', 'question': 'What is the most common amount of extra digits in a device number?'}, '5a7b83a521c2de001afea0e8': {'truth': '', 'predicted': 'A', 'question': 'What is the most common letter suffix?'}, '56f9409c9b226e1400dd12c9': {'truth': 'Bellevue', 'predicted': 'Bellevue Hospital Center', 'question': 'Which hospital is located at the end if 27th Street?'}, '56f9409c9b226e1400dd12ca': {'truth': 'Chelsea', 'predicted': 'Chelsea Park', 'question': 'Which park does 27th Street pass through between Ninth and Tenth Avenues?'}, '5731628fe6313a140071ceb3': {'truth': 'wax or resin', 'predicted': 'wax or resin on a wooden panel', 'question': 'What were the tesserae usually set in for miniature mosaic icons?'}, '5731628fe6313a140071ceb4': {'truth': 'in the 12th century', 'predicted': '12th century', 'question': 'The more humanistic conception of Christ appeared when?'}, '56f756c6a6d7ea1400e171d4': {'truth': 'the score', 'predicted': '', 'question': 'Improvisation is integral before what took a high significance?'}, '56f756c6a6d7ea1400e171d5': {'truth': 'the 20th century', 'predicted': '20th century', 'question': 'When did oral tradition disappear?'}, '57268a4cdd62a815002e88ac': {'truth': '\"snobs\" who want to \"impose their tastes on everyone else\"', 'predicted': 'snobs', 'question': 'How did Murdoch describe critics of his newspaper?'}, '57268a4cdd62a815002e88ad': {'truth': 'they are \"giving the public what they want\"', 'predicted': '', 'question': 'What did Murdoch and Mackenzie say in defense of The Sun?'}, '57268a4cdd62a815002e88ae': {'truth': 'John Pilger', 'predicted': '', 'question': 'Who is one critic of The Sun?'}, '57268a4cdd62a815002e88af': {'truth': \"the genocide in Pol Pot's Cambodia\", 'predicted': \"Pol Pot's Cambodia\", 'question': \"What was the focus of Pilger's reporting in one issue of The Daily Mirror?\"}, '57268a4cdd62a815002e88b0': {'truth': 'the only edition of the Daily Mirror to ever sell every single copy issued throughout the country', 'predicted': '', 'question': 'What distinction does one single edition of The Daily Mirror hold?'}, '57109988a58dae1900cd6aba': {'truth': 'upper', 'predicted': 'upper classes', 'question': 'Did natural history in particular become increasingly popular amoung the upper or lower classes?'}, '570969eaed30961900e840cf': {'truth': 'published in more than one language, and their reach extends to almost all the Hindi-speaking states', 'predicted': '', 'question': 'What are the newspapers famous for?'}, '5a3636d5788daf001a5f8790': {'truth': '', 'predicted': 'Aapka Faisla, Amar Ujala, Panjab Kesari, Divya Himachal', 'question': 'What are the names of newspapers read widely in English?'}, '5732321ce17f3d1400422717': {'truth': 'edict of Milan', 'predicted': 'Milan', 'question': 'What edict defined imperial ideas as being those of toleration?'}, '5727faefff5b5019007d99d2': {'truth': 'no proposal has yet been made', 'predicted': '', 'question': 'What proposal has been made for the Mayan script? '}, '5727faefff5b5019007d99d3': {'truth': 'Unicode Consortium Web site', 'predicted': 'the Unicode Roadmap page of the Unicode Consortium Web site', 'question': 'Where does the Unicode Roadmap Committee post information on these scripts?'}, '5acd1d2407355d001abf3588': {'truth': '', 'predicted': 'Michael Everson, Rick McGowan, and Ken Whistler', 'question': 'Who are the potential candidates for encoding?'}, '571aeca132177014007e9ff2': {'truth': 'False Claims Act', 'predicted': '', 'question': 'What do illegal marketing cases fall under?'}, '5ad3b079604f3c001a3fecaa': {'truth': '', 'predicted': \"Eli Lilly's antipsychotic Zyprexa, and the other involved Bextra\", 'question': 'What companies have been involved with wrongdoing?'}, '570c56adfed7b91900d458d7': {'truth': 'acquiring the large sums of money needed', 'predicted': 'acquiring the large sums of money needed for his proposed campaigns to reclaim Normandy', 'question': \"What was one of John's principal challenges?\"}, '57300a06b2c2fd140056878a': {'truth': 'North Charleston', 'predicted': '', 'question': 'What is the third largest city in South Carolina?'}, '57300a06b2c2fd140056878d': {'truth': 'Berkeley County', 'predicted': 'Berkeley', 'question': 'In which county is Moncks Corner located?'}, '570b221b6b8089140040f76a': {'truth': 'Las Vegas, Nevada', 'predicted': '', 'question': 'Where was the first HD video conferencing system displayed?'}, '570b221b6b8089140040f76b': {'truth': 'Polycom', 'predicted': '', 'question': 'What company introduced the first HD video conferencing system to the general market?'}, '5726ad725951b619008f79f2': {'truth': 'the white-necked petrel, Kermadec petrel', 'predicted': 'white-necked petrel', 'question': 'What other types of petrels breed on Phillip Island?'}, '5a81b79c31013a001a334dd1': {'truth': '', 'predicted': 'Nepean Island', 'question': 'What island in the Norfolk Island Group is home to bioluminescent seabirds?'}, '5a81b79c31013a001a334dd2': {'truth': '', 'predicted': 'The providence petrel', 'question': 'What near extinct bird of Norfolk Island has shown signs of population decrease?'}, '5727fb814b864d1900164148': {'truth': 'Northwestern', 'predicted': '', 'question': 'Where is the home of the Center for Catalysis and Surface Science?'}, '5727fb814b864d1900164149': {'truth': 'Northwestern', 'predicted': '', 'question': 'Where is the home of the International Institute for Nanotechnology?'}, '5727fb814b864d190016414a': {'truth': 'Northwestern', 'predicted': '', 'question': 'Where is the home of the Materials Research Center?'}, '5727fb814b864d190016414b': {'truth': 'Northwestern', 'predicted': '', 'question': 'Where is the home for the Institute for Policy Research?'}, '5727fb814b864d190016414c': {'truth': 'Northwestern', 'predicted': '', 'question': 'Where is the home of the Buffet Center for International and Comparative Studies?'}, '572f90e2a23a5019007fc768': {'truth': 'the 1890s', 'predicted': '1890s', 'question': 'When did Washington University begin to expand west?'}, '572f90e2a23a5019007fc769': {'truth': 'Olmsted, Olmsted & Eliot of Boston', 'predicted': 'Olmsted, Olmsted & Eliot', 'question': 'What architecture firm was hired by the Board of Directors at Washington University? '}, '572f90e2a23a5019007fc76b': {'truth': '\"Hilltop\" campus', 'predicted': 'Hilltop', 'question': 'What nickname was given to the new campus site?'}, '5ace156432bba1001ae49a6c': {'truth': '', 'predicted': 'Olmsted, Olmsted & Eliot', 'question': \"Who was one of the university's Board of Directors members in the 1890s?\"}, '5ace156432bba1001ae49a6e': {'truth': '', 'predicted': 'Robert Brookings', 'question': 'Who designed the Manual School?'}, '5ace156432bba1001ae49a6f': {'truth': '', 'predicted': 'Washington Ave., Lucas Place, and Locust Street', 'question': 'What is one of the streets that the Danforth campus now lies next to?'}, '57313831497a881900248c71': {'truth': 'low elevation', 'predicted': '', 'question': 'What geological situation makes Tuvalu prone to storm damage?'}, '57313831497a881900248c73': {'truth': '4.6 metres', 'predicted': '4.6 metres (15 ft)', 'question': 'What is the highest elevation on Tuvalu?'}, '57313831497a881900248c75': {'truth': 'second-lowest', 'predicted': 'second', 'question': 'Where does Tuvalu rank among other countries as to lowest elevation?'}, '56d384e559d6e4140014660b': {'truth': '19 Entertainment', 'predicted': '19', 'question': 'What company are contestants required to sign a contract with on American Idol?'}, '572a470efed8de19000d5b68': {'truth': 'the Eli Whitney Museum', 'predicted': 'Eli Whitney Museum', 'question': 'What is the name of the museum that specifically focus on a single inventor located the city?'}, '5730048aa23a5019007fcc4d': {'truth': 'the Islamic Golden Age', 'predicted': 'Islamic Golden Age', 'question': 'What period was known for an era where Iranian civilization blossomed and peaked?'}, '5730048aa23a5019007fcc4e': {'truth': 'by the 10th and 11th centuries', 'predicted': '10th and 11th centuries', 'question': 'When did the Islamic Golden Age reach its zenith?'}, '5730048aa23a5019007fcc50': {'truth': 'scientific writing', 'predicted': 'scientific', 'question': 'Prominent Iranian writers during this time of the Islamic Golden Age contributed to what area of writing?'}, '59fb34d4ee36d60018400d67': {'truth': '', 'predicted': 'Round of 16', 'question': 'Where did Slovenia place in the 2010 World Cup games?'}, '59fb34d4ee36d60018400d69': {'truth': '', 'predicted': '2010', 'question': \"In what year was Germany's football team's worst defeat?\"}, '572b749abe1ee31400cb83ae': {'truth': 'exercise of reason and intellect', 'predicted': '', 'question': 'How did Hegel believe historical reality to be knowable to a philosopher?'}, '5a7c8ba9e8bc7e001a9e1eb5': {'truth': '', 'predicted': 'subjective idealism', 'question': 'How did Berkeley label his own idealism?'}, '572955e1af94a219006aa2cd': {'truth': 'requires the applicant to show their ability to test software', 'predicted': 'software testers and quality assurance specialists. No certification now offered actually requires the applicant to show their ability to test software', 'question': 'With several certifications out there that can be aquired, what is the one trait they all share?'}, '572955e1af94a219006aa2ce': {'truth': 'testing field is not ready for certification', 'predicted': '', 'question': 'What has the inability for the applicant to show how well they test led to?'}, '572955e1af94a219006aa2cf': {'truth': \"individual's productivity, their skill, or practical knowledge\", 'predicted': '', 'question': 'What four traits can a certification not measure?'}, '5a7b9f0621c2de001afea1e3': {'truth': '', 'predicted': \"an individual's productivity, their skill, or practical knowledge\", 'question': 'Certification is a measure of what?'}, '56cbedde6d243a140015edf4': {'truth': 'August', 'predicted': '11 August', 'question': 'During what month did Frédéric make his first appearance in Vienna?'}, '56cf6af94df3c31400b0d763': {'truth': 'Piano Concerto No. 2 in F minor, Op. 21', 'predicted': 'Piano Concerto No. 2 in F minor', 'question': 'What piece did Chopin debut after returning to Warsaw?'}, '56cf6af94df3c31400b0d765': {'truth': 'accustomed to the piano-bashing of local artists', 'predicted': 'for those accustomed to the piano-bashing of local artists', 'question': 'Why did some critics say that Chopin was too delicate?'}, '57279c2edd62a815002ea1ee': {'truth': 'several', 'predicted': '', 'question': 'How many ways does the Mimamsa separate into subschools?'}, '57279c2edd62a815002ea1f0': {'truth': 'perception', 'predicted': '', 'question': 'How is pratyaksa defined in the Prabhakara subschool?'}, '57279c2edd62a815002ea1f2': {'truth': 'upamāṇa', 'predicted': 'pratyakṣa (perception), anumāṇa (inference), upamāṇa', 'question': 'What is comparison and analogy in the Prabhakara school?'}, '572726a2f1498d1400e8f41c': {'truth': '1877', 'predicted': '1865-1877', 'question': 'When did the Reconstruction Era end?'}, '572726a2f1498d1400e8f41e': {'truth': 'Forty acres and a mule', 'predicted': '\"Forty acres and a mule\" policy', 'question': 'What was the policy of dividing land among families of color referred to as?'}, '5ad40ef3604f3c001a400145': {'truth': '', 'predicted': 'General William Tecumseh Sherman', 'question': 'Who proposed that land be divided and split up among white families?'}, '5727d1683acd2414000ded2d': {'truth': 'Roman artifacts', 'predicted': '', 'question': 'What has been found along the current Route des Romains? '}, '5727d1683acd2414000ded2f': {'truth': 'Christians', 'predicted': 'early Christians', 'question': 'Who shattered the fragments of a grand Mithraeum?'}, '5acd583707355d001abf3e0b': {'truth': '', 'predicted': 'fourth century', 'question': ' In what year was the Bishopric of Strasbourg established?'}, '56f75c11a6d7ea1400e17202': {'truth': 'the Baroque and early romantic eras', 'predicted': 'during both the Baroque and early romantic eras', 'question': 'When was improvisation in classical music performance common?'}, '56f75c11a6d7ea1400e17203': {'truth': 'the second half of the 19th and in the 20th centuries', 'predicted': 'second half of the 19th and in the 20th centuries', 'question': 'When did improvisation begin to lessened strongly?'}, '56f75c11a6d7ea1400e17204': {'truth': 'the cadenzas to their piano concertos', 'predicted': 'cadenzas to their piano concertos', 'question': 'What part did Mozart and Beethoven often improvise?'}, '56f75c11a6d7ea1400e17206': {'truth': 'soprano Maria Callas', 'predicted': 'Maria Callas', 'question': 'Who strongly supposed ome scritto?'}, '5734296dd058e614000b6a6e': {'truth': '40,000-plus', 'predicted': '40,000', 'question': 'How many Montanans entered the miltary in the first year of the war?'}, '5734296dd058e614000b6a6f': {'truth': 'over 57,000', 'predicted': '57,000', 'question': 'How many Montanans joined the military in the war total?'}, '5734296dd058e614000b6a70': {'truth': 'At least 1500', 'predicted': '1500', 'question': 'About how many Montanans  died in the war?'}, '5734296dd058e614000b6a71': {'truth': 'First Special Service Force or \"Devil\\'s Brigade,\"', 'predicted': 'First Special Service Force or \"Devil\\'s Brigade', 'question': 'Who trained at the military grounds in Montana?'}, '56d0e42e17492d1400aab68a': {'truth': 'the Buddhacarita', 'predicted': 'Buddhacarita', 'question': 'What is one of the earlier biographies on Buddhism?'}, '56d0e42e17492d1400aab68c': {'truth': 'Buddha', 'predicted': '', 'question': 'Who founded a monastic order in his life?'}, '56d1c2d2e7d4791d0090211f': {'truth': 'Buddha', 'predicted': 'the Buddha', 'question': \"Scholars do not make claims without evidence about who's life?\"}, '56d1c2d2e7d4791d00902121': {'truth': '5th ce', 'predicted': '5th century CE', 'question': 'The Jataka tales of the Theravada happened in what century?'}, '570f887880d9841400ab35a1': {'truth': 'Elizabeth', 'predicted': '', 'question': \"Who is the world's oldest reigning monarch?\"}, '570f887880d9841400ab35a3': {'truth': 'great-great-grandmother', 'predicted': 'her great-great-grandmother', 'question': 'How is Victoria related to Elizabeth?'}, '56d1335f17492d1400aabc14': {'truth': 'The Legend of Zelda: Twilight Princess HD', 'predicted': '', 'question': 'What is the name of the remastered game?'}, '56d1335f17492d1400aabc16': {'truth': 'Amiibo', 'predicted': 'Amiibo functionality', 'question': 'What kind of functionality will the remaster feature?'}, '5a8db847df8bba001a0f9ba1': {'truth': '', 'predicted': 'Tantalus Media', 'question': 'Which company is responsible for the HD version of Nintendo Direct?'}, '5a8db847df8bba001a0f9ba2': {'truth': '', 'predicted': 'Wii U', 'question': 'For which console is Nintendo Direct being made?'}, '5a8db847df8bba001a0f9ba4': {'truth': '', 'predicted': 'November 12, 2015', 'question': 'When were plans for Nintendo Direct revealed?'}, '572804792ca10214002d9b9e': {'truth': 'the Critique of Pure Reason', 'predicted': 'Critique of Pure Reason', 'question': 'In what did Immanuel Kant describe time as a priori intuition that allows humankind to understand sense experience?'}, '572804792ca10214002d9ba1': {'truth': 'temporal measurements', 'predicted': 'temporal', 'question': 'What type of measurements are used to quantify the duration of events?'}, '572804792ca10214002d9ba2': {'truth': 'Spatial measurements', 'predicted': 'Spatial', 'question': 'What type of measurements are used to quantify the distances between objects?'}, '5a7e003d70df9f001a8753fe': {'truth': '', 'predicted': 'comprehend sense experience', 'question': 'What does the mind allow us to do according to Immanuel Kant?'}, '5a8103d68f0597001ac0022a': {'truth': '', 'predicted': 'an a priori intuition', 'question': 'What did Kant portray measurement and time to be?'}, '5a8103d68f0597001ac0022b': {'truth': '', 'predicted': 'abstract conceptual framework', 'question': 'Kant thought of measurement as a fundamental part of what?'}, '5a8103d68f0597001ac0022c': {'truth': '', 'predicted': 'temporal', 'question': 'What type of measurements are used to quantify the number of events?'}, '5a8103d68f0597001ac0022d': {'truth': '', 'predicted': 'Spatial', 'question': 'What type of measurements are used to quantify the distance between measurements?'}, '570d5aabfed7b91900d45f05': {'truth': 'Moriscos', 'predicted': 'Jews and the Moriscos', 'question': 'What people group was descended from Muslim converts to Christianity?'}, '56f8a3aa9b226e1400dd0d24': {'truth': 'snowmaking in the ski resorts', 'predicted': 'snowmaking', 'question': 'Water is diverted from rivers for what purpose?'}, '56f8a3aa9b226e1400dd0d25': {'truth': 'unknown', 'predicted': '', 'question': 'What are the effects of diverting the water from rivers?'}, '56e7909700c9c71400d772e4': {'truth': 'Yuan Shikai', 'predicted': 'Yuan', 'question': 'Who moved the capital from Nanjing to Beijing?'}, '570b69c1ec8fbc190045ba00': {'truth': 'the New Wave of British Heavy Metal', 'predicted': 'New Wave of British Heavy Metal', 'question': 'What sub-genre of hard rock does Def Leppard belong to?'}, '570b69c1ec8fbc190045ba02': {'truth': '\"Photograph\", \"Rock of Ages\" and \"Foolin\\'\"', 'predicted': 'Photograph\", \"Rock of Ages\" and \"Foolin\\'\"', 'question': \"What were the three Top 40 singles from Def Leppard's Pyromania album?\"}, '5a5a3dd89c0277001abe70c0': {'truth': '', 'predicted': 'Def Leppard', 'question': 'Who is often categorized with the New Wave of American Heavy Metal?'}, '5a5a3dd89c0277001abe70c1': {'truth': '', 'predicted': \"High 'n' Dry\", 'question': 'What is the first album released by Def Leppard?'}, '5a5a3dd89c0277001abe70c3': {'truth': '', 'predicted': '1981', 'question': 'When did the band Ratt release their Too Fast for Love album?'}, '5ad02ad977cf76001a686c72': {'truth': '', 'predicted': 'Islam', 'question': 'What religion is notable due to an influx of foreign workers in the rural areas?'}, '5ad02ad977cf76001a686c74': {'truth': '', 'predicted': 'Islam', 'question': 'What is the most popular religion in South Africa, according to the CIA World Factbook?'}, '5ad02ad977cf76001a686c76': {'truth': '', 'predicted': '19.9%', 'question': 'What is the percentage of Protestants in South Africa?'}, '572fc623947a6a140053cc95': {'truth': '1991', 'predicted': 'after the collapse of the Soviet Union in 1991 and with the establishment of the Ministry of Defence in 1992', 'question': 'When was the Armenian military created?'}, '572fc623947a6a140053cc98': {'truth': 'Colonel General Seyran Ohanyan', 'predicted': '', 'question': 'Who is in charge of the Ministry of Defence?'}, '570e6b020b85d914000d7eb5': {'truth': 'early second millennium BCE', 'predicted': 'second millennium BCE', 'question': 'When is it thought that early speakers of Sanskrit came to India?'}, '570e6b020b85d914000d7eb6': {'truth': 'close', 'predicted': '', 'question': 'What is the relationship between Indo-Iranian and Baltic languages?'}, '5a2994e803c0e7001a3e17ed': {'truth': '', 'predicted': 'the early second millennium BCE', 'question': 'When did the original speakers of Sanskrit migrate to the north-west?'}, '5a2ab45c5b078a001a2f06c1': {'truth': '', 'predicted': 'Indo-Aryan migration theory', 'question': 'What theory explains the different features of Sanskrit and other Indo-European languages?'}, '5a2ab45c5b078a001a2f06c2': {'truth': '', 'predicted': 'India and Pakistan', 'question': 'To where did early speakers from the north-east bring Sanskrit?'}, '572ebac003f98919007569ad': {'truth': 'Canada', 'predicted': 'Canada and stretching nearly to Mexico', 'question': 'Which northern country do the Rocky Mountains begin at?'}, '572ebac003f98919007569ae': {'truth': 'Mexico', 'predicted': '', 'question': 'Which northern country do the Rocky Mountains terminate at?'}, '572ebac003f98919007569af': {'truth': 'The Rocky Mountain', 'predicted': 'Rocky Mountain', 'question': 'In the US, which region is the highest by elevation?'}, '5a0f25d8d7c85000188645bd': {'truth': '', 'predicted': 'the Rocky Mountains', 'question': 'Where did the Great Plains come to a gradual and at?'}, '5a0f25d8d7c85000188645be': {'truth': '', 'predicted': 'The Rocky Mountain region is the highest region of the United States', 'question': 'The appellation Mountains are the highest region of what?'}, '5a0f25d8d7c85000188645c0': {'truth': '', 'predicted': 'Colorado', 'question': 'The Lowes peak of the Rockies is found in what state?'}, '5726128a89a1e219009ac1ea': {'truth': 'senators', 'predicted': 'only senators', 'question': 'Who was allowed to wear gamrents dyed with Tyrian purple in ancient Rome?'}, '5726128a89a1e219009ac1ec': {'truth': 'the right to cover their upper body', 'predicted': 'cover their upper body', 'question': 'What right were lower caste women required to pay a tax to acquire?'}, '5a0cf7c8f5590b0018dab6b4': {'truth': '', 'predicted': 'senators', 'question': 'Who is not allowed to wear garments dyed purple in ancient Rome'}, '56fb2e3bf34c681400b0c1f5': {'truth': '1350', 'predicted': '', 'question': 'When did the Black Death end?'}, '56fb2e3bf34c681400b0c1f6': {'truth': 'Late', 'predicted': 'Late Middle Ages was marked by difficulties and calamities including famine, plague, and war, which significantly diminished the population of Europe; between 1347 and 1350', 'question': 'In what period of the Middle Ages did the Black Death occur?'}, '56fb2e3bf34c681400b0c1f8': {'truth': 'the early modern period', 'predicted': 'modern', 'question': 'What era occurred after the Late Middle Ages?'}, '572f54dba23a5019007fc554': {'truth': \"agreed to Army's demand that China be united under a Beijing government.\", 'predicted': '', 'question': 'What did China agree to avoid the undermining of the Republic?'}, '572f54dba23a5019007fc555': {'truth': 'Beijing, Shikai', 'predicted': 'Shikai', 'question': 'Who was sworn in as the second provisional president of the republic of China?'}, '572fdf3904bcaa1900d76e1f': {'truth': 'FC Ararat Yerevan team', 'predicted': 'FC Ararat Yerevan', 'question': 'What team won the Soviet football Cup in 1973 and 1975?'}, '572fdf3904bcaa1900d76e22': {'truth': 'eight teams', 'predicted': 'eight', 'question': 'How many teams does the Armenian Premier League have?'}, '56f74604aef2371900625a8b': {'truth': 'Roman Catholicism', 'predicted': 'Orthodox Christianity is predominant in the East and South Slavs, while Roman Catholicism', 'question': 'What religion is predominant in the West and western South Slavs?'}, '56cbd8c66d243a140015ed86': {'truth': 'his love life and his early death', 'predicted': 'love life and his early death', 'question': \"What parts of Frédéric's personal life influenced his legacy as a leading symbol of the era?\"}, '56cbd8c66d243a140015ed87': {'truth': 'Romantic era', 'predicted': 'Romantic', 'question': 'In which era was Frédéric leave a legacy of as a leading symbol?'}, '56ce1138aab44d1400b8842a': {'truth': 'political insurrection', 'predicted': '', 'question': 'He had a non-direct association with what?'}, '56ce1138aab44d1400b8842b': {'truth': 'Romantic era', 'predicted': 'Romantic', 'question': 'Chopin is closely associated with what era?'}, '56de708f4396321400ee28e2': {'truth': 'the ecumenical councils', 'predicted': 'ecumenical councils', 'question': 'What were the meetings called that were hosted by Constantine that helped enforce orthodoxy by Imperial authority?'}, '5a5abce09c0277001abe7162': {'truth': '', 'predicted': 'Constantine the Great', 'question': 'Who was the first Roman babtized?'}, '5a5abce09c0277001abe7165': {'truth': '', 'predicted': 'Pontifex Maximus', 'question': 'Who was high priest according to Christian tradition?'}, '57325b9fe99e3014001e670a': {'truth': \"Orwell's Nineteen Eighty-four\", 'predicted': '', 'question': \"What do former Jehovah's Witnesses members Heath and Gary Botting compare the culture of the religion to?\"}, '57325b9fe99e3014001e670d': {'truth': '\"intellectual dominance\"', 'predicted': 'intellectual dominance', 'question': \"What do critics of the Jehovah's Witnesses accuse the religion's leaders of exercising over members?\"}, '5725d36038643c19005acdb1': {'truth': '1948', 'predicted': 'Since the establishment of the State in 1948, and particularly since the late 1970s', 'question': 'When was Israeli fusion cuisine first developed?'}, '5728162d4b864d1900164442': {'truth': 'Call Of Duty 3', 'predicted': '', 'question': 'Which Call of Duty title does Sony include in their low-end price range?'}, '5728162d4b864d1900164443': {'truth': '2009', 'predicted': '', 'question': 'In what year was Devil May Cry 4 added to the budget game offerings for PS3?'}, '5728162d4b864d1900164444': {'truth': 'Greatest Hits', 'predicted': '', 'question': 'What words would you see in the United States or Canada on a PS3 game that would signify its lower price?'}, '5ad33f9a604f3c001a3fdbcb': {'truth': '', 'predicted': 'The Best', 'question': \"What's Sony's budget line of PS2 games called in Japan?\"}, '5ad33f9a604f3c001a3fdbcc': {'truth': '', 'predicted': 'Platinum', 'question': 'If you live in Australia and want affordable PlayStation 4 games, what range would you shop for?'}, '5ad33f9a604f3c001a3fdbcd': {'truth': '', 'predicted': 'Call Of Duty 3', 'question': 'Which Call of Duty title does Sony include in their high-end price range?'}, '5722ccb20dadf01500fa1ef4': {'truth': \"Through reading her mother's papers\", 'predicted': \"Through reading her mother's papers, Victoria discovered that her mother had loved her deeply; she was heart-broken\", 'question': 'How did Victoria realize that her mother deeply loved her?'}, '5722ccb20dadf01500fa1ef7': {'truth': \"Prince of Wales's philandering\", 'predicted': \"worry over the Prince of Wales's philandering\", 'question': \"What did Victoria blame Albert's death on?\"}, '5723d010f6b826140030fc8d': {'truth': 'chronic stomach trouble', 'predicted': 'stomach trouble', 'question': 'What illness was Albert suffering from while he helped Victoria through her grief?'}, '5724d5ba0a492a190043563a': {'truth': 'an actress in Ireland', 'predicted': '', 'question': 'Who was the Prince of Wales suspected to be having an affair with? '}, '57257e8fcc50291900b28535': {'truth': '1861', 'predicted': 'March 1861', 'question': 'When did the Duchess die?'}, '57257e8fcc50291900b28539': {'truth': 'had slept with an actress in Ireland', 'predicted': 'slept with an actress in Ireland', 'question': 'What gossip did Prince Albert hear about their son?'}, '5ad176e7645df0001a2d1d20': {'truth': '', 'predicted': '1861', 'question': \"During which year did Victoria's mother live?\"}, '5ad176e7645df0001a2d1d21': {'truth': '', 'predicted': \"Through reading her mother's papers, Victoria discovered that her mother had loved her deeply\", 'question': 'How did Victoria realize that her mother slightly loved her?'}, '56f95c439b226e1400dd13a9': {'truth': 'the Gilbert and Marshall Islands campaign', 'predicted': 'Gilbert and Marshall Islands campaign', 'question': 'What was the name of the campaign in which the US occupied the Marshalls?'}, '5726ed3ddd62a815002e9572': {'truth': 'Hanja', 'predicted': '', 'question': 'What is still being used according to experts?'}, '5726ed3ddd62a815002e9574': {'truth': 'weddings', 'predicted': '', 'question': 'What is considered a location with a high level of ambiguity?'}, '572a268f6aef051400155313': {'truth': '31 January 1773', 'predicted': '1773', 'question': 'In what year was the territory of Warmia incorporated? '}, '5a3bf219cc5d22001a521c3d': {'truth': '', 'predicted': 'Marienwerder (Kwidzyn)', 'question': 'What was the capital of Royal Prussia?'}, '5a3bf219cc5d22001a521c3e': {'truth': '', 'predicted': 'Marienwerder (Kwidzyn)', 'question': 'What was capital of Poland?'}, '5a3bf219cc5d22001a521c3f': {'truth': '', 'predicted': 'Frederick the Great', 'question': 'Who was the king of Poland? '}, '56fb85aab28b3419009f1df8': {'truth': '11th', 'predicted': 'late 11th century', 'question': 'During what century did the Investiture Controversy occur?'}, '56fb85aab28b3419009f1df9': {'truth': '1049', 'predicted': '1049–1054', 'question': 'When did the reign of Pope Leo IX begin?'}, '56fb85aab28b3419009f1dfc': {'truth': 'German princes', 'predicted': '', 'question': 'What secular rulers did the Concordat of Worms increase the power of?'}, '5726bd86f1498d1400e8e9b0': {'truth': 'Semantic-phonetic compounds', 'predicted': 'Semantic-phonetic compounds or pictophonetic compounds', 'question': 'What are the most numerous characters?'}, '57280c3f3acd2414000df30f': {'truth': 'Teddington Lock to the sea', 'predicted': 'Teddington Lock', 'question': \"What area of the River Thames does the Port of London Authority's jurisdiction cover?\"}, '57280c3f3acd2414000df312': {'truth': 'the London Ambulance Service (LAS) NHS Trust', 'predicted': 'London Ambulance Service (LAS) NHS Trust', 'question': 'The world\\'s largest \"free-at-the-point-of-use\" ambulance service is known as what?'}, '57280c3f3acd2414000df313': {'truth': 'The London Air Ambulance charity', 'predicted': 'London Air Ambulance charity', 'question': 'What agency operates in conjunction with the LAS as needed?'}, '572913111d0469140077901b': {'truth': 'the Weimar Constitution of 1919', 'predicted': 'Weimar Constitution of 1919', 'question': 'What does the term Länder date back to?'}, '572913111d0469140077901c': {'truth': 'Staaten', 'predicted': 'Staaten (States)', 'question': 'Before 1919 what were the German states called?'}, '5a4736e95fd40d001a27dd77': {'truth': '', 'predicted': 'Bundesland', 'question': 'What term is commonly used today for the Weimar Republic?'}, '5a4736e95fd40d001a27dd78': {'truth': '', 'predicted': 'Hamburg and Bremen', 'question': 'What two city-states are in Bavaria?'}, '5a513e8ece860b001aa3fc7d': {'truth': '', 'predicted': '1949', 'question': 'What year was the German Constitution written?'}, '5a513e8ece860b001aa3fc7e': {'truth': '', 'predicted': 'Staaten (States)', 'question': 'What were the constituent states of the German Empire called after 1919?'}, '5725e48589a1e219009ac05b': {'truth': 'Parthian', 'predicted': 'Parthian counterattack', 'question': 'Who was Antiochus VII Sidetes killed by which army?'}, '5725e48589a1e219009ac05d': {'truth': 'Greek drachmas', 'predicted': '', 'question': 'What was the currency in the Parthian Empire?'}, '56e0bc7b231d4119001ac364': {'truth': 'December 6, 195', 'predicted': 'December 6, 1957', 'question': 'Project Vanguard launch failed on what date?'}, '572ac792111d821400f38d5f': {'truth': 'turn over every single bit of his chemical weapons to the international community in the next week', 'predicted': '', 'question': 'What did Kerry say Syria could do to avoid a military strike?'}, '572ac792111d821400f38d61': {'truth': 'Russian Foreign Minister', 'predicted': 'Foreign Minister', 'question': \"What was Sergey Lavrov's position?\"}, '57307352069b5314008320eb': {'truth': 'total number of Greeks living outside Greece and Cyprus today is a contentious issue', 'predicted': '', 'question': 'How many people that are of Greek ascendancy live elsewhere than Greece ?'}, '57307352069b5314008320ec': {'truth': 'World Council of Hellenes Abroad', 'predicted': '', 'question': 'Who provided the contradictory  population numbers for Greeks abroad ?'}, '57307352069b5314008320ef': {'truth': 'George Prevelakis of Sorbonne University, the number is closer to just below 5 million', 'predicted': '', 'question': 'Who has presented the contradictory number to the census groups ?'}, '56df64a68bc80c19004e4bb3': {'truth': 'The University of St Mark & St John', 'predicted': 'University of St Mark & St John', 'question': 'What institution of higher  education is colloquially known as Marjons?'}, '572cb837750c471900ed4cf3': {'truth': 'involve claims and defenses under state laws', 'predicted': 'Most cases are litigated in state courts and involve claims and defenses under state laws', 'question': 'What types of cases are argued in the state courts?'}, '572cb837750c471900ed4cf5': {'truth': '272,795', 'predicted': '', 'question': 'How many cases did appellate courts receice in 2010?'}, '572cb837750c471900ed4cf6': {'truth': '282,000 new civil cases, 77,000 new criminal cases, and 1.5 million bankruptcy cases', 'predicted': '', 'question': 'What types of cases did federal district courts receive in 2010?'}, '5a79f25b17ab25001a8a01fe': {'truth': '', 'predicted': '103.5 million', 'question': 'How many new cases were filed in 2012?'}, '5a79f25b17ab25001a8a0200': {'truth': '', 'predicted': '5.9 million', 'question': 'How many trial courts received domestic relations cases in 2010?'}, '5a79f25b17ab25001a8a0201': {'truth': '', 'predicted': \"National Center for State Courts' Court Statistics Project\", 'question': 'What organization started in 2012?'}, '5a591bbb3e1742001a15cf91': {'truth': '', 'predicted': 'DNA', 'question': 'What does a molecule contain?'}, '5a591bbb3e1742001a15cf92': {'truth': '', 'predicted': 'DNA (or RNA in RNA viruses). The genome includes both the genes and the non-coding sequences of the DNA/RNA', 'question': 'What is one thing that genes have?'}, '5a591bbb3e1742001a15cf93': {'truth': '', 'predicted': 'DNA', 'question': 'What is included in virus RNA?'}, '57283f892ca10214002da183': {'truth': 'at least 14', 'predicted': '14', 'question': 'How many parts does a VHS tape have that must be manufactured?'}, '57283f892ca10214002da185': {'truth': 'as little as $1.00', 'predicted': '$1.00', 'question': 'How much did VHS cost to produce by the 1990s?'}, '5728ea364b864d1900165084': {'truth': 'translator', 'predicted': '', 'question': \"What was William Scott Wilson's occupation?\"}, '5728ea364b864d1900165085': {'truth': 'bushi (武士?, [bu.ɕi]) or buke (武家?)', 'predicted': 'bushi', 'question': 'What are samurai usually called in Japanse?'}, '56f8ec5d9e9bad19000a0704': {'truth': 'regions in their inventory', 'predicted': '', 'question': 'Assyria, Chaldea, Mesopotamia, Persia, Armenia, Egypt, Arabia, Syria, Palestine, Ethiopia, Caucasus, Libya, Anatolia, and Abyssinia were all what?'}, '5729355b1d0469140077916f': {'truth': 'specific region', 'predicted': '', 'question': 'What can forensic anthropologists determine about the ancestors of someone from their skeletal remains?'}, '5729355b1d04691400779170': {'truth': 'particular context', 'predicted': '', 'question': 'What does Brace feel the term \"black\" in meaningful in?'}, '5729355b1d04691400779171': {'truth': 'is not itself scientifically valid', 'predicted': '', 'question': 'Why is it bad that a category is merely socially constructed?'}, '5726baf5f1498d1400e8e92c': {'truth': 'Charles Wide', 'predicted': 'Judge Charles Wide', 'question': 'Who was appointed presiding judge over the retrial in 2015?'}, '5726baf5f1498d1400e8e92f': {'truth': 'why Marks was being replaced by Wide', 'predicted': '', 'question': 'What did Rumfit state that the defendants should have been informed about?'}, '5726baf5f1498d1400e8e930': {'truth': 'take the decision to judicial review', 'predicted': '', 'question': 'What did the lawyers for the defendants threaten to do?'}, '5729fcffaf94a219006aa725': {'truth': 'when it is trapped in a system with zero momentum', 'predicted': '', 'question': 'When does energy give rise to weight?'}, '5729fcffaf94a219006aa726': {'truth': 'certain amount of energy', 'predicted': 'a certain amount of energy', 'question': 'Mass is also equivalent to what?'}, '5729fcffaf94a219006aa728': {'truth': '1905', 'predicted': '', 'question': 'In what year did Einstein create E = mc2?'}, '5acd46d507355d001abf3bce': {'truth': '', 'predicted': 'Albert Einstein', 'question': 'Who created the formula E = nc2?'}, '56dcdbe566d3e219004dab35': {'truth': 'Pygmy', 'predicted': 'Pygmy people', 'question': 'What group of people were living in the area that would become the Congo prior to the arrival of Bantu tribes?'}, '5acff76f77cf76001a686698': {'truth': '', 'predicted': 'Bantu kingdoms', 'question': 'What Pygmy kingdom built trade links with the Congo River basin?'}, '5726f52bdd62a815002e9642': {'truth': 'Nigeria', 'predicted': '', 'question': 'What was the last African country to still have significant Polio problems?'}, '5726f52bdd62a815002e9646': {'truth': 'bone marrow transplant', 'predicted': 'life-saving bone marrow transplant', 'question': 'Nigeria was the second African country to perform which medical procedure?'}, '572b72e9be1ee31400cb83a3': {'truth': 'state of Great Moravia', 'predicted': 'the state of Great Moravia', 'question': 'What did the ninth century bring?'}, '572b72e9be1ee31400cb83a5': {'truth': 'Glagolitic', 'predicted': 'Glagolitic alphabet', 'question': 'What alphabet did the missionaries bring to the West Slavs?'}, '5a7a0eba17ab25001a8a0284': {'truth': '', 'predicted': 'church system', 'question': 'What kind of system was established by Cech?'}, '572a2b821d04691400779803': {'truth': 'physically possible process', 'predicted': 'a physically possible process', 'question': 'If if a cinematographic film were taken by means of physical  laws and then played backwards, it would still portray what?'}, '572a2b821d04691400779805': {'truth': 'the future', 'predicted': '', 'question': 'What do we not have memories of?'}, '5a42cd804a4859001aac7327': {'truth': '', 'predicted': 'physical laws', 'question': 'What laws are time-reversal variant?'}, '57277595708984140094de38': {'truth': 'their areas of origin', 'predicted': 'areas of origin', 'question': 'What does the head regalia of the bell-ringers represent?'}, '5727b1c13acd2414000de9eb': {'truth': 'Advaita', 'predicted': '', 'question': 'What means '}, '5a5e5b755bc9f4001a75af3b': {'truth': '', 'predicted': 'not two, sole, unity', 'question': 'Who coined the term Advaita?'}, '5a5e5b755bc9f4001a75af3c': {'truth': '', 'predicted': 'Adi Shankara', 'question': \"Who was Gaudapada's teacher?\"}, '57095defed30961900e84005': {'truth': 'ships', 'predicted': 'line parts of ships', 'question': \"Because of copper's biostatic properties where is a common use for copper?\"}, '57095defed30961900e84008': {'truth': 'corrosion-resistant', 'predicted': 'structural and corrosion-resistant', 'question': 'Name a property that makes copper a good material to use in marine environments?'}, '57279c33708984140094e235': {'truth': 'remove profanity, but he also made stylistic revisions', 'predicted': 'stylistic revisions', 'question': 'What alterations did Crane make to secure commercial publication?'}, '57279c33708984140094e236': {'truth': 'to preserve the stylistic and literary changes of 1896', 'predicted': '', 'question': \"What was Bower's first step in editing multiple works into a single product?\"}, '57279c33708984140094e237': {'truth': \"to revert to the 1893 readings where he believed that Crane was fulfilling the publisher's intention rather than his own\", 'predicted': '', 'question': \"What was Bower's second step in editing multiple works into a single product?\"}, '57279c33708984140094e238': {'truth': 'his judgment', 'predicted': '', 'question': 'What was one of the criticisms Bowers faced after editing Maggie?'}, '57279d21ff5b5019007d910f': {'truth': 'to preserve the stylistic and literary changes of 1896,', 'predicted': '', 'question': 'What was the first step Bowers took in editing a single work with two versions?'}, '57279d21ff5b5019007d9110': {'truth': \"to revert to the 1893 readings where he believed that Crane was fulfilling the publisher's intention\", 'predicted': '', 'question': 'What was the second step Bowers took in editing a single work with two versions?'}, '57279d21ff5b5019007d9111': {'truth': 'to remove profanity, but he also made stylistic revisions', 'predicted': '', 'question': 'What changes were made for the commercial publication of Maggie?'}, '570a6c176d058f1900182e4c': {'truth': 'Kurt Kortschal', 'predicted': 'Kurt Kortschal 2013', 'question': 'Who researched the role of emotional phenotype temperaments on social connectedness?'}, '570a6c176d058f1900182e4f': {'truth': 'five', 'predicted': '', 'question': 'How many million years ago did the evolution of chimpanzees and humans diverge?'}, '570a6c176d058f1900182e50': {'truth': '200,000 years', 'predicted': '', 'question': 'About how long ago did modern human beings first come into existence?'}, '5ad255afd7d075001a428d48': {'truth': '', 'predicted': '1.2%', 'question': ' What percentage of similarity is there between the genetic material of humans and the genetic material of chimpanzees?'}, '572a22256aef0514001552f8': {'truth': 'Sultans', 'predicted': 'weak Sultans', 'question': 'Poor rule by what class of people strained the empire?'}, '572a22256aef0514001552f9': {'truth': 'military technology', 'predicted': 'military', 'question': 'Europeans gained on the Ottoman empire in what type of technology? '}, '572a22256aef0514001552fa': {'truth': 'religious and intellectual', 'predicted': 'religious and intellectual conservatism', 'question': 'What types of conservative beliefs slowed the expansion of the empire?'}, '57261a8e38643c19005acff5': {'truth': 'Empiric', 'predicted': 'Empiric school', 'question': 'Which school of medicine was based on strict observation?'}, '570d9a31df2f5219002ed00e': {'truth': 'the Kerrison Predictor', 'predicted': 'Kerrison Predictor', 'question': 'What was the name of the mechanical computer that used automation?'}, '570d9a31df2f5219002ed00f': {'truth': 'the proper aim point automatically', 'predicted': 'proper aim point automatically', 'question': 'What did the Predictor calculate after it was pointed at a target?'}, '570d9a31df2f5219002ed010': {'truth': 'as a pointer mounted on the gun', 'predicted': 'a pointer', 'question': 'How did the Predictor display the information needed?'}, '570d9a31df2f5219002ed011': {'truth': 'followed the pointer and loaded the shells', 'predicted': 'simply followed the pointer and loaded the shells', 'question': 'What two things did the operators of the gun have to do?'}, '57316532a5e9cc1400cdbf19': {'truth': 'Strange Stories', 'predicted': 'Strange Stories from a Chinese Studio', 'question': \"What was the name of Pu Songling's collection of short stories?\"}, '572668e2708984140094c517': {'truth': 'paternal grandmother', 'predicted': 'her paternal grandmother', 'question': \"Who did Madonna turn to for comfort during her mother's illness?\"}, '572668e2708984140094c518': {'truth': 'rebelled', 'predicted': 'rebelled against', 'question': 'How did the Ciccone siblings behaved towards anyone brought to their home to replace their beloved mother?'}, '572668e2708984140094c51a': {'truth': 'unable to sleep unless she was near him', 'predicted': 'unable to sleep', 'question': 'Afraid that Tony would be taken from her, what does she do?'}, '57300da0947a6a140053cffa': {'truth': 'Pictish (northern Britain)', 'predicted': 'Hiberni (Ireland), Pictish (northern Britain) and Britons (southern Britain) tribes', 'question': 'Which is one of the tribes that spoke Insular Celtic?'}, '57300da0947a6a140053cffb': {'truth': 'beginning of the 1st millennium AD', 'predicted': 'at the beginning of the 1st millennium AD', 'question': 'When did the Pictish tribe start to inhabit the islands?'}, '5acd1c9c07355d001abf357e': {'truth': '', 'predicted': 'Hiberni (Ireland), Pictish (northern Britain) and Britons (southern Britain) tribes', 'question': 'Wich was the first tribe to speak Insular Celtic?'}, '5acd1c9c07355d001abf3582': {'truth': '', 'predicted': 'six', 'question': 'How many counties fought with the UK during the Irish War of Independence?'}, '5acd6a7807355d001abf4126': {'truth': '', 'predicted': 'Pictish (northern Britain) and Britons (southern Britain) tribes, all speaking Insular Celtic', 'question': 'The Hiberni tribes of northern Britain speak which language?'}, '5acd6a7807355d001abf4128': {'truth': '', 'predicted': 'AD 43', 'question': 'The Anglo-Saxon empire was conquered by the Roman Empire from when? '}, '5acd6a7807355d001abf4129': {'truth': '', 'predicted': 'England', 'question': 'The Anglo Saxons arrived in the 3rd century and would end up dominated what is modernly known as what country?'}, '5acd6a7807355d001abf412a': {'truth': '', 'predicted': '1919–1922', 'question': 'The Anglo-Viking treaty is associated with which time frame? '}, '56df6b9d56340a1900b29ae5': {'truth': 'Isaai', 'predicted': '', 'question': 'What do followers of Jesus call themselves in the Indian subcontinent?'}, '5ad2de1cd7d075001a42a57d': {'truth': '', 'predicted': 'In the Indian subcontinent', 'question': 'Where do Indians call themselves Isaai?'}, '5ad2de1cd7d075001a42a57e': {'truth': '', 'predicted': 'Isa Masih', 'question': 'Which term means followers of Jesus?'}, '5ad2de1cd7d075001a42a580': {'truth': '', 'predicted': 'Isa Masih', 'question': 'What do people in Nasrani call Jesus?'}, '5730b1488ab72b1400f9c6a5': {'truth': 'Super Advantage', 'predicted': 'Super Advantage is an arcade-style joystick', 'question': 'What game accessory was similar to the NES Advantage?'}, '5730b1488ab72b1400f9c6a8': {'truth': 'BatterUP', 'predicted': '', 'question': 'What game came with a baseball bat controller?'}, '5a42e7df4a4859001aac7394': {'truth': '', 'predicted': 'turbo', 'question': 'What adjustable settings did the Super Scope have on its joystick?'}, '572a5f33b8ce0319002e2ae5': {'truth': 'traditional Byzantine art', 'predicted': 'Byzantine art', 'question': 'Ottoman artists mixed Chinese art with the art of what else?'}, '572a5f33b8ce0319002e2ae6': {'truth': 'fountains and schools', 'predicted': 'mosques, bridges, fountains and schools', 'question': 'The ottoman empire built structures in Romania that included Mosques, and Bridges, what else was built?'}, '572a5f33b8ce0319002e2ae7': {'truth': 'the wide ethnic range of the Ottoman Empire', 'predicted': 'wide ethnic range of the Ottoman Empire', 'question': 'Why did the art of the Ottoman empire develop the way it did?'}, '570b609e6b8089140040f8eb': {'truth': 'Rush, Fly by Night and Caress of Steel', 'predicted': 'Fly by Night and Caress of Steel', 'question': 'What are the first three Rush albums?'}, '570b609e6b8089140040f8ed': {'truth': '\"The Boys Are Back in Town\"', 'predicted': 'The Boys Are Back in Town', 'question': \"What was Thin Lizzy's hit single?\"}, '5a5a32199c0277001abe70a3': {'truth': '', 'predicted': 'progressive', 'question': 'What sound did the band Rush move toward in the 1976 album Caress of Steel?'}, '5a5a32199c0277001abe70a5': {'truth': '', 'predicted': 'The Boys Are Back in Town', 'question': 'Which song by Lizzy reached number 8 on the US charts in 1976?'}, '572a8990f75d5e190021fb59': {'truth': '2015', 'predicted': '', 'question': 'In what year did the Indian government begin to stop recognizing madaris as schools?'}, '57277e4edd62a815002e9eb4': {'truth': 'southern', 'predicted': 'southern provinces North Brabant and Limburg', 'question': 'In what provinces is the Carnival mainly celebrated in the Netherlands?'}, '57277e4edd62a815002e9eb5': {'truth': 'Ash', 'predicted': 'Ash Wednesday', 'question': 'Dutch Carnaval is celebrated until which Wednesday?'}, '57277e4edd62a815002e9eb7': {'truth': 'herring', 'predicted': 'eating herring', 'question': 'What is consumed on Ash Wednesday?'}, '570e432f0dc6ce1900204ee2': {'truth': '1–2%', 'predicted': '', 'question': 'What percentage of high-density penetrators is not made up of depleted uranium?'}, '570e432f0dc6ce1900204ee3': {'truth': 'molybdenum', 'predicted': '', 'question': 'Along with titanium, what element often makes up the portion of high-density penetrators not made of depleted uranium?'}, '5ad113dc645df0001a2d0c8d': {'truth': '', 'predicted': 'Persian Gulf', 'question': 'Along with the Balkans, in what geographical location did a war take place where the UN used depleted uranium munitions?'}, '5ad113dc645df0001a2d0c8e': {'truth': '', 'predicted': 'Gulf War Syndrome', 'question': 'What illness is definitely tied to the use of depleted uranium munitions?'}, '5732549b0fdd8d15006c69c9': {'truth': 'professional', 'predicted': 'professional and governmental', 'question': 'Along with business and government, what leaders did Eisenhower see meeting at the Council on Foreign Relations?'}, '5732549b0fdd8d15006c69ca': {'truth': 'economic', 'predicted': 'economic analysis', 'question': 'What sort of analysis did Eisenhower first experience with the Council on Foreign Relations?'}, '572ba616111d821400f38f37': {'truth': 'when the entire expression is in nominative or accusative case', 'predicted': '', 'question': 'When is the genitive case used?'}, '572ba616111d821400f38f38': {'truth': 'a Slavic language', 'predicted': 'Slavic language', 'question': \"What is Czech's handling of cardinal numbers typical of?\"}, '5727972a708984140094e1a9': {'truth': 'Bangladesh, Brazil, China, Egypt', 'predicted': '', 'question': 'What are some of the target countries?'}, '5727972a708984140094e1aa': {'truth': 'universal primary school', 'predicted': 'universal primary school availability', 'question': 'What do they want to do with regards to schooling of young children?'}, '5ad014c177cf76001a686917': {'truth': '', 'predicted': 'December 30, 2010', 'question': 'When did the Congolese parliament strike down a law to protect indigenous people?'}, '5ad014c177cf76001a686919': {'truth': '', 'predicted': 'property', 'question': 'What does the Congolese Human Rights Observatory say Bantus are treated as?'}, '56de45d84396321400ee2754': {'truth': 'being a non-UN member or unable or unwilling to provide the necessary data at the time of publication', 'predicted': '', 'question': 'What three reasons were mentioned for countries being excluded?'}, '5acd71d807355d001abf428e': {'truth': '', 'predicted': 'a new age of scientific and intellectual inquiry and appreciation of ancient Greek and Roman civilizations', 'question': 'What did the Renaissance in Asia usher in?'}, '5acd71d807355d001abf4292': {'truth': '', 'predicted': 'Missionaries and scholars', 'question': 'Who delivered new ideas to other civilizations?'}, '56dff3c2231d4119001abef1': {'truth': 'the Woolpack', 'predicted': 'Woolpack', 'question': 'What pub is featured on Emmerdale?'}, '5726a3fc5951b619008f78b5': {'truth': 'French and Germans', 'predicted': 'European nations, particularly the French and Germans', 'question': 'Which two nations has The Sun been very antagonistic towards?'}, '5726a3fc5951b619008f78b6': {'truth': '\"frogs\", \"krauts\" or \"hun\"', 'predicted': '', 'question': 'What names were used by The Sun to characterize the French and Germans?'}, '5726a3fc5951b619008f78b7': {'truth': 'opposed', 'predicted': '', 'question': \"What is the paper's stance on the EU?\"}, '5726a3fc5951b619008f78b8': {'truth': 'le Worm', 'predicted': '', 'question': 'How was French president Jacques Chirac described by The Sun?'}, '5726872c5951b619008f75c9': {'truth': \"one of the world's longest-running ongoing civil wars.\", 'predicted': 'civil wars', 'question': 'What major conflict is Myanmar known for?'}, '5726872c5951b619008f75ca': {'truth': 'the military junta', 'predicted': '', 'question': 'What portion of the government was eventually separated from government participation? '}, '5726872c5951b619008f75cb': {'truth': 'a nominally civilian government', 'predicted': 'civilian', 'question': 'What type of government is now established in Myanmar?'}, '5726872c5951b619008f75cc': {'truth': 'former military leaders still wield enormous power in the country', 'predicted': 'wield enormous power', 'question': 'Are previous leaders a hendrence to the current government?'}, '57279c1aff5b5019007d90ea': {'truth': 'some point between 915 and 922', 'predicted': '', 'question': 'When did Viking travelers establish a trading post in Cork?'}, '57279c1aff5b5019007d90ec': {'truth': 'otherwise unobtainable trade goods', 'predicted': 'unobtainable trade goods', 'question': 'What did the Norsemen provide to the monastery?'}, '5a5d0b835e8782001a9d5e73': {'truth': '', 'predicted': 'the Norsemen', 'question': 'Who did the monastery provide religious services for?'}, '56de71b2cffd8e1900b4b8fc': {'truth': 'European Reformation', 'predicted': 'the 16th-century European Reformation', 'question': 'What event in England during the 16th century had an outcome of many deaths for heresy?'}, '56de71b2cffd8e1900b4b8fd': {'truth': 'Henry VIII', 'predicted': '', 'question': \"During what king's reign did 60 Protestants die for heresy?\"}, '5a5ad1eb9c0277001abe71b1': {'truth': '', 'predicted': 'Henry VIII', 'question': \"During who's reign were Catholics executed as heretics?\"}, '572f2ce3a23a5019007fc4b3': {'truth': '3 kV DC', 'predicted': '', 'question': 'What voltage is being used in railway system of South Africa and Chile?'}, '572f2ce3a23a5019007fc4b5': {'truth': 'Western Railroad', 'predicted': '', 'question': 'What was New Jersey Transit called before?'}, '572f2ce3a23a5019007fc4b6': {'truth': 'AC', 'predicted': '', 'question': 'What does the railway system of US use DC or AC?'}, '5acd72be07355d001abf42c3': {'truth': '', 'predicted': 'Kolkata', 'question': 'The Kukuburra suburban railway is located where?'}, '5acd72be07355d001abf42c4': {'truth': '', 'predicted': '25 kV 50 Hz AC', 'question': 'The Kolkata suburban railway in India converted from what?'}, '570a55836d058f1900182d68': {'truth': '1950s', 'predicted': '1950s & 1960s', 'question': 'In which decade did the expansion of the South Kensington campus being?'}, '5a4860a984b8a4001a7e7858': {'truth': '', 'predicted': 'Natural History Museum, the Science Museum, the Victoria and Albert Museum, the Royal College of Music, the Royal College of Art, the Royal Geographical Society and the Royal Albert Hall', 'question': \"What are some of the institutions are across from Imperial's main campus?\"}, '5a4860a984b8a4001a7e785a': {'truth': '', 'predicted': 'South Kensington', 'question': 'What campus was absorbed by the Imperial Institute?'}, '5723df4df6b826140030fccf': {'truth': 'amity', 'predicted': '', 'question': 'What must two lodges be in, in order to inter-visit?'}, '5723df4df6b826140030fcd1': {'truth': 'Exclusive Jurisdiction and Regularity', 'predicted': '', 'question': 'What can be causes of one Grand Lodge withdrawing Recognition from another?'}, '5723df4df6b826140030fcd2': {'truth': 'brethren', 'predicted': '', 'question': 'What are the members of a Grand Lodge called?'}, '5726225d271a42140099d4c3': {'truth': 'amity', 'predicted': '', 'question': 'Gran Lodges are in what when they are in Masonic Communication with each other?'}, '5726225d271a42140099d4c5': {'truth': 'a list', 'predicted': 'a list of other Grand Lodges that it recognises', 'question': 'What does each Grand Lodge maintain?'}, '56beabab3aeaaa14008c91dc': {'truth': 'Ban Bossy campaign', 'predicted': 'Ban Bossy', 'question': 'Beyonce supported which campaign that encourages leadership in girls?'}, '570a6f996d058f1900182e60': {'truth': 'Baruch Spinoza', 'predicted': 'Niccolò Machiavelli, Baruch Spinoza', 'question': 'Along with Descartes, Machiavelli and Hume, what notable philosopher developed a theory of emotions?'}, '572f6c2cb2c2fd14005680f9': {'truth': 'the Kakatiya dynasty was annexed', 'predicted': 'annexed', 'question': 'What did the Malik Kafur do to the Kakatiya dynasty in 1321?'}, '572f6c2cb2c2fd14005680fb': {'truth': '1347', 'predicted': '', 'question': 'When was the Behmani Sultanate established?'}, '570d6cf1fed7b91900d460a7': {'truth': 'lethal French gunfire', 'predicted': 'French gunfire', 'question': 'What factor immobilised the Prussian Guard?'}, '571ae71a32177014007e9fc6': {'truth': 'those based on re-formulation of an existing active ingredient', 'predicted': '', 'question': 'What drugs are the least expensive to develop?'}, '571ae71a32177014007e9fc7': {'truth': 'opportunity cost of investing capital', 'predicted': '', 'question': 'What accounts for nearly half of the costs to develop drugs?'}, '571ae71a32177014007e9fc8': {'truth': 'increasingly outsource risks related to fundamental research', 'predicted': '', 'question': 'What is the consequence in the value chain?'}, '571ae71a32177014007e9fc9': {'truth': 'somewhat reshapes the industry ecosystem with biotechnology companies', 'predicted': '', 'question': 'What happens when companies outsource?'}, '571d1d495efbb31900334ea8': {'truth': 'nearly half the total expense', 'predicted': 'half', 'question': 'Investing capital can increase drug development costs by how much?'}, '5ad399f2604f3c001a3fe847': {'truth': '', 'predicted': 'Line-extensions', 'question': 'What is re-formulations of approved drugs referred to?'}, '57286d4b4b864d19001649dd': {'truth': 'a single vote', 'predicted': 'single', 'question': 'How many votes did the vote of confidence lose by in 1979?'}, '5ad13b7e645df0001a2d1300': {'truth': '', 'predicted': 'March 1979', 'question': 'When was Welsh devolution approved of?'}, '5ad13b7e645df0001a2d1302': {'truth': '', 'predicted': 'the Labour government', 'question': 'Who pushed ahead with setting up a Scottish Assembly?'}, '5ad13b7e645df0001a2d1304': {'truth': '', 'predicted': 'single vote', 'question': \"How much did Callaghan's government win a vote of confidence by?\"}, '5732aeedcc179a14009dac04': {'truth': 'about 10,000 years ago', 'predicted': '10,000 years ago', 'question': 'How long ago did the last glacial period end?'}, '5732aeedcc179a14009dac05': {'truth': '35 metres (115 ft)', 'predicted': '', 'question': 'By what height did sea levels rise at the end of the last glacial period?'}, '5732aeedcc179a14009dac06': {'truth': 'Holocene', 'predicted': '', 'question': 'During what period did sea levels rice 115 feet?'}, '5732aeedcc179a14009dac07': {'truth': 'Pleistocene', 'predicted': '', 'question': 'Glaciars from what period depressed the height of northern lands by 591 feet?'}, '5732aeedcc179a14009dac08': {'truth': 'Tyrrell Sea', 'predicted': '', 'question': 'What sea did the Hudson Bay used to be a part of?'}, '5a4ebffaaf0d07001ae8cc2e': {'truth': '', 'predicted': 'about 10,000 years ago', 'question': 'When did the last glacial period begin?'}, '572f21e6b2c2fd1400567f3d': {'truth': 'Donald Wills Douglas, Sr', 'predicted': 'Donald Wills Douglas', 'question': 'Who built a plant in Clover field?'}, '572f21e6b2c2fd1400567f3e': {'truth': 'Douglas Aircraft', 'predicted': 'Douglas Aircraft Company', 'question': 'What company was the structure at Clover Field for?'}, '572f21e6b2c2fd1400567f41': {'truth': 'Two', 'predicted': '', 'question': 'How Many Planes returned from the circumnavigation in 1924?'}, '5a2d5455f28ef0001a52648e': {'truth': '', 'predicted': 'Donald Wills Douglas', 'question': 'Who was one of the pilots that flew four Douglas-built planes attempting the first aerial circumnavigation of the world?'}, '56de43294396321400ee2738': {'truth': 'North Korea', 'predicted': '', 'question': 'Which East Asian dictatorship was excluded from the 2011 report?'}, '5ad0cfc9645df0001a2d0482': {'truth': '', 'predicted': 'unavailability of certain crucial data', 'question': 'What is not the main reason that countries were excluded from the 2011 report?'}, '5731fe53e17f3d14004225bc': {'truth': 'President Franklin D. Roosevelt', 'predicted': 'Franklin D. Roosevelt', 'question': 'What American president was a member of the Pacific War Council?'}, '56e030597aa994140058e32b': {'truth': '1770', 'predicted': 'about 1770', 'question': 'When did the Island start to experience a period of prosperity?'}, '56e030597aa994140058e32c': {'truth': 'James Cook', 'predicted': 'Captain James Cook', 'question': 'What captain visited the island in 1775 on their trip around the world?'}, '570cf05db3d812140066d349': {'truth': 'in the mouth', 'predicted': 'the mouth', 'question': 'Where does the digestions of some fats start?'}, '570cf05db3d812140066d34b': {'truth': 'produces hormones that stimulate the release of pancreatic lipase from the pancreas and bile from the liver', 'predicted': '', 'question': 'What happens when there is a presence of fat in the small intestine?'}, '570cf05db3d812140066d34c': {'truth': 'helps in the emulsification of fats for absorption of fatty acids', 'predicted': 'emulsification of fats for absorption of fatty acids', 'question': 'What does bile from the liver help do?'}, '573236d2e17f3d1400422735': {'truth': 'Julian', 'predicted': '', 'question': 'Who rejected the Christian religion?'}, '573236d2e17f3d1400422739': {'truth': 'Christian control', 'predicted': 'Christian', 'question': \"After Julian's death, under to what type of religion did the empire return?\"}, '57272cf65951b619008f8691': {'truth': 'federal', 'predicted': 'black citizens had been calling for - an enhanced role of federal', 'question': \"Who's authority did Truman want to increase throughout the states?\"}, '5ad4105d604f3c001a40019f': {'truth': '', 'predicted': 'Truman', 'question': ' Who was the first President to address the NICP?'}, '5ad4105d604f3c001a4001a0': {'truth': '', 'predicted': '10,000', 'question': ' How many people left the speech that Truman made?'}, '5ad4105d604f3c001a4001a1': {'truth': '', 'predicted': 'the Lincoln Memorial', 'question': \"Where did Truman's historic speech end?\"}, '5ad4105d604f3c001a4001a2': {'truth': '', 'predicted': 'equality of opportunity', 'question': \"During the speech, Truman made the statement that each man shouldn't be guaranteed what?\"}, '5ace3ac232bba1001ae49f7c': {'truth': '', 'predicted': 'Ann Arbor Civic Theatre', 'question': 'What Civic Theatre is associated with the university?'}, '57099d82ed30961900e84384': {'truth': 'The Cyber Incident Management Framework for Canada', 'predicted': 'Cyber Incident Management Framework for Canada', 'question': 'What outlines the responsibilities and provides a plan for coordination during a cyber incident?'}, '5a557754134fea001a0e1ab4': {'truth': '', 'predicted': 'Public Safety Canada', 'question': 'Canadian cyberspace was boosted by who? '}, '5a557754134fea001a0e1ab7': {'truth': '', 'predicted': 'boost the security of Canadian cyberspace', 'question': 'Why was a cyber security plan unveiled in October of 2010? '}, '5a5cfad65e8782001a9d5e37': {'truth': '', 'predicted': 'multiple', 'question': 'How many departments does the new strategy involve?'}, '570629ba52bb891400689917': {'truth': 'compression ratios', 'predicted': '', 'question': 'What can CD parameters be used as references for?'}, '5730b108069b531400832267': {'truth': '3000 years', 'predicted': '3000', 'question': 'How many years ago did migrations of people happen in the Pacific area?'}, '5730b108069b531400832268': {'truth': 'canoe', 'predicted': 'canoe voyaging', 'question': 'By what means did locale people travel between Pacific islands?'}, '5730b108069b53140083226b': {'truth': 'eight standing together', 'predicted': '\"eight standing together\"', 'question': 'What is the native language meaning of Tuvalu?'}, '56cddcae62d2951400fa6914': {'truth': 'fifteen hundred', 'predicted': 'over fifteen hundred extras were hired for the pre-title sequence set in Mexico, though they were duplicated in the film, giving the effect of around ten thousand', 'question': 'How many actual people were used for the opening sequence of Spectre?'}, '56cf45bcaab44d1400b88ef4': {'truth': 'Austria.', 'predicted': 'Austria', 'question': 'In which country were the scenes with Detlef Bothe shot?'}, '5ad22be1d7d075001a428608': {'truth': '', 'predicted': 'Stephanie Sigman', 'question': 'Alessandro who was cast as Estrella?'}, '5ad22be1d7d075001a428609': {'truth': '', 'predicted': 'Detlef Bothe', 'question': 'Who was cast as a hero for scenes set in Austria?'}, '5ad22be1d7d075001a42860a': {'truth': '', 'predicted': 'pre-title sequence', 'question': 'Over one hundred extras were hired for what?'}, '56d9b546dc89441400fdb714': {'truth': 'respond with aggression', 'predicted': 'run away (52%) or respond with aggression', 'question': 'When these feral dogs are approached by a person, they tend to do this 11% of the time?'}, '56d9b546dc89441400fdb715': {'truth': 'pet dogs living in human homes.', 'predicted': 'pet dogs living in human homes', 'question': 'Dog cognition has been studied on what kind of dogs?'}, '5730e777aca1c71400fe5b46': {'truth': '40 years later', 'predicted': '', 'question': 'When did the US Air Force separate from the War Department? '}, '5730e777aca1c71400fe5b47': {'truth': '68,000', 'predicted': '', 'question': 'How many causalities did the US Air Force suffer during WWII? '}, '5727891b708984140094e031': {'truth': 'Kazakh students', 'predicted': 'troops, volunteers, militia units, and Kazakh students', 'question': 'Who was demonstrating?'}, '5727891b708984140094e033': {'truth': '30,000 to 40,000', 'predicted': '', 'question': 'What were the attendance estimates from non governmental groups?'}, '572805f5ff5b5019007d9b1a': {'truth': 'RFC 3629', 'predicted': '', 'question': 'What is the UTF-8 standard? '}, '5acd211a07355d001abf3601': {'truth': '', 'predicted': 'lone', 'question': 'What type of bytes are required in a high bit set?'}, '56e7860900c9c71400d77233': {'truth': '280,000 registered households', 'predicted': '280,000', 'question': 'During the time of the North–South division, how many households were in Nanjing?'}, '56e7860900c9c71400d77234': {'truth': 'more than 1.4 million residents', 'predicted': '1.4 million', 'question': 'What is the estimated population of Nanjing during that time?'}, '56fb7c108ddada1400cd6457': {'truth': 'Baltic', 'predicted': 'the Baltic', 'question': 'What sea were the Hanseatic cities located on?'}, '57313a17497a881900248c93': {'truth': 'Russia', 'predicted': 'Albazin, the far eastern outpost of the Tsardom of Russia', 'question': 'What European country did Kangxi fight?'}, '572a3b61af94a219006aa8e3': {'truth': 'In 1915', 'predicted': '1915', 'question': 'When did the Russian Caucasus Army advance into eastern Anatolia?'}, '572a3b61af94a219006aa8e4': {'truth': 'ethnic Armenian', 'predicted': 'Armenian', 'question': 'What ethnic group was deported by the Ottoman Government from eastern Anatolia?'}, '572a3b61af94a219006aa8e6': {'truth': 'the Syrian desert', 'predicted': 'Syrian desert', 'question': 'Armenian women and children were deported on death marches through what desert?'}, '572a3b61af94a219006aa8e7': {'truth': 'Greek and Assyrian minorities', 'predicted': 'Greek and Assyrian', 'question': 'Two other ethnic minorities were massacred during Ottoman ethnic cleansing, what were they?'}, '572a8395111d821400f38b94': {'truth': 'free', 'predicted': '', 'question': 'How much does it cost to use Metromover?'}, '5ad3984c604f3c001a3fe7eb': {'truth': '', 'predicted': 'Metrorail', 'question': \"What is the name of Miami's light-rail system?\"}, '5ad3984c604f3c001a3fe7ec': {'truth': '', 'predicted': '24.4', 'question': 'How many yards long is Metrorail?'}, '573036ea947a6a140053d2bf': {'truth': 'high-occupancy-vehicle (HOV) \"managed lanes\"', 'predicted': 'high-occupancy-vehicle (HOV) \"managed lanes', 'question': 'What type of special lanes were added to Interstate 15?'}, '5ad4dbaa5b96ef001a10a450': {'truth': '', 'predicted': 'The Merge', 'question': 'What is the name given to the highly congested spot where Interstates 85 and 805 meet?'}, '5ad4dbaa5b96ef001a10a451': {'truth': '', 'predicted': 'The South Bay Expressway', 'question': 'What tollway connects Otay Mesa with SR 45?'}, '56dfb0e97aa994140058dfe8': {'truth': 'first', 'predicted': 'first test', 'question': \"On which run did Bell's metal detector give a small indication?\"}, '56dfb0e97aa994140058dfe9': {'truth': 'in tests', 'predicted': '1881. The device was quickly put together in an attempt to find the bullet in the body of U.S. President James Garfield. According to some accounts, the metal detector worked flawlessly in tests', 'question': \"When did Bell's metal detector work well?\"}, '570a5bbf4103511400d5965b': {'truth': 'Funding Council', 'predicted': '', 'question': 'Who granted Imperial close to 170 million pounds in grants?'}, '57313c1205b4da19006bcf05': {'truth': 'the 3rd century BC', 'predicted': '3rd century BC', 'question': 'When was the Greek figure style mostly formed?'}, '57313c1205b4da19006bcf06': {'truth': 'the 4th-century BC', 'predicted': '4th-century BC', 'question': 'When was the famous mosaic \"The Beauty of Durres\" created?'}, '56e141e2e3433e1400422d04': {'truth': 'People of Irish descent', 'predicted': 'Irish descent', 'question': 'What people form the largest ethnic group in the city?'}, '56e141e2e3433e1400422d06': {'truth': 'Italians', 'predicted': '', 'question': 'What is the second largest ethnic group in the city?'}, '5727d11d3acd2414000ded19': {'truth': '83 °F', 'predicted': '83 °F (28 °C)', 'question': \"What is Oklahoma's record high temperature for Nov 11?\"}, '5727d11d3acd2414000ded1b': {'truth': 'an Arctic cold front', 'predicted': 'Arctic cold front', 'question': \"What caused Oklahoma's temperature to plummet 66 degrees on Nov 11, 1911?\"}, '5727d11d3acd2414000ded1c': {'truth': 'one tornado per hour', 'predicted': 'one tornado per hour over the course of a day', 'question': 'How fast did the 1912 tornado outbreak make tornadoes?'}, '5727d11d3acd2414000ded1d': {'truth': 'a day', 'predicted': 'about one tornado per hour over the course of a day', 'question': 'How long did the 1912 tornado outbreak last?'}, '56e06d44231d4119001ac104': {'truth': '/p pʰ b/', 'predicted': 'three-way distinction in stops and affricates: /p pʰ b/', 'question': 'What is the actual distinction for Wu Chinese?'}, '5acd2a8c07355d001abf378c': {'truth': '', 'predicted': 'stops and affricates', 'question': 'Wu Chinese has a four-way distinction in what?'}, '56ce771daab44d1400b887cf': {'truth': 'hydrogen production from protons', 'predicted': 'hydrogen production', 'question': 'What is a possible alternative to making carbon-based fuels from reduction of carbon dioxide?'}, '5709a2e5200fba1400368202': {'truth': 'hardware-based or assisted computer security', 'predicted': '', 'question': 'What offers an alternative to soft-ware only computer security?'}, '5709a2e5200fba1400368204': {'truth': 'physical access (or sophisticated backdoor access)', 'predicted': 'physical access', 'question': 'What is required in order for hardware to be compromised?'}, '5a55606b134fea001a0e1a66': {'truth': '', 'predicted': 'hardware-based or assisted', 'question': 'What step can increase computer security?'}, '5a55606b134fea001a0e1a67': {'truth': '', 'predicted': 'sophisticated backdoor access', 'question': 'What is used in physical access of a computer?'}, '5a5cd75a5e8782001a9d5dde': {'truth': '', 'predicted': 'hardware-based or assisted computer security', 'question': 'What is an example of software-only computer security?'}, '572713ce708984140094d965': {'truth': 'a positional decimal system on counting boards', 'predicted': 'positional decimal system', 'question': 'What method did early Chinese mathematicians use to calculate?'}, '56fa2008f34c681400b0bfc7': {'truth': 'diffuse-porous', 'predicted': 'diffuse-porous woods', 'question': 'What kind of woods have pores that are uniformly sized?'}, '56defa45c65bf219000b3e71': {'truth': 'Eastern Catholic cardinals', 'predicted': '', 'question': 'Which Cardinals still wear traditional clothing?'}, '5ad31659604f3c001a3fdb4f': {'truth': '', 'predicted': 'scarlet', 'question': 'What color are Western style cassocks made entirely of?'}, '572fddea947a6a140053cd7e': {'truth': 'Ice Hockey Federation', 'predicted': 'International Ice Hockey Federation', 'question': 'What does IIHF stand for?'}, '5ad178b7645df0001a2d1d7b': {'truth': '', 'predicted': 'al-Qaida', 'question': 'Terrorists in the Koreas were linked to what larger terrorist organization?'}, '5ad178b7645df0001a2d1d7c': {'truth': '', 'predicted': 'the Sulu Archipelago', 'question': ' Where did terrorist fighting take place?'}, '5ad178b7645df0001a2d1d7d': {'truth': '', 'predicted': '2,000', 'question': ' How many troops did the US initially send to the Koreas?'}, '5731417005b4da19006bcf61': {'truth': 'Cyprian', 'predicted': '', 'question': 'Who became metropolitan in 1370?'}, '5ad0185377cf76001a6869b1': {'truth': '', 'predicted': '1238–1264', 'question': 'When did Prince Daniil rule?'}, '56f8c2519e9bad19000a0448': {'truth': 'City of Southampton Sunday Football League', 'predicted': 'City of Southampton Sunday Football League and the Southampton and District Sunday Football League', 'question': 'What\\'s the name of the local league with \"City\" in its name?'}, '56e136e7cd28a01900c676bb': {'truth': 'More than two-thirds', 'predicted': 'two-thirds', 'question': 'How much of Inner Bostons land area did not exist when the city was founded?'}, '56e136e7cd28a01900c676be': {'truth': 'the Back Bay', 'predicted': 'Back Bay', 'question': 'A train full of gravel came from Needham to fill what?'}, '56e136e7cd28a01900c676bf': {'truth': 'the Back Bay', 'predicted': 'Back Bay', 'question': 'The Boston Public Library is located in what part of Boston?'}, '56e78bb537bdd419002c4108': {'truth': 'admiral Zheng He', 'predicted': 'Zheng He', 'question': 'What Admiral called Nanjing his home?'}, '56e78bb537bdd419002c410b': {'truth': 'a tortoise stele', 'predicted': 'tortoise', 'question': \"What stele is at Boni's tomb?\"}, '56df6e5156340a1900b29b2c': {'truth': 'Oklahoma Secondary School Activities Association', 'predicted': 'The Oklahoma Secondary School Activities Association', 'question': 'What organization organizes High School football?'}, '5a6b5ebba9e0c9001a4e9f50': {'truth': '', 'predicted': 'thrombolysis', 'question': 'What medicine is used to remove blockages?'}, '5a6b5ebba9e0c9001a4e9f52': {'truth': '', 'predicted': 'heparin', 'question': 'What blood thinner is used in ST elevation treatments?'}, '56cfb1a2234ae51400d9be85': {'truth': 'half million', 'predicted': 'over half million', 'question': 'About how many students attend schools in the City University of New York system?'}, '56cfb1a2234ae51400d9be86': {'truth': 'three out of five', 'predicted': 'three', 'question': 'What fraction of Manhattan residents graduated from college?'}, '56cfb1a2234ae51400d9be89': {'truth': '600,000', 'predicted': 'Over 600,000', 'question': 'How many students in New York partcipate in higher education?'}, '5731e07b0fdd8d15006c65e6': {'truth': 'are not recognised by the state', 'predicted': '', 'question': \"What recognition do Baha'i and Hmadi community get from Egyptian government?\"}, '5731e07b0fdd8d15006c65e7': {'truth': '2008', 'predicted': '', 'question': 'Until what year did some minorities need to lie about religion or not get mandatory state issued ID?'}, '570fe7425ab6b819003910b3': {'truth': 'Wisconsin', 'predicted': '', 'question': 'What state has had only a single execution?'}, '5ad3f120604f3c001a3ff85e': {'truth': '', 'predicted': '1978', 'question': 'In what year did an Oregon referendum succeed in restoring the death penalty, only to be passed due to a court ruling?'}, '5ad3f120604f3c001a3ff85f': {'truth': '', 'predicted': 'Rhode Island', 'question': 'What state notably abolished the death penalty and then reintroduced it, and used it frequently?'}, '57267ab2dd62a815002e868b': {'truth': 'Ant-Man', 'predicted': '', 'question': 'What Marvel hero is related to a tiny insect?'}, '5a5fa27aeae51e001ab14b67': {'truth': '', 'predicted': 'Chicago', 'question': 'What fictional universe was Los Angeles based on?'}, '5a5fa27aeae51e001ab14b68': {'truth': '', 'predicted': 'Doctor Doom, Magneto, Galactus, Loki, the Green Goblin, and Doctor Octopus', 'question': \"Who was Daredevil's main antagonist?\"}, '5726fd98708984140094d7d2': {'truth': \"Al-Shifa'\", 'predicted': \"Al-Shifa' (Sanatio)\", 'question': \"What is the name of Avicenna's larger encyclopedic treatise? \"}, '5726fd98708984140094d7d3': {'truth': 'the Bodleian Library', 'predicted': 'Bodleian Library', 'question': \"Where is Avicenna's Al-Shifa manuscript located?\"}, '5726fd98708984140094d7d4': {'truth': 'the An-najat', 'predicted': 'An-najat', 'question': 'What is the shorter form of Al-Shifa called?'}, '5726fd98708984140094d7d5': {'truth': 'Logic and Metaphysics', 'predicted': '', 'question': \"What two subjects of Avicenna's have been reprinted extensively?\"}, '5aceaec232bba1001ae4b02a': {'truth': '', 'predicted': 'An-najat', 'question': 'What is the longer form of Al-Shifa called?'}, '572819473acd2414000df487': {'truth': 'downtown core', 'predicted': 'most of the downtown core', 'question': 'What was the most affected area of The Great Fire of 1892?'}, '5a6271a6f8d794001af1bfdc': {'truth': '', 'predicted': 'residential and other wood-frame buildings', 'question': 'What type of buildings date to the Industrial revolution?'}, '5726ee62dd62a815002e9582': {'truth': 'true theories', 'predicted': 'theories', 'question': 'What replaced false beliefs?'}, '5726ee62dd62a815002e9583': {'truth': 'Thomas Kuhn', 'predicted': '', 'question': 'Who depicts the history of science in a wider matrix?'}, '5726a7b4dd62a815002e8c24': {'truth': 'The liberal-communitarian debate', 'predicted': 'liberal-communitarian debate', 'question': 'What is often considered valuable for generating a new set of philosophical problems? '}, '5726a7b4dd62a815002e8c26': {'truth': 'policies which encourage the growth of social capital', 'predicted': 'greater local control as well as economic and social policies which encourage the growth of social capital', 'question': 'What type of economic and social policies do Communitarians tend to support?'}, '570fa4675ab6b81900390f5d': {'truth': 'it inappropriately measures heterosexuality and homosexuality', 'predicted': '', 'question': 'What was a concern of the Kinsey scale?'}, '570fa4675ab6b81900390f5f': {'truth': 'they act as tradeoffs such, whereby to be more feminine one had to be less masculine and vice versa', 'predicted': '', 'question': 'What happens if the concepts are measured on the same scale?'}, '570fa4675ab6b81900390f60': {'truth': 'the degree of heterosexual and homosexual can be independently determined, rather than the balance between heterosexual and homosexual', 'predicted': '', 'question': 'What is the advantage of measuring these elements separately?'}, '570ffc01a58dae1900cd67ae': {'truth': 'inappropriately measures heterosexuality and homosexuality on the same scale', 'predicted': 'inappropriately measures heterosexuality and homosexuality', 'question': 'What is considered to be a problem with the Kinsey scale?'}, '570ffc01a58dae1900cd67af': {'truth': 'more appropriately measured as independent concepts on a separate scale rather than as a single continuum', 'predicted': 'more appropriately measured as independent concepts on a separate scale', 'question': 'What did the research performed in the 1970s show about masculinity and feminity?'}, '570ffc01a58dae1900cd67b0': {'truth': 'would allow one to be both very heterosexual and very homosexual or not very much of either.', 'predicted': '', 'question': 'What would be possible if homesexuality and heterosexuality where measured on different scales?'}, '570ffc01a58dae1900cd67b1': {'truth': 'the degree of heterosexual and homosexual can be independently determined,', 'predicted': '', 'question': 'What is another benefit of measuring sexuality on two scaless verses just the Kinsey scale?'}, '5728c1414b864d1900164d5b': {'truth': 'Iatromantis', 'predicted': '', 'question': 'Which epithet did Apollo have as god of healing and of prophecy?'}, '5726a1d5708984140094cc64': {'truth': 'political', 'predicted': '', 'question': 'The Annual Register covered international events of what type?'}, '5726a1d5708984140094cc65': {'truth': '1766', 'predicted': '', 'question': 'Burke was the only known writer for the Register until what year?'}, '5ad0b591645df0001a2d0108': {'truth': '', 'predicted': '1789', 'question': 'When did Burke become the chief editor?'}, '57320ba7e99e3014001e647a': {'truth': 'Minerva', 'predicted': 'Jupiter, Juno and Minerva', 'question': 'What goddess became a part of the Capitoline triad?'}, '56f71740711bf01900a44933': {'truth': 'It is possible', 'predicted': 'It is possible, however, for a bilateral treaty to have more than two parties', 'question': 'Is it possible for a bilateral treaty to have more than two parties?'}, '56f71740711bf01900a44934': {'truth': 'the European Economic Area agreement', 'predicted': 'European Economic Area agreement', 'question': 'The bilateral treaties between Switzerland and the European Union followed the Swiss rejection of what?'}, '57278fc7dd62a815002ea071': {'truth': 'familiarize privileged schoolboys with social conditions in deprived areas', 'predicted': 'to familiarize privileged schoolboys with social conditions in deprived areas', 'question': 'What was the purpose for creating an Eton Mission?'}, '57278fc7dd62a815002ea072': {'truth': 'it was decided that a more local project (at Dorney) would be more realistic', 'predicted': '', 'question': 'Why did construction of Eton Mission cease in 1971?'}, '57278fc7dd62a815002ea074': {'truth': 'district of Hackney Wick in east London', 'predicted': 'Hackney Wick', 'question': 'Where was the Eton Mission originally to be located?'}, '56e02a437aa994140058e2de': {'truth': 'as albums', 'predicted': 'directly as albums', 'question': 'How were comics published when serialization became less common?'}, '56e02a437aa994140058e2df': {'truth': \"L'Association\", 'predicted': '', 'question': 'Which small publisher published in formats that were not traditional?'}, '5acf875d77cf76001a685104': {'truth': '', 'predicted': 'directly as albums', 'question': 'How were comics published when serialization became more common?'}, '5acf875d77cf76001a685106': {'truth': '', 'predicted': 'shrinking print market', 'question': 'Comics continue to decline regardless of the decrease in what market?'}, '56fb879b8ddada1400cd64d1': {'truth': 'the jacquerie', 'predicted': 'jacquerie', 'question': 'What popular uprising occurred in France during this period?'}, '56fb879b8ddada1400cd64d2': {'truth': \"the Peasants' Revolt\", 'predicted': \"Peasants' Revolt\", 'question': 'What English popular revolt took place during this period?'}, '572e8578c246551400ce42ba': {'truth': 'The Dutch Republic, long-time British ally, kept its neutrality intact, fearing the odds against Britain and Prussia fighting the great powers of Europe', 'predicted': '', 'question': 'Why did Denmark-Norway remain neutral rather than assisting its longtime ally, Britain?'}, '572e8578c246551400ce42bb': {'truth': 'the taxation of salt and alcohol begun by Empress Elizabeth in 1759 to complete her addition to the Winter Palace.', 'predicted': 'to complete her addition to the Winter Palace', 'question': 'What did Russian Empress Elizabeth use the proceeds of the tax on salt and alcohol for?'}, '572e8578c246551400ce42bc': {'truth': 'Naples, Sicily, and Savoy, although sided with Franco-Spanish party, declined to join the coalition under the fear of British power', 'predicted': '', 'question': 'Why did Naples remain neutral?'}, '572e8578c246551400ce42bd': {'truth': 'Sicily, and Savoy, although sided with Franco-Spanish party', 'predicted': 'Franco-Spanish party', 'question': 'Who would Sicily and Savoy normally align with?'}, '57326cefe99e3014001e67a7': {'truth': 'comedy', 'predicted': \"Bronx native Nancy Savoca's 1989 comedy\", 'question': \"What genre was 'True Love'?\"}, '5ad155de645df0001a2d17e2': {'truth': '', 'predicted': 'Paramount', 'question': 'What company did B. Hal Wallis work at before Universal?'}, '5ad155de645df0001a2d17e3': {'truth': '', 'predicted': 'Universal', 'question': 'What company did B. Hal Wallis work at after Paramount?'}, '5ad155de645df0001a2d17e5': {'truth': '', 'predicted': '1971', 'question': 'What year did Anne, Queen of Scots come out?'}, '5ad155de645df0001a2d17e6': {'truth': '', 'predicted': 'Rooster Cogburn', 'question': 'What 1969 film was a sequel to True Grit?'}, '5726c709f1498d1400e8eb02': {'truth': 'In the 1990s', 'predicted': '1990s', 'question': 'When did Taiwanese Hokkien have a fast change in development?'}, '5726c709f1498d1400e8eb04': {'truth': '2001', 'predicted': '1993, Taiwan became the first region in the world to implement the teaching of Taiwanese Hokkien in Taiwanese schools. In 2001', 'question': 'What year did it become mandatory to teach Hokkien in Taiwan schools?'}, '5a1e1aa53de3f40018b264cb': {'truth': '', 'predicted': 'A number of universities', 'question': 'Who offers poetry or literature courses for training Hokkien-fluent talents?'}, '5a1e1aa53de3f40018b264cc': {'truth': '', 'predicted': 'all schools in Taiwan', 'question': 'Where was Tai-lo extended to in 2001?'}, '5a82122f31013a001a3351a5': {'truth': '', 'predicted': 'sounds', 'question': 'What are assigned to phonemes by different languages?'}, '5a82122f31013a001a3351a6': {'truth': '', 'predicted': 'brain', 'question': 'What part of a human does allophone processing?'}, '5a82122f31013a001a3351a7': {'truth': '', 'predicted': 'allophones', 'question': 'The phonetical similarity of what thing causes disagreements between phenomes?'}, '572837e7ff5b5019007d9f47': {'truth': 'in the aftermath of the Soviet Union', 'predicted': 'aftermath of the Soviet Union', 'question': \"When was Russia's subdivision liberalized?\"}, '5acfafea77cf76001a685874': {'truth': '', 'predicted': 'Vladimir Putin', 'question': 'Who escalted some reforms from Yeltsin? '}, '57240e580a492a1900435608': {'truth': 'average of 2,500', 'predicted': '2,500', 'question': 'How many words a day did Victoria write?'}, '57240e580a492a190043560c': {'truth': 'transcribed and edited', 'predicted': '', 'question': 'What did Beatrice do to her mothers diaries after her death?'}, '5725677c69ff041400e58c6a': {'truth': '122', 'predicted': '122 volumes', 'question': 'How many journals did Queen Victoria write in her lifetime?'}, '5725677c69ff041400e58c6c': {'truth': 'burned the originals', 'predicted': \"burned the originals in the process. Despite this destruction, much of the diaries still exist. In addition to Beatrice's edited copy, Lord Esher transcribed the volumes from 1832 to 1861 before Beatrice destroyed\", 'question': 'What did Beatrice do with the journals after she transcribed and edited them? '}, '57267d5c5951b619008f7483': {'truth': 'Victoria wrote an average of 2,500 words a day', 'predicted': '', 'question': 'How avid of a writer was the Queen?'}, '57267d5c5951b619008f7484': {'truth': '122 volumes', 'predicted': '122', 'question': 'How many volumes did her journal span?'}, '57267d5c5951b619008f7485': {'truth': 'her youngest daughter, Princess Beatrice', 'predicted': 'Princess Beatrice', 'question': \"Who was Victoria's literary executer?\"}, '57267d5c5951b619008f7486': {'truth': 'transcribed and edited', 'predicted': '', 'question': \"What did Beatrice do with her mother's journals?\"}, '5ad17f1f645df0001a2d1e4a': {'truth': '', 'predicted': '2,500', 'question': 'How many words a day did Victoria read?'}, '5ad17f1f645df0001a2d1e4d': {'truth': '', 'predicted': 'Princess Beatrice', 'question': 'Who was Victorias oldest daughter?'}, '5726d4705951b619008f7f58': {'truth': 'sixteen enterprises', 'predicted': '', 'question': 'How many enterprises have been made completely private since the agreement?'}, '5730af4a069b53140083224b': {'truth': 'original sin only', 'predicted': '', 'question': 'What is the Immaculate Conception a representation of the avoidance of ?'}, '5730af4a069b53140083224c': {'truth': 'Mary was preserved from any stain (in Latin, macula or labes', 'predicted': '', 'question': 'What was Mary prevented from having to endure ?'}, '5730af4a069b53140083224d': {'truth': 'privilege granted by Almighty God, in view of the merits of Jesus Christ, the Saviour of the human race, was preserved free from all stain', 'predicted': '', 'question': 'Who was believed to have prevented this from occurring to Mary ?'}, '5730af4a069b53140083224e': {'truth': 'Jesus Christ, the Saviour of the human race, was preserved free from all stain of original sin.\"', 'predicted': '', 'question': 'What was the outcome of preventing Mary from having to endure such an injustice ?'}, '5730af4a069b53140083224f': {'truth': 'sanctifying grace that would normally come with baptism after birth.', 'predicted': '', 'question': 'What normally followed the delivery of a child by a woman in Mary time period ?'}, '5a2716bbc93d92001a4003cd': {'truth': '', 'predicted': 'sanctifying grace', 'question': 'What did Mary receive at baptism?'}, '572a4e22fed8de19000d5b8a': {'truth': 'cause of this type of diarrhea is a cholera toxin that stimulates the secretion of anions, especially chloride ions', 'predicted': 'cholera toxin', 'question': 'What are the causes of secretory diarrhea?'}, '572a4e22fed8de19000d5b8b': {'truth': '. There is little to no structural damage.', 'predicted': 'little to no', 'question': 'Is there any structural damage associated with secretory diarrhea?'}, '5a0dfd6ed7c85000188644b7': {'truth': '', 'predicted': 'little to no', 'question': 'Is there any structural damage associated with oral food intake?'}, '5a0dfd6ed7c85000188644b9': {'truth': '', 'predicted': 'cholera toxin', 'question': 'What stimulates the secretion of chloride plasma?'}, '570b27b4ec8fbc190045b894': {'truth': 'Game Room', 'predicted': 'Game Room to Xbox Live', 'question': 'What is the name of the online virtual arcade that launched in 2010?'}, '57284af44b864d19001648d2': {'truth': 'Kashmiri militants', 'predicted': 'Kashmiri militants, who were now fighting NATO in support of Al-Qaeda', 'question': 'After 2009, who began fighting in Waziristan?'}, '57284af44b864d19001648d3': {'truth': 'Hizbul Mujahideen', 'predicted': '', 'question': 'What group did Al-Badar Mujahideen break away from?'}, '5a85e99bb4e223001a8e72d5': {'truth': '', 'predicted': 'Ilyas Kashmiri', 'question': 'Who ordered the SUS Drone strike in September 2009?'}, '5728be814b864d1900164d3f': {'truth': 'The Baltic Council', 'predicted': '', 'question': 'What is the combined group of the interparliamentary Baltic Assembly and the intergovernmental Baltic Council of Ministers?'}, '572805304b864d1900164251': {'truth': '1963 from 12 May to 29 June', 'predicted': '1963', 'question': 'When was the next session scheduled?'}, '572805304b864d1900164253': {'truth': 'Pope Pius IX', 'predicted': 'Pius IX', 'question': 'Who did John XXIII wish to see canonized?'}, '5a611100e9e1cc001a33ce89': {'truth': '', 'predicted': '1959', 'question': 'When was Pope Pius IX at a spiritual retreat?'}, '5a611100e9e1cc001a33ce8a': {'truth': '', 'predicted': 'holy and glorious memory', 'question': 'When thinking of John XXIII, what did Pope Pius IX want to be worthy of?'}, '5a611100e9e1cc001a33ce8b': {'truth': '', 'predicted': 'holy and glorious', 'question': 'How did Pope Pius IX remember John XXIII as being?'}, '56fdfbee19033b140034ce25': {'truth': 'more slowly,', 'predicted': 'more slowly', 'question': 'Multitasking would seemingly cause a computer to run in what fashion?'}, '56fdfbee19033b140034ce26': {'truth': 'input/output devices', 'predicted': 'slow input/output devices', 'question': 'What do a lot of programs spend time waiting for?'}, '572fefcb947a6a140053ce2e': {'truth': '101 BC', 'predicted': '113–101 BC', 'question': 'When did the Cimbrian War end?'}, '572fefcb947a6a140053ce30': {'truth': 'The Cimbrian War', 'predicted': 'Cimbrian War', 'question': 'What war began in the year 113 BC?'}, '572fefcb947a6a140053ce31': {'truth': 'northern Europe', 'predicted': \"France), both of which they defeated with apparent ease. The Cimbrian War (113–101 BC) was a far more serious affair than the earlier clashes of 121 BC. The Germanic tribes of the Cimbri and the Teutons migrated from northern Europe into Rome's northern territories, and clashed with Rome and her allies. At the Battle of Aquae Sextiae\", 'question': 'Where did the tribes that were almost annihilated in the Battle of Vercellae hail from?'}, '57288f642ca10214002da46f': {'truth': '45 percent of the student body at BYU has been missionaries for LDS Church', 'predicted': 'missionaries for LDS Church', 'question': \"What can be attributed to BYU's high percentage of second language proficient students?\"}, '57288f642ca10214002da471': {'truth': 'largest of their kind in the nation', 'predicted': '', 'question': \"What designation does BYU's Russian language program hold?\"}, '5acea68932bba1001ae4aeef': {'truth': '', 'predicted': 'missionaries', 'question': 'What have 60% of the student body done for the LDS Church?'}, '5acea68932bba1001ae4aef0': {'truth': '', 'predicted': 'second language', 'question': 'Over one-third of the student body has some proficiency in what?'}, '5acea68932bba1001ae4aef1': {'truth': '', 'predicted': 'foreign language classes', 'question': 'What are three quarters of students enrolled in during any given semester?'}, '5706b4af0eeca41400aa0d5c': {'truth': '\"No UFOs\"', 'predicted': 'Model 500 \"No UFOs\"', 'question': 'what hit single did atkins release in 1985?'}, '5706b4af0eeca41400aa0d5d': {'truth': '\"Strings of Life\"', 'predicted': 'Strings of Life', 'question': 'what unusual single did derrick may release, featuring a darker strain of house?'}, '5706b4af0eeca41400aa0d5e': {'truth': 'Techno-Scratch', 'predicted': '', 'question': 'what did knights of the turntable release in 1984?'}, '5ad28d6fd7d075001a429a2a': {'truth': '', 'predicted': 'Cybotron', 'question': 'May was a former member of what music group?'}, '5ad28d6fd7d075001a429a2b': {'truth': '', 'predicted': 'Model 500 \"No UFOs\"', 'question': 'What hit single did May release in 1985?'}, '5ad28d6fd7d075001a429a2c': {'truth': '', 'predicted': 'Strings of Life', 'question': 'What unusual single did Derrick Atkins release, featuring a darker strain of house?'}, '5ad28d6fd7d075001a429a2e': {'truth': '', 'predicted': 'Tony Wilson', 'question': 'Who was the manager of the factory nightclub and co-owner of The Hummingbird?'}, '57267786dd62a815002e85f4': {'truth': 'two days', 'predicted': 'two', 'question': 'How many days a week does the city encourage people to go without a car?'}, '57267786dd62a815002e85f5': {'truth': 'ozone and nitrogen oxides', 'predicted': '', 'question': 'What pollutants does the city closely monitor?'}, '572814c34b864d190016441f': {'truth': '2007', 'predicted': '', 'question': 'What year was Warhawk released for the PlayStation 3?'}, '572814c34b864d1900164421': {'truth': 'Gran Turismo 5 Prologue', 'predicted': '', 'question': 'What Gran Turismo game was shown in 2007 but not released until after 2007?'}, '5ad2a7a2d7d075001a429e12': {'truth': '', 'predicted': \"Drake's Fortune\", 'question': 'Which Uncharted title debuted at E3 2008?'}, '5ad33f45604f3c001a3fdbc1': {'truth': '', 'predicted': \"Heavenly Sword, Lair, Ratchet & Clank Future: Tools of Destruction, Warhawk and Uncharted: Drake's Fortune\", 'question': 'Which Ratchet & Clank title debuted at E2 2007?'}, '5ad33f45604f3c001a3fdbc5': {'truth': '', 'predicted': 'Metal Gear Solid 4: Guns of the Patriots', 'question': 'Which much anticipated third-party game with the name of a month of the year in it did Sony show at E4 2007?'}, '57303815947a6a140053d2c8': {'truth': '25 commercial films', 'predicted': '25', 'question': 'How many commercial films were produced yearly on average in the early 1960s in Iran?'}, '56e0758e7aa994140058e503': {'truth': 'hydrogen chloride and hydrogen fluoride', 'predicted': 'hydrogen halides, hydrogen chloride and hydrogen fluoride', 'question': 'What are two other dangerous acids?'}, '56e0758e7aa994140058e505': {'truth': 'room temperature', 'predicted': '', 'question': 'What temperature does hydrogen react with these elements?'}, '56cf7f014df3c31400b0d85b': {'truth': 'Obama', 'predicted': 'President Obama', 'question': 'What president did Kanye comment on as having trouble pushing policies while in office?'}, '570e08690dc6ce1900204d97': {'truth': 'Antarctic fur seal', 'predicted': 'The Antarctic fur seal', 'question': 'What animal was greatly hunted during the 18th and 19th centuries?'}, '5ad2623cd7d075001a42908a': {'truth': '', 'predicted': 'Sir James Weddell', 'question': 'What whaler had a seal named after him?'}, '5ad2c367d7d075001a42a134': {'truth': '', 'predicted': 'The Weddell seal', 'question': 'What is named after James Sir Weddell?'}, '5ad2c367d7d075001a42a135': {'truth': '', 'predicted': 'British sealing expeditions', 'question': 'What was James Sir Weddell commander of?'}, '5725cc5938643c19005acd27': {'truth': 'the government of Montevideo', 'predicted': 'government of Montevideo', 'question': 'Who owns The Solis Theater?'}, '5731bb10b9d445190005e4d0': {'truth': 'fervently Catholic', 'predicted': 'Catholic', 'question': \"What was the Duke of York's relationship to his religion described as being?\"}, '5ad141b5645df0001a2d13fb': {'truth': '', 'predicted': 'no religious test', 'question': 'What did the Province of West Jersey specify there would be for those running for an office, in 1681?'}, '5ad141b5645df0001a2d13fc': {'truth': '', 'predicted': '1799', 'question': 'When was an oath requiring militia to abjure the pretensions of the pope not replaced?'}, '572749fdf1498d1400e8f5ab': {'truth': 'the cotton gin', 'predicted': 'cotton gin', 'question': 'What invention, for which he is primarily known, did Eli Whitney develop in New Haven?'}, '572749fdf1498d1400e8f5ad': {'truth': '\"The Arsenal of America\"', 'predicted': 'The Arsenal of America', 'question': 'What was the nickname given to Connecticut due to the large number of arms manufacturers that arose in the state? '}, '572948091d0469140077924b': {'truth': 'A. C. Gilbert', 'predicted': 'A. C. Gilbert Company', 'question': 'What was the company that was responsible Connecticut rise as a manufacturing economy? '}, '572a329aaf94a219006aa886': {'truth': 'hunter-gatherer communities', 'predicted': '', 'question': 'What type of societies were not affected by famine?'}, '5a7d388a70df9f001a875019': {'truth': '', 'predicted': 'agrarian communities', 'question': 'What type of societies were usually still successful after dealing with cultivation?'}, '57098140200fba14003680cb': {'truth': 'suicide attempts', 'predicted': '', 'question': 'What are copper salts sometimes used for?'}, '57098140200fba14003680cc': {'truth': 'copper toxicity', 'predicted': 'acute copper toxicity', 'question': 'When ingested in large amounts what does copper salts produce in humans?'}, '57098140200fba14003680ce': {'truth': 'growth rates', 'predicted': 'influence feed conversion efficiency, growth rates, and carcass dressing percentages', 'question': 'What is a major benefit to rabbits having a higher concentration of copper in their diet?'}, '5a8371c6e60761001a2eb71c': {'truth': '', 'predicted': 'acute copper toxicity', 'question': 'When ingested in small amounts what does copper salts produce in humans?'}, '5a8371c6e60761001a2eb71d': {'truth': '', 'predicted': '3 ppm', 'question': 'What is the minimum amount of copper dragons should have in their diet?'}, '572771b85951b619008f8a07': {'truth': 'tetraploid cotton', 'predicted': '', 'question': 'What is the final sequencing goal of sequencing diploid cotton genomes first ?'}, '572771b85951b619008f8a0a': {'truth': 'diploid', 'predicted': '', 'question': 'What type of genome must be sequenced first to prevent confusion before the tetraploid form?'}, '5a81a8b031013a001a334d32': {'truth': '', 'predicted': 'tetraploid', 'question': 'What type of cotton has two separate genomes within its sequence?'}, '5a81a8b031013a001a334d35': {'truth': '', 'predicted': 'retrotransposons', 'question': 'In order to understand the tetraploid forms, what must be used as a comparison in cotton GORGE?'}, '57260e5b271a42140099d406': {'truth': 'Sophists', 'predicted': 'The Sophists', 'question': 'Who declared the centrality of humanity and agnosticism?'}, '57260e5b271a42140099d409': {'truth': 'down to earth', 'predicted': 'earth', 'question': 'Rulers brought the concept of divinity to where?'}, '5727ec41ff5b5019007d988e': {'truth': 'there was no industry standard', 'predicted': '', 'question': 'What was the industry standard in Europe for record equalization?'}, '5727ec41ff5b5019007d988f': {'truth': 'US', 'predicted': '', 'question': 'In which country was the treble roll off greater?'}, '5727ec41ff5b5019007d9891': {'truth': 'there was no industry standard', 'predicted': '', 'question': 'What was the industry standard on equalization practices?'}, '573036c4947a6a140053d2b4': {'truth': 'helical antenna', 'predicted': '', 'question': 'What antenna does not have linear polarization?'}, '573036c4947a6a140053d2b5': {'truth': 'resistor', 'predicted': '', 'question': 'What are undirectional traveling wave directions terminated by?'}, '573036c4947a6a140053d2b6': {'truth': \"antenna's characteristic resistance\", 'predicted': '', 'question': 'What is the resistor equal to?'}, '57283ac5ff5b5019007d9f8e': {'truth': 'Democratic-Republicans', 'predicted': '', 'question': 'What was the federalist party of the United States opposed to?'}, '57283ac5ff5b5019007d9f8f': {'truth': 'the Legislature had too much power (mainly because of the Necessary and Proper Clause) and that they were unchecked', 'predicted': '', 'question': 'What did the democratic-republican party believe in?'}, '5acfb20177cf76001a68590c': {'truth': '', 'predicted': 'the judicial system of courts', 'question': 'Who decided the rights in all cases?'}, '5725cedc38643c19005acd5a': {'truth': 'Chapman', 'predicted': '', 'question': 'What football club manager got an underground station renamed for Arsenal?'}, '5acd005507355d001abf3158': {'truth': '', 'predicted': 'Herbert Chapman', 'question': \"What was the name of one of Arsenal's coaches prior to 1925?\"}, '5acd005507355d001abf315b': {'truth': '', 'predicted': 'Herbert Chapman', 'question': 'Who named the team Arsenal?'}, '5728d7c4ff5b5019007da7f5': {'truth': 'from $5,200 to $6,600', 'predicted': '$5,200 to $6,600', 'question': 'How much was the average cost of hospital stays for asthma-related issues for adults?'}, '5a503d98ce860b001aa3fb1d': {'truth': '', 'predicted': '$5,200 to $6,600', 'question': 'What was the average cost per hospital stay for high income communites in 2010?'}, '5a503d98ce860b001aa3fb1e': {'truth': '', 'predicted': '$5,200 to $6,600', 'question': 'How much did the cost increase for a child to be hospitalized for asthma in 2000?'}, '5ad00faf77cf76001a686846': {'truth': '', 'predicted': 'the early 1990s', 'question': 'Since when has Congo-Brazzaville has a single party political system?'}, '5ad00faf77cf76001a686847': {'truth': '', 'predicted': 'Denis Sassou Nguesso', 'question': 'Who has serious competition in the presidential elections?'}, '5ad00faf77cf76001a686848': {'truth': '', 'predicted': 'Parti Congolais du Travail', 'question': 'What is the Congolese Labour Party called in German?'}, '57265bdaf1498d1400e8dd1c': {'truth': 'English descent and Americans of Scots-Irish descent began moving into northern Florida from the backwoods of Georgia and South Carolina', 'predicted': '', 'question': 'Where did English and Scotch Irish descent move to Florida from '}, '57265bdaf1498d1400e8dd1d': {'truth': 'Florida Crackers', 'predicted': '', 'question': 'Backwoods settlers of Northern Florida are known as '}, '5acd893b07355d001abf4636': {'truth': '', 'predicted': 'the border region', 'question': 'Where did the Spanish patrol well?'}, '572a229a3f37b31900478721': {'truth': 'the plague and famine', 'predicted': '', 'question': \"What wiped out one third of East Prussia's population during the early 1700's?\"}, '572a229a3f37b31900478722': {'truth': 'speakers of Old Prussian', 'predicted': 'Old Prussian', 'question': \"What was lost in Prussia's history during the Plague?\"}, '572a229a3f37b31900478723': {'truth': 'Russian troops', 'predicted': '', 'question': 'What military overran much of East Prussia?'}, '5728283a2ca10214002d9f71': {'truth': 'ridges whose tips bear hooked chetae', 'predicted': '', 'question': 'What kind of parapodia do burrowing annelids often have?'}, '570cff2cb3d812140066d393': {'truth': 'Virgin Mary', 'predicted': 'the prophecy of the Virgin Mary', 'question': 'Who do Christians believe is prophesized in Isaiah 7:14?'}, '570cff2cb3d812140066d394': {'truth': 'until', 'predicted': 'until) denotes a state up to a point', 'question': 'What is the meaning of the Greek word \"heos?\"'}, '570cff2cb3d812140066d397': {'truth': 'virgin', 'predicted': '', 'question': 'What is the English tranlation of the Greek word \"parthenos?\"'}, '5ad18f44645df0001a2d1f4e': {'truth': '', 'predicted': 'Joseph', 'question': 'Who knew Mary before she brought forth her first born son?'}, '5ad18f44645df0001a2d1f50': {'truth': '', 'predicted': 'Galilee', 'question': 'Where did Matthew and Luke gather with the disbelieving Jews?'}, '5ad18f44645df0001a2d1f51': {'truth': '', 'predicted': 'Bart Ehrman', 'question': 'What Biblical scholar currently believes that Mary was a perpetual virgin?'}, '5ad18f44645df0001a2d1f52': {'truth': '', 'predicted': 'Hebrew', 'question': 'What language does Biblical scholar Bart Ehrmam primarily speak?'}, '572ba8ea111d821400f38f3f': {'truth': 'disputes around whether the schools are affordable for the poor', 'predicted': 'whether the schools are affordable for the poor', 'question': 'What was the reason for the polarisation for affordable schooling?'}, '5acd809c07355d001abf449d': {'truth': '', 'predicted': 'excess demand', 'question': 'What is the failure attributed to?'}, '57062c2552bb89140068992c': {'truth': '1000 MB', 'predicted': '', 'question': 'What was the average high end hard drive size?'}, '56e0a41f7aa994140058e68d': {'truth': 'the Russian SFSR', 'predicted': 'Russian SFSR', 'question': 'What administrative division did Kaliningrad Oblast become a part of?'}, '5ace063c32bba1001ae49997': {'truth': '', 'predicted': 'East Prussia', 'question': 'What state of Germany did Kaliningrad Oblast form a part of?'}, '5ace063c32bba1001ae49998': {'truth': '', 'predicted': 'East Prussia', 'question': 'What province of Germany did Kaliningrad Oblast cease to be a part of?'}, '5ace063c32bba1001ae49999': {'truth': '', 'predicted': 'East Prussia', 'question': 'What administrative division did Kaliningrad Oblast leave?'}, '5726cc10f1498d1400e8eb80': {'truth': 'precision and speed limitations', 'predicted': '', 'question': 'Why were stepper motors abandoned in computer drive designs?'}, '5726cc10f1498d1400e8eb82': {'truth': 'perpendicular to the magnetic lines of force', 'predicted': '', 'question': 'In what direction do modern coil actuators move?'}, '56e8379037bdd419002c44a8': {'truth': 'logograms', 'predicted': 'logograms that do not always give hints to its pronunciation', 'question': 'From what did Chinese characters derive?'}, '56e8379037bdd419002c44a9': {'truth': 'Cantonese', 'predicted': '', 'question': 'What is another name for the Yue language?'}, '56e8379037bdd419002c44aa': {'truth': 'the south', 'predicted': 'south', 'question': 'Hakka is a language from what geographic part of China?'}, '5ad27b63d7d075001a429652': {'truth': '', 'predicted': 'alphabets', 'question': 'What do Chinese characters use to give hints to pronunciation?'}, '5ad27b63d7d075001a429654': {'truth': '', 'predicted': 'Hong Kong', 'question': 'Where is Macau the most commonly used language?'}, '5ad27b63d7d075001a429655': {'truth': '', 'predicted': 'Yue', 'question': 'Which language is the most commonly used in Cantonese?'}, '5ad27b63d7d075001a429656': {'truth': '', 'predicted': 'alphabets', 'question': 'What do most languages use to indicate varieties?'}, '57264be9dd62a815002e80bc': {'truth': 'Commercial turkeys are usually reared indoors under controlled conditions', 'predicted': 'indoors under controlled conditions', 'question': 'What type of accomidations are domesticated turkey normally grown in?'}, '57264be9dd62a815002e80be': {'truth': 'Females achieve slaughter weight at about 15 weeks of age and males at about 19', 'predicted': '', 'question': 'At what age is the average turkey considered ready for the initial step of the commercial food process?'}, '57264be9dd62a815002e80bf': {'truth': 'Mature commercial birds may be twice as heavy as their wild counterparts', 'predicted': 'twice as heavy', 'question': 'How much more does a average commercial  turkey weigh in comparison to its wild turkey cousins ?'}, '57264be9dd62a815002e80c0': {'truth': '60 million birds in the United States', 'predicted': '60 million', 'question': 'What the average for the amount of  turkeys are consumed in the U.S on Thanksgiving Day?'}, '5a860699b4e223001a8e73c3': {'truth': '', 'predicted': '60 million', 'question': 'What is the average for the amount of parrots that are consumed in the U.S on Thanksgiving Day?'}, '5a860699b4e223001a8e73c4': {'truth': '', 'predicted': 'controlled', 'question': 'What type of conditions are used to decrease the weight and profitability of commercial turkeys?'}, '5a860699b4e223001a8e73c5': {'truth': '', 'predicted': 'twice as heavy', 'question': 'How much more does a average commercial feather weigh in comparison to its wild turkey cousins ?'}, '5726fad05951b619008f840a': {'truth': 'MTV', 'predicted': 'MTV and music videos', 'question': 'Madonna used which TV company to help with her career?'}, '572e7f8003f98919007566de': {'truth': 'Ottoman Empire', 'predicted': 'the Ottoman Empire', 'question': 'Which empire frequently raided Cyprus during Venetian rule?'}, '57270821708984140094d8d1': {'truth': 'state', 'predicted': 'public-private partnership between the state’s largest university system and largest health insurer, Louisiana State Agricultural Center and Blue Cross and Blue Shield of Louisiana Foundation', 'question': 'Smart Bodies is an example of an initiative that was started by what level of government?'}, '57270821708984140094d8d2': {'truth': 'to promote nutrition literacy', 'predicted': 'nutrition literacy', 'question': 'What is the goal of Smart Bodies?'}, '570ff9cda58dae1900cd679d': {'truth': '6', 'predicted': '6 percent', 'question': 'About what percentage of capital convictions are overturned due to state collateral review?'}, '570ff9cda58dae1900cd679f': {'truth': 'ineffective assistance of counsel', 'predicted': '', 'question': 'What is an example of an issue that is raised in collateral review?'}, '5ad3f6d9604f3c001a3ffa03': {'truth': '', 'predicted': '53', 'question': ' In 2010, how many death sentences were never overturned due to reversals from courts or appeals?'}, '56de0abc4396321400ee2563': {'truth': 'Islam', 'predicted': 'Islamic Conquest of Persia', 'question': 'The proliferation of which religion had a profound effect on the development of Iranian languages?'}, '56de0abc4396321400ee2564': {'truth': 'Dari', 'predicted': '', 'question': \"What language displaced Middle Iranian as the court's official tongue?\"}, '56de0abc4396321400ee2565': {'truth': 'Saffarid', 'predicted': 'Saffarid dynasty', 'question': 'What was the first dynasty to use Dari?'}, '56de0abc4396321400ee2567': {'truth': 'Khorasan', 'predicted': 'eastern province of Khorasan', 'question': 'What area was the name Dari connected to by medieval Iranian thinkers?'}, '5a18e9499aa02b0018605f2a': {'truth': '', 'predicted': 'Khuzi', 'question': 'What was the official dialect of the royalty itself?'}, '5a81c94f31013a001a334eb4': {'truth': '', 'predicted': 'David Buffett', 'question': 'Who is the Chief Master of Norfolk Island?'}, '5a81c94f31013a001a334eb5': {'truth': '', 'predicted': 'a financial bailout from the federal government to cover significant debts', 'question': \"What reason did David Buffett give for Norfolk Island keeping its' tax-free status?\"}, '5a81c94f31013a001a334eb6': {'truth': '', 'predicted': 'July 1, 2016', 'question': 'When will income tax be removed by Norfolk Island?'}, '5a81c94f31013a001a334eb7': {'truth': '', 'predicted': 'social', 'question': 'What type of services were Norfolk Island inhabitants unable to receive after this announcement?'}, '5728f9a14b864d190016515c': {'truth': 'December 2014, Myanmar signed an agreement to set up its first stock exchange', 'predicted': '', 'question': 'What occurred in the winter of 2014 of significance for Myanmar ?'}, '5728f9a14b864d1900165160': {'truth': 'Yangon Stock Exchange (YSX) officially opened for business on Friday, March 25, 2016.', 'predicted': 'Friday, March 25, 2016', 'question': 'What day did the business that first rang a bell to begin in the winter of 2014 in Myanmar open its doors to customers?'}, '56cd796762d2951400fa65ed': {'truth': 'Peter Oppenheimer', 'predicted': '', 'question': 'Who was Chief Financial Officer of Apple in July of 2009?'}, '572904223f37b31900477f86': {'truth': 'Homo habilis', 'predicted': '', 'question': 'Members of what species populated parts of Africa in a relatively short time?'}, '56dddab666d3e219004dad2f': {'truth': 'the House of Burgundy and subsequently the House of Habsburg', 'predicted': 'House of Burgundy', 'question': 'The majority of the Low Countries were ruled by which houses?'}, '56dddab666d3e219004dad30': {'truth': 'Holy Roman Emperor Charles V', 'predicted': 'Charles V', 'question': 'Who issued the Pragmatic Sanction?'}, '56dddab666d3e219004dad32': {'truth': \"high taxes, persecution of Protestants by the government, and Philip's efforts to modernize and centralize the devolved-medieval government structures of the provinces\", 'predicted': 'high taxes', 'question': 'Why did the people of the Netherlands rise up against Philip II?'}, '5a11c08c06e79900185c354a': {'truth': '', 'predicted': '1568', 'question': 'When did Philip II conquer the Netherlands?'}, '5a1c89fcb4fb5d001871468c': {'truth': '', 'predicted': '1549', 'question': 'When did the House of Burgundy issue the Pragmatic Sanction?'}, '5a1c89fcb4fb5d001871468e': {'truth': '', 'predicted': '1568', 'question': 'When did the House of Burgundy revolt against Philip II?'}, '5a1c89fcb4fb5d001871468f': {'truth': '', 'predicted': 'high taxes', 'question': 'Name two reasons why the Low Countries turned against Philip II?'}, '5a1c89fcb4fb5d0018714690': {'truth': '', 'predicted': \"the Eighty Years' War\", 'question': 'What event started after the revolt let by King Philip II of Spain?'}, '5726dcbf708984140094d3fb': {'truth': 'Goodluck Jonathan', 'predicted': \"Goodluck Jonathan served as Nigeria's president till 16 April 2011, when a new presidential election in Nigeria was conducted. Jonathan\", 'question': 'Who won the 2011 election?'}, '57277434708984140094de03': {'truth': 'illegality of male conversion to Judaism', 'predicted': 'the illegality of male conversion to Judaism', 'question': 'Name one reason historians believe the conversion during the Roman era was limited in number and did not account for much of the Jewish population growth?'}, '5ace989e32bba1001ae4abe3': {'truth': '', 'predicted': 'Judaism', 'question': 'What religion was it legal to convert to in the Roman world from the mid-2nd century?'}, '5ace989e32bba1001ae4abe4': {'truth': '', 'predicted': 'Roman world', 'question': 'Where was it legal to convert to Judaism?'}, '5ace989e32bba1001ae4abe5': {'truth': '', 'predicted': 'halakhic requirement of circumcision', 'question': 'What made conversion easy in the Roman world?'}, '5ace989e32bba1001ae4abe6': {'truth': '', 'predicted': '96 CE', 'question': 'When was the Fiscus Judaicus expanded to also include Christians?'}, '5acd416807355d001abf3aa2': {'truth': '', 'predicted': 'Earthquakes', 'question': 'What releases stored elastic kinetic energy in rocks?'}, '572a4f507a1753140016ae93': {'truth': 'eliminating all remnants of German history', 'predicted': '', 'question': 'In the Soviet section to the north, what did they want to expel from their land?'}, '572a4f507a1753140016ae94': {'truth': 'names were replaced by new Russian names', 'predicted': '', 'question': 'What else happened in the northern part of East Prussia in the now Russian area?'}, '5a3c02eacc5d22001a521d25': {'truth': '', 'predicted': '1967', 'question': 'In what year was the new \"House of the Soviets\" completed?'}, '57342937d058e614000b6a66': {'truth': 'a potato-based stew that can be made from several types of fish', 'predicted': 'potato-based stew', 'question': 'What is caldeirada?'}, '57342937d058e614000b6a68': {'truth': 'arroz de sarrabulho (rice stewed in pigs blood) or the arroz de cabidela (rice and chickens meat stewed in chickens blood)', 'predicted': 'arroz de sarrabulho', 'question': 'What are two popular Northern Portugal dishes?'}, '57301519b2c2fd1400568828': {'truth': '0.0042 inches', 'predicted': '0.0042 inches (4.2 mils, 105 μm) thick', 'question': 'About how thick would a PCB layer be if it contained three oz. of copper?'}, '57301519b2c2fd140056882b': {'truth': 'external heat sinks', 'predicted': '', 'question': 'Where does the heat go when it leaves heavy copper-plated vias?'}, '5ace96bf32bba1001ae4ab4f': {'truth': '', 'predicted': 'IPC 2152', 'question': 'What is the standard for determining current-carrying capacity of printed circuit board trees?'}, '5ace96bf32bba1001ae4ab50': {'truth': '', 'predicted': 'current-carrying capacity of printed circuit board traces', 'question': 'IAC 2152 is the standard for what?'}, '57338497d058e614000b5c4c': {'truth': 'a series of regulatory proposals', 'predicted': '', 'question': 'What was introduced by President Barack Obama in June 2009?'}, '57338497d058e614000b5c4d': {'truth': 'consumer protection', 'predicted': 'consumer protection, executive pay, bank financial cushions or capital requirements', 'question': 'What was one of the items important to consumers that was addressed by the new regulatory proposals introduced in June 2009?'}, '57338497d058e614000b5c4e': {'truth': 'proprietary', 'predicted': 'proprietary trading', 'question': 'Regulations were proposed by Obama in January 2010 to limit the ability of banks to engage in which type trading?'}, '57277a72708984140094deb3': {'truth': 'Communist Party', 'predicted': 'Communist Party of the Soviet Union', 'question': 'What party did Gorbachev belong to?'}, '57277a72708984140094deb5': {'truth': '1990', 'predicted': '', 'question': 'When were opposition parties first allowed in the Soviet Union?'}, '57255c8a69ff041400e58c3a': {'truth': 'new antiseptic carbolic acid spray', 'predicted': 'antiseptic carbolic acid spray', 'question': \"What cutting edge treatment did Joseph Lister use to treat Queen Victoria's illness?\"}, '5ad17959645df0001a2d1db5': {'truth': '', 'predicted': 'Trafalgar Square', 'question': 'Where was the republica rally held that called for Vuictorias promotion?'}, '5ad17959645df0001a2d1db6': {'truth': '', 'predicted': 'Radical MPs', 'question': 'Who spoke in support of Victoria at the rally in Trafalgar Square?'}, '5725cfda38643c19005acd71': {'truth': 'the Bill & Melinda Gates Foundation Trust, which manages the endowment assets and the Bill & Melinda Gates Foundation', 'predicted': '', 'question': 'What two entities was the foundation split into in october 2016'}, '5725cfda38643c19005acd72': {'truth': \"spend all of [the Trust's] resources within 20 years after Bill's and Melinda's deaths\", 'predicted': 'within 20 years', 'question': 'When must the trust resources be spent'}, '5725cfda38643c19005acd73': {'truth': 'the proceeds from the Berkshire Hathaway shares he still owns at death are to be used for philanthropic purposes within 10 years', 'predicted': '', 'question': 'What does warren Buffet stipulate his berkshire hathaway  shares be used for in the 10 year period after his death '}, '5a0cfad6f5590b0018dab6d1': {'truth': '', 'predicted': '10 years', 'question': 'How long after the estate has been settled Will the Bill and Melinda gates foundation shares be used for philanthropy?'}, '56e83bdf37bdd419002c44ba': {'truth': 'avere', 'predicted': '', 'question': 'What Italian word is similar to the French word \"avoir\"?'}, '56e83bdf37bdd419002c44bb': {'truth': 'être', 'predicted': '', 'question': 'What French word is similar to the Italian word \"essere\"?'}, '5ad27222d7d075001a429487': {'truth': '', 'predicted': 'Italian and Spanish', 'question': 'Which two languages have undergone more change than French regarding phonological structures?'}, '5ad27222d7d075001a429488': {'truth': '', 'predicted': 'essere', 'question': 'What Spanish word is similar to the Italian word avere?'}, '5ad27222d7d075001a429489': {'truth': '', 'predicted': 'avere', 'question': 'Which Spanish word is similar to the Italian word essere?'}, '5ad27222d7d075001a42948a': {'truth': '', 'predicted': 'avere\" and \"essere', 'question': 'Which Italian term is similar to avoir?'}, '56f7165e3d8e2e1400e3733a': {'truth': '2nd', 'predicted': '2nd grade', 'question': 'What grade did he fail?'}, '57295b726aef051400154d56': {'truth': 'sailing, athletics, swimming, diving, triathlon and equestrian events', 'predicted': 'sailing, athletics, swimming, diving, triathlon and equestrian', 'question': 'What events did Bermuda compete in at the 2004 Summer Olympics?'}, '57295b726aef051400154d57': {'truth': 'made history by becoming the first black female diver to compete in the Olympic Games.', 'predicted': 'becoming the first black female diver to compete in the Olympic Games', 'question': 'What did Katura Horton-Perinchief do?'}, '57295b726aef051400154d58': {'truth': 'bronze medal in boxing.', 'predicted': '', 'question': 'What is the only medal Bermuda has ever won?'}, '57295b726aef051400154d59': {'truth': 'march in the Opening Ceremony in Bermuda shorts', 'predicted': 'march in the Opening Ceremony in Bermuda shorts, regardless of the summer or winter Olympic celebration', 'question': 'What is the Olympic tradition for Bermuda, regardless of season?'}, '5ad42d05604f3c001a40094b': {'truth': '', 'predicted': 'sailing, athletics, swimming, diving, triathlon and equestrian', 'question': 'What events did Bermuda participate in during the 2004 winter olympics?'}, '5ad42d05604f3c001a40094c': {'truth': '', 'predicted': 'becoming the first black female diver to compete in the Olympic Games', 'question': 'What did Katura Perinchief-Horton do?'}, '5ad42d05604f3c001a40094d': {'truth': '', 'predicted': \"Men's Skeleton\", 'question': 'What event did Bermuda participate in during the 2006 summer olympics?'}, '5ad42d05604f3c001a40094f': {'truth': '', 'predicted': 'Beijing Olympics', 'question': 'What games did Bermuda host in 2008?'}}, 'incorrect_text': {'57326c5ae99e3014001e6794': {'truth': 'the South Bronx', 'predicted': 'Yankee Stadium', 'question': 'Where was arson a big problem in the Bronx?'}, '57326c5ae99e3014001e6796': {'truth': 'BBC', 'predicted': 'Edwin Pagan', 'question': 'Who made a documentary called \"The Bronx is burning\"?'}, '5709b308ed30961900e84430': {'truth': '1777', 'predicted': '1862', 'question': 'Which year was it when paper money was first issued without the backing of precious metals?'}, '56be932e3aeaaa14008c90fb': {'truth': 'five', 'predicted': 'Australia, Hungary, Ireland, New Zealand and the United States', 'question': 'How many countries did her song \"Irreplaceable\" get number one status in?'}, '56be932e3aeaaa14008c90fc': {'truth': 'five', 'predicted': 'three', 'question': 'How many singles did her second album produce?'}, '5726ceaa5951b619008f7e93': {'truth': 'Russia', 'predicted': 'the Tsar', 'question': 'Who was given the special role of guardian over the Orthodox Christians in Moldavia and Wallachia?'}, '5726ceaa5951b619008f7e95': {'truth': 'Nicholas', 'predicted': 'Austria', 'question': 'Who felt Europe would not object to the joining of neighboring Ottoman provinces?'}, '5733e6a54776f41900661475': {'truth': 'Sheffield United', 'predicted': 'Manchester United', 'question': 'For which team was the first goal scored?'}, '5725f36689a1e219009ac0f2': {'truth': 'white sleeves', 'predicted': 'brighter pillar box red', 'question': 'What distinctive change did Chapman make to the Arsenal shirts?'}, '5725f36689a1e219009ac0f3': {'truth': 'red and white', 'predicted': 'all-red', 'question': 'For what style of shirts Arsenal known ?'}, '570e6d560dc6ce1900205050': {'truth': 'below the level required for the George Medal', 'predicted': 'gallantry', 'question': 'Of what acts did the Members of the Order of the British Empire appoint?'}, '570e6d560dc6ce1900205051': {'truth': 'same criteria as usual, and not by the level of gallantry', 'predicted': 'The grade', 'question': 'What grade was determined?'}, '5730c36ab7151e1900c01530': {'truth': \"purification was eventually associated with the feast of Mary's very conception\", 'predicted': '\"Conception of St. Ann', 'question': \"What became associated with the celebration of Mary's inception in the womb ? \"}, '570af6876b8089140040f648': {'truth': 'hand-held mobile devices', 'predicted': 'workers on an off-shore oil rig', 'question': 'What is an example of a place that videoconferencing can be used today?'}, '56f9313f9b226e1400dd1286': {'truth': 'Greenwich Avenue', 'predicted': 'from Eighth Avenue to Tenth Avenue', 'question': 'Where does the second part of 13th Street end?'}, '572fcc11947a6a140053ccd2': {'truth': 'circular chromosome', 'predicted': 'single circular', 'question': 'What shape is chromosome of bacteria?'}, '5727dad64b864d1900163e9c': {'truth': 'loose tolerances', 'predicted': 'relatively loose', 'question': 'What type of tolerances does the USB standard specify for compliant USB connectors?'}, '572f9a2ba23a5019007fc7ca': {'truth': 'oxygen', 'predicted': 'pig iron', 'question': 'What element was used in the production of wrought iron?'}, '570a661f6d058f1900182e0d': {'truth': 'Oxford', 'predicted': 'Robert C. Solomon', 'question': 'Who published What Is An Emotion?: Classic and Contemporary Readings?'}, '572b8f5d111d821400f38f12': {'truth': 'foreign language most often used', 'predicted': '24.86 percent', 'question': 'What distinction does Czech have in Slovakia?'}, '57274eb3f1498d1400e8f60c': {'truth': 'one of the smallest genomes', 'predicted': 'The sequencing of some other relatively small genomes', 'question': 'Why was this plant chosen for sequencing?'}, '57274eb3f1498d1400e8f60d': {'truth': 'understanding the genetics', 'predicted': 'The sequencing of some other relatively small genomes', 'question': 'Why is sequencing done on plants?'}, '571adfb39499d21900609b6f': {'truth': 'knew Greek', 'predicted': 'not knowing Hebrew', 'question': 'Did Athanasius speak Greek?'}, '571adfb39499d21900609b70': {'truth': 'not knowing Hebrew', 'predicted': 'Greek', 'question': 'Did he understand Hebrew?'}, '5725ca1589a1e219009abeaf': {'truth': 'this foundation helps move public libraries into the digital age.', 'predicted': 'installed computers and software', 'question': 'What has the grant enabled '}, '57264dedf1498d1400e8db8c': {'truth': 'the North German Confederation', 'predicted': 'Schutz des geistigen Eigentums', 'question': 'Which constitution gave legislative power to protect intellectual property?'}, '57273aa2dd62a815002e99c5': {'truth': 'chivalry', 'predicted': 'chivalric', 'question': 'What was the code of conduct of the military orders called?'}, '56def0acc65bf219000b3e3d': {'truth': 'the cultural aspects of Christianity', 'predicted': 'cultural aspects of Christianity, irrespective of personal religious', 'question': 'Whether one partakes in practices or beliefs, the label Christian is sometimes attached because they associate with what?'}, '57301bfca23a5019007fcd83': {'truth': 'penicillin, vancomycin, penicillin and vancomycin, or chlortetracycline', 'predicted': 'subtherapeutic antibiotic treatment', 'question': 'What are some antibiotics can be used for STAT?'}, '57328cf2b3a91d1900202e35': {'truth': 'unclear', 'predicted': 'increased body mass in humans and mouse models. Early life is a critical period for the establishment of the intestinal microbiota and for metabolic development. Mice exposed to subtherapeutic antibiotic treatment (STAT)– with either penicillin, vancomycin, penicillin and vancomycin, or chlortetracycline had altered composition of the gut microbiota as well as its metabolic capabilities', 'question': 'Do antibiotics increase the chance of getting fat for humans?'}, '56f8ee329e9bad19000a0719': {'truth': 'boys', 'predicted': 'males', 'question': 'What gender has a higher enrollment?'}, '57318f33a5e9cc1400cdc083': {'truth': 'streets', 'predicted': 'Calçada Portuguesa) is a kind of two-tone stone mosaic paving created in Portugal, and common throughout the Lusosphere', 'question': 'What other cityscape is done with Portuguese pavement?'}, '56d3847159d6e414001465ff': {'truth': 'church', 'predicted': 'Bible Belt', 'question': 'Where do people in the Southern United States often begin singing? '}, '56db3ebee7c41114004b4f97': {'truth': 'church', 'predicted': 'Bible Belt', 'question': 'Where do a lot of people get their start in singing in the south?'}, '5728498d3acd2414000df8a4': {'truth': 'powers which are not either exclusively of European competence or shared between EU and state as concurrent powers are retained by the constituent states.', 'predicted': 'Lisbon Treaty', 'question': 'What is the libson treaty?'}, '572ac154be1ee31400cb8215': {'truth': '\"... you end up getting us stuck in a war in Iraq. Just ask President Bush.\"', 'predicted': 'a jab at President Bush', 'question': \"What was Kerry supposed to say when he 'botched a joke'?\"}, '572ac154be1ee31400cb8217': {'truth': 'inadvertently left out the key word \"us\"', 'predicted': 'botched', 'question': 'What mistake did Kerry make in the joke?'}, '5726069c38643c19005acf61': {'truth': 'Greek', 'predicted': 'Hellenic overlords', 'question': 'Native populations in the Hellenistic world were discriminated by what peoples?'}, '56f8b9549e9bad19000a03b7': {'truth': 'their specific DNA loci', 'predicted': 'their functional products (proteins or RNA)', 'question': 'What does the typical definition of a gene categorize genes by?'}, '57282a944b864d190016463a': {'truth': 'vertical mesenteries', 'predicted': 'septa', 'question': \"What are annelids' body cavities separated from each other by?\"}, '56e087957aa994140058e5c1': {'truth': '2', 'predicted': 'two', 'question': 'How many different spin isomers exist?'}, '5727a6233acd2414000de8d5': {'truth': 'Yale University', 'predicted': 'Albertus Magnus College', 'question': 'What private university is located in downtown New Haven?'}, '572ea0bedfa6aa1500f8d21f': {'truth': 'university entrance examinations', 'predicted': 'scores', 'question': 'What plays a large factor in determining admission at a university?'}, '5727ff933acd2414000df1bf': {'truth': 'standard record player with a suitable pickup, a phono-preamp (pre-amplifier) and a typical personal computer', 'predicted': 'archivists play back the disc on suitable equipment and record the result, typically onto a digital format, which can be copied and manipulated to remove analog flaws without any further damage to the source recording', 'question': 'What would a hobbiest need to transfer historic recordings to digital formats?'}, '570ddc210dc6ce1900204cd0': {'truth': 'early maturing', 'predicted': 'late', 'question': 'Do early or late maturing girls have more unwanted pregnancies?'}, '56f8d8959e9bad19000a05e2': {'truth': 'PRS (Party for Social Renewal)', 'predicted': 'African Party for the Independence of Guinea and Cape Verde), one of the two major political parties in Guinea-Bissau, along with the PRS', 'question': 'Besides the PAIGC, what is the other major political party?'}, '5726c9c25951b619008f7e22': {'truth': 'the Politburo', 'predicted': \"the People's Liberation Army's (PLA) Taiwan invasion force was reorganized into the PLA North East Frontier Force\", 'question': 'Where did Mao Zedong declare that he would intervene in the Korean conflict?'}, '572ee3a8c246551400ce477f': {'truth': 'General Zhang Huan', 'predicted': 'Chen Fan', 'question': 'Who made accusations of treason against Dou Wu?'}, '56f744beaef2371900625a77': {'truth': 'The common practice period', 'predicted': 'Baroque era', 'question': 'When did many of the ideas that make up western classical music take shape?'}, '56ceb9c4aab44d1400b8892c': {'truth': 'in Wenchuan', 'predicted': 'Wenchuan County', 'question': 'Where is Yingxiu located?'}, '572f820a04bcaa1900d76a37': {'truth': 'French reinforcements were blocked by British naval victory in the Battle of Cartagena', 'predicted': 'a wave of major operations', 'question': 'How did the British assure numerical superiority in taking Louisbourg?'}, '572629d9ec44d21400f3db29': {'truth': \"The company's mainstay business\", 'predicted': 'mainstay businesses', 'question': 'How important were cotton, silk, indigo dye,saltpetre and tea to the company?'}, '572ff0a2947a6a140053ce37': {'truth': 'natural gas', 'predicted': 'fossil fuels', 'question': 'What resource does Iran have the largest supply of in the world?'}, '572ff0a2947a6a140053ce38': {'truth': 'proven oil reserves', 'predicted': 'fossil fuels', 'question': 'What resource does Iran have the fourth largest supply of in the world?'}, '572690d45951b619008f76ce': {'truth': 'the order of the Emperor Maximilian', 'predicted': 'Emperor Maximilian of Mexico', 'question': 'Who made the Angel of Independence?'}, '572690d45951b619008f76cf': {'truth': 'National Palace (seat of government) with the Castle of Chapultepec', 'predicted': 'the National Palace', 'question': 'The angel of independence is over the roadway that connects what?'}, '5726a1ccf1498d1400e8e56c': {'truth': 'Gerber Products Company in Bridgwater is the largest producer of fruit juices in Europe, producing brands such as \"Sunny Delight\" and \"Ocean Spray', 'predicted': 'Agriculture and food and drink production', 'question': 'What area is important to the fruit juice industry '}, '5731d56fe17f3d1400422472': {'truth': 'Methodist and Wesleyan', 'predicted': 'Protestant Christianity', 'question': 'What were the roots of the modern Pentecostal movement?'}, '5728b2813acd2414000dfcf9': {'truth': 'traditional and folk culture', 'predicted': 'Madeira Island', 'question': 'Where can you see mandolins a part of in Portgal? '}, '572ec004c246551400ce45f6': {'truth': 'five days of artillery bombardment', 'predicted': 'Battle of Gross-Jägersdorf', 'question': 'What led to the defeat of the Prussians at Memel?'}, '572ec004c246551400ce45f7': {'truth': 'used Memel as a base to invade East Prussia', 'predicted': 'artillery bombardment', 'question': 'How did the Russians used the captured Memel?'}, '572ec004c246551400ce45f8': {'truth': 'defeated a smaller Prussian force', 'predicted': 'artillery bombardment', 'question': 'What caused the Russians based in Memel to be successful deeper into Prussia?'}, '57313b16e6313a140071cd53': {'truth': 'collection of the land tax', 'predicted': 'unorthodox sects', 'question': 'What did Yongzheng crack down on?'}, '57327ed206a3a419008aca8d': {'truth': 'Greek manuscripts', 'predicted': 'the fall of the Byzantine Empire to the Turks', 'question': 'What caused a large migration of Greek refuges in the 1450s?'}, '5726d3475951b619008f7f26': {'truth': 'General MacArthur', 'predicted': 'the President', 'question': 'Who refused to meet on continental United States?'}, '5726d3475951b619008f7f27': {'truth': 'little risk of Chinese intervention in Korea', 'predicted': \"because of the General's discourteous refusal to meet the President on the continental United States\", 'question': 'What was President Truman told at this meeting?'}, '57318a1305b4da19006bd25d': {'truth': 'Roman', 'predicted': 'the Arabs', 'question': 'In Syria and Egypt, other than early Christians, who influenced their mosaic work?'}, '5732488d0fdd8d15006c68ee': {'truth': 'San Antonio', 'predicted': 'Fort Sam Houston', 'question': 'In what city was the 3rd Army based in June of 1941?'}, '57266708708984140094c4e7': {'truth': 'KaDeWe', 'predicted': 'Karstadt', 'question': 'What major department store operates in Berlin? '}, '56d64a821c85041400947075': {'truth': 'inadequately engineered', 'predicted': 'Due to the one-child policy', 'question': 'Why did so many schools collapse during the earthquake?'}, '56e0f68f7aa994140058e836': {'truth': 'June 16, 1963', 'predicted': 'Vostok 6', 'question': 'The first woman to launch into space was on what date?'}, '56e0f68f7aa994140058e837': {'truth': 'The USSR', 'predicted': 'Soviet Union', 'question': 'The first woman to go into space was from which country?'}, '5709720ded30961900e84163': {'truth': 'fermented', 'predicted': 'wine, brandy', 'question': 'What kind of grapes are made into vinegar?'}, '5709720ded30961900e84164': {'truth': 'Concord grapes', 'predicted': 'purple', 'question': 'What is the most common grape used to make juice in North America?'}, '57269b6b708984140094cb77': {'truth': 'two or more elements', 'predicted': 'pure or fairly pure chemical elements', 'question': 'What is an alloy composed of?'}, '572675a3dd62a815002e85b5': {'truth': \"Britain's dominant position in world trade\", 'predicted': 'Between 1815 and 1914, a period referred to as Britain\\'s \"imperial century', 'question': \"What was the period of Britain acting as the world's police called?\"}, '5726dc97708984140094d3f5': {'truth': 'a submarine base', 'predicted': 'Basis Nord', 'question': 'What did the Germans use to avoid British blockades?'}, '5728a75e4b864d1900164b93': {'truth': 'Anweisung die Mandoline von selbst zu erlernen nebst einigen Uebungsstucken von Bortolazzi', 'predicted': 'Cremonese mandolin', 'question': \"What was Bartolomeo Bortolazzi's popular mandolin method?\"}, '572ac9cbbe1ee31400cb8256': {'truth': 'from around $165 million to as high as $3.2 billion', 'predicted': '$750 million', 'question': \"What range of estimates have been given for Teresa Heinz Kerry's net worth?\"}, '5726fec6dd62a815002e973d': {'truth': 'The area in which a glacier forms', 'predicted': 'corrie or cwm', 'question': 'What is a cirque?'}, '56e763f737bdd419002c3f2e': {'truth': '800 kg/m3', 'predicted': '250 kg/m3', 'question': 'What is the common density of printing paper?'}, '572a54e07a1753140016aeb2': {'truth': 'On December 16, 2010', 'predicted': 'August 6, 2010 news release to state that it had the money for and is striving to transition all 27 transmitters by August 31, 2011', 'question': 'On which day did CBC release an updated announcement stating they were striving to update all 27 transmitters?'}, '5730501a396df91900096053': {'truth': 'engage the unsuspecting German bomber from beneath', 'predicted': 'its configuration of four machine guns', 'question': 'How could aircraft engage bombers when fitted with a turret?'}, '5730501a396df91900096054': {'truth': 'larger target, compared to attacking tail-on, as well as a better chance of not being seen by the bomber', 'predicted': 'a larger target', 'question': 'What benefits did attacking from below offer?'}, '570f40f65ab6b81900390eb4': {'truth': '9 p.m', 'predicted': 'dim-light', 'question': 'When is the melatonin onset?'}, '570f40f65ab6b81900390eb7': {'truth': 'melatonin phase', 'predicted': 'melatonin offset', 'question': 'What are the more reliable markers in determining sleep timing?'}, '572a630e7a1753140016aefd': {'truth': 'northern outskirts', 'predicted': 'Neustift am Walde cemetery', 'question': 'Where was Hayek buried in relation to his home town of Vienna?'}, '572fa91e04bcaa1900d76b6a': {'truth': 'light-gathering complexes may even form lipid-enclosed structures', 'predicted': 'chlorosomes', 'question': 'Can membrane of bacteria create lipid structure?'}, '56e7b14c37bdd419002c4370': {'truth': '2016', 'predicted': '2014', 'question': 'When does the CAFL plan on start its first season?'}, '5727e243ff5b5019007d977d': {'truth': 'Seattle SuperSonics', 'predicted': 'Oklahoma City Thunder', 'question': \"What was the Thunder's previous name?\"}, '5726994b708984140094cb4d': {'truth': 'although it was a major first step towards Cubism it is not yet Cubist.', 'predicted': 'exaggeration', 'question': 'Is it true that the first Cubist picture is The Demoiselles? '}, '56f78981aef2371900625bab': {'truth': 'the South Pacific Mandate', 'predicted': 'German New Guinea', 'question': 'What group was the Marshall Islands a part of following World War I?'}, '56f78981aef2371900625bac': {'truth': 'the United States', 'predicted': 'the Empire of Japan', 'question': 'Who took over the Marshall Islands in the second world war?'}, '56f952ba9b226e1400dd1312': {'truth': 'Spain', 'predicted': 'Spanish East Indies', 'question': 'What European nation owned the Marshall Islands in 1874?'}, '56ce5f72aab44d1400b8870f': {'truth': '50', 'predicted': '30%', 'question': 'What percentage of energy in commercial buildings comes from HVAC systems?'}, '572e8519c246551400ce42b1': {'truth': 'Tyneside docks', 'predicted': 'earlier steam driven elevators', 'question': 'Were was the hydraulic crane initially used?'}, '572734eb5951b619008f86b5': {'truth': 'competition was suspended due to the First World War', 'predicted': 'Following the 1914–15 edition', 'question': 'Was competition suspended due to the first world war? '}, '570ac16f4103511400d5998c': {'truth': 'the penalty it exacts on aircraft size, payload, and fuel load', 'predicted': 'penalty it exacts on aircraft size, payload, and fuel load (and thus range)', 'question': 'What is the disadvantage of the ski-jump?'}, '56cf39c4aab44d1400b88ebb': {'truth': 'Spectre', 'predicted': 'SPECTRE', 'question': 'How was the Spectre acronym originally written?'}, '57267c12dd62a815002e86c3': {'truth': 'Canada', 'predicted': 'the UK', 'question': 'The first Home Rule bill would have given Ireland less self-control than what other territory?'}, '57267c12dd62a815002e86c5': {'truth': '1914', 'predicted': '1886', 'question': 'When was a Home Rule bill passed?'}, '572714d2708984140094d977': {'truth': 'consumption of carcinogenic preserved foods', 'predicted': \"affluence or a 'Western lifestyle\", 'question': 'What were cancers such as liver cancer or stomach cancer found to have a link to?'}, '56df86855ca0a614008f9c1d': {'truth': '3', 'predicted': 'three', 'question': 'How many major highways cross through Oklahoma City?'}, '56e6f6e0de9d371400068100': {'truth': 'Contemporary Christian music', 'predicted': 'Christian AC', 'question': 'What is CCM an acronym of?'}, '5706aab252bb891400689b3c': {'truth': 'differentiate the clubs and DJs', 'predicted': 'to maintain such exclusives', 'question': 'Why were DJs inspired to create their own house records?'}, '5729a3d56aef051400155078': {'truth': 'first-world countries', 'predicted': 'in cultures that have other protein sources such as fish or livestock', 'question': 'Where is eating insects considered taboo?'}, '56e6d988de9d371400068086': {'truth': 'hot AC', 'predicted': 'CHR', 'question': 'What AC format is still viable?'}, '5729667c3f37b3190047833c': {'truth': 'Florio, the Ducrot, the Rutelli, the Sandron, the Whitaker, the Utveggio', 'predicted': 'several families', 'question': 'Which families help to start cultural, industrial, and economic growth in Palermo?'}, '56df4fb48bc80c19004e4a61': {'truth': 'North Oklahoma City', 'predicted': 'north side', 'question': 'Which side is more urban and fashionable?'}, '56df84d756340a1900b29cd5': {'truth': 'instructors', 'predicted': 'Fuller', 'question': 'Bell trained who in Boston?'}, '56ce7376aab44d1400b887a7': {'truth': 'Roman times', 'predicted': '16th century', 'question': 'When were the first greenhouses used?'}, '56d0875b234ae51400d9c349': {'truth': 'enabling year-round production and the growth (in enclosed environments) of specialty crops', 'predicted': 'keep exotic plants brought back from explorations abroad', 'question': 'What is one purpose of a greenhouse?'}, '572eb9a703f98919007569a8': {'truth': 'metaphysics', 'predicted': '\"creation from nothing', 'question': 'Salam suggests physics and science be kept separate from which topics which are more suited to religion?'}, '5729778f6aef051400154f5b': {'truth': 'entire Northeast was affected', 'predicted': 'the postwar period', 'question': 'Was the city the only one that suffer a decline within the manufacturing sector?'}, '573040dd947a6a140053d33a': {'truth': 'support the German submarine force', 'predicted': 'attack British port facilities', 'question': 'What did Erich Raeder believe the Luftwaffe needed to do?'}, '573040dd947a6a140053d33b': {'truth': 'attack British port facilities.', 'predicted': 'the need to attack British port facilities', 'question': 'Raeder convinced Hitler to do what?'}, '573040dd947a6a140053d33c': {'truth': 'the high success rates of the U-Boat force', 'predicted': 'the need to attack British port facilities', 'question': 'What ultimately convinced Hitler that Raeder was right?'}, '56cee43eaab44d1400b88c05': {'truth': '1799', 'predicted': '1827', 'question': 'In what year did the state of New York pass a law to free the slaves?'}, '5729e5501d0469140077965c': {'truth': 'from a reservoir of electrical potential energy between electrons', 'predicted': 'a reservoir of electrical potential energy between electrons, and the molecules or atomic nuclei that attract them.', 'question': 'Where is chemical energy stored and released?'}, '5732696fe17f3d1400422961': {'truth': 'Eisenhower', 'predicted': 'the Russians', 'question': 'Who refused to permit nuclear weapons inspections in the wake of the 1955 talks?'}, '57268054dd62a815002e8764': {'truth': 'pesticides are also used for non-agricultural purposes', 'predicted': 'most common', 'question': 'What is the difference between a pesticide and a plant protection product?'}, '57268054dd62a815002e8768': {'truth': 'sanitizer', 'predicted': 'plant protection products', 'question': 'What item commonly used in hospitals, schools and offices is a pesticide?'}, '5727d0914b864d1900163dc4': {'truth': 'Mexico', 'predicted': 'Canada', 'question': 'Where does Oklahoma receive hot dry air from?'}, '570b4c3c6b8089140040f862': {'truth': 'al-Qaeda', 'predicted': 'Islamic Extremist', 'question': 'What is one prominent, specific terrorist group targeted by the War on Terrorism?'}, '5732836406a3a419008acab0': {'truth': 'Gherardo', 'predicted': 'winning earthly glory and praising virtue', 'question': 'Petrarch felt that although he tried to do his own form of good whose life may have more meaning?'}, '5731f24bb9d445190005e6d5': {'truth': 'defiance of the omen', 'predicted': 'the sacred chickens would not eat', 'question': \"What was Publius's critical mistake in his sea campaign?\"}, '5731f24bb9d445190005e6d7': {'truth': 'impiety', 'predicted': 'The efforts of military commanders to channel the divine will', 'question': \"What was the cause of Publius's failures according to Roman feeling?\"}, '570b2dd76b8089140040f7d7': {'truth': 'confusion', 'predicted': 'to identify which system is used in the British Empire and other countries that did not immediately change', 'question': 'What did the use of Old Style and New Style cause?'}, '57262d20271a42140099d704': {'truth': 'as they got hotter, their electrical resistance decreased', 'predicted': 'negative temperature coefficient of resistance', 'question': 'What was the primary problem with early carbon filaments?'}, '57262d20271a42140099d705': {'truth': 'improved the uniformity and strength of filaments as well as their efficiency', 'predicted': \"stabilize the lamp's power consumption, temperature and light output against minor variations in supply voltage\", 'question': 'What were the positive effects of the flashing process?'}, '572efc3503f9891900756b16': {'truth': 'by sequencing the genome of an ancient flowering plant', 'predicted': 'sequencing the genome of an ancient flowering plant, Amborella trichopoda,', 'question': 'How are duplication events studied?'}, '56dedc703277331400b4d776': {'truth': 'Hummers', 'predicted': 'street-legal, civilian', 'question': 'In addition to the Humvee, what other vehicle manufactured by AM General was Schwarzenegger first to own?'}, '5726245b89a1e219009ac2f0': {'truth': 'ram it', 'predicted': 'bailed out', 'question': 'What did Holmes do to stop the German Plane?'}, '57305ed58ab72b1400f9c4ab': {'truth': 'an altar', 'predicted': 'Private and personal worship', 'question': 'What religious element could be found in all Roman households?'}, '5730b55f396df919000962cd': {'truth': \"Sony's PlayStation Portable (PSP), the Nintendo DS and Game Boy Advance, the Gizmondo, the Dingoo and the GP2X by GamePark Holdings\", 'predicted': 'handheld units, such as Android', 'question': 'What portable game systems have SNES emulators?'}, '56f9ebe18f12f3190062fffb': {'truth': 'an alliance of broadcasters, consumer electronics manufacturers and regulatory bodies', 'predicted': 'Digital Video Broadcasting', 'question': 'What is the DVB?'}, '5729058baf94a219006a9f6d': {'truth': 'join the army or get government jobs, the main route to success in the country.', 'predicted': 'non-Buddhists to join the army or get government jobs', 'question': 'What are the best routes for career achievement in Burma ?'}, '56de2fa0cffd8e1900b4b63f': {'truth': '2013', 'predicted': '2014 Human Development Report by the United Nations Development Program was released on July 24, 2014', 'question': 'Which year was used for estimates in the 2014 report?'}, '56e7a21637bdd419002c42a0': {'truth': '25th', 'predicted': 'silver', 'question': 'What anniversary did the Arena Football League celebrate in 2012?'}, '56e039947aa994140058e3d3': {'truth': 'komiks', 'predicted': 'Комикс', 'question': 'What Russian word is used for comics?'}, '56de471ccffd8e1900b4b770': {'truth': 'Grover Cleveland', 'predicted': 'Johnson Administration', 'question': 'Under which President was the Tenure of Office Act repealed? '}, '5722d357f6b826140030fc67': {'truth': 'fiftieth anniversary', 'predicted': '1887', 'question': 'What year anniversary does the Golden Jubilee celebrate?'}, '572635ccec44d21400f3dc40': {'truth': 'the fiftieth anniversary of her accession', 'predicted': 'extremely popular', 'question': 'What is the point of the Golden Jubilee?'}, '57277778708984140094de55': {'truth': 'the texts, as transmitted, contain a considerable amount of variation,', 'predicted': 'because the texts, as transmitted, contain a considerable amount of variation', 'question': 'What is one reasons Shakespeare is a good place to focus on textual criticism?'}, '572802332ca10214002d9b52': {'truth': 'provided the stylus, record player, and record itself are built to withstand it', 'predicted': 'CDJ and DJ advances, such as DJ software and time-encoded vinyl', 'question': 'Are all turn tables capable of DJ manipulation of vinyl records?'}, '572802332ca10214002d9b53': {'truth': 'any electronic dance music and hip hop releases today are still preferred on vinyl', 'predicted': 'direct manipulation of the medium', 'question': 'What is commonly preferred by DJs vinyl or CD?'}, '572f7b31b2c2fd140056817d': {'truth': 'social and societal upheaval', 'predicted': 'The Jazz Age', 'question': 'What is the \"Roaring Twenties\" ?'}, '56e7921a37bdd419002c4171': {'truth': 'the opening of the Suez Canal', 'predicted': 'they were no longer needed as a stopping port or for shelter for journeys from Europe to East Asia.', 'question': 'what is one reason that caused the island to become less used?'}, '570bd2ec6b8089140040fa69': {'truth': 'Control-Q', 'predicted': 'Control-S', 'question': 'What caused the automatic paper tape reader to start again?'}, '573039c004bcaa1900d773c5': {'truth': '480 infantrymen', 'predicted': 'six centuries of 80 men each', 'question': 'Around how many units could be expected to be contained within a cohort?'}, '573039c004bcaa1900d773c6': {'truth': '8 men', 'predicted': '10', 'question': 'How many troops were placed into each tent group?'}, '573039c004bcaa1900d773c8': {'truth': 'cohort', 'predicted': 'heavy infantry', 'question': 'What designation of troops was considered to make up the majority of a legion?'}, '572785c8dd62a815002e9f6e': {'truth': 'physical and psychological implications', 'predicted': 'very dangerous', 'question': 'What are the risks of child labour in drug cartels?'}, '572842a0ff5b5019007da032': {'truth': 'gradually changed', 'predicted': 'made more and more of the decisions', 'question': 'What did Nasser do over the years of his rule?'}, '56fc975cb53dbe1900755134': {'truth': 'feeding', 'predicted': 'phonological alternation', 'question': 'Aside from bleeding what is an order of rules that define how pronunciation of a sound changes?'}, '56fc975cb53dbe1900755135': {'truth': 'Phonology', 'predicted': 'phonotactics (the phonological constraints on what sounds can appear in what positions in a given language) and phonological alternation (how the pronunciation of a sound changes through the application of phonological rules, sometimes in a given order which can be feeding or bleeding,) as well as prosody, the study of suprasegmentals', 'question': 'Phonotactics, phonological alternation and prosody are topics contained in what discipline?'}, '56fc975cb53dbe1900755136': {'truth': 'prosody', 'predicted': 'stress', 'question': 'Stress and intonation are studied under what topic?'}, '572eb63603f989190075697e': {'truth': 'one was granted, one was partially granted, one was denied and three were withdrawn', 'predicted': 'identification of reasonable and prudent alternatives to avoid jeopardy', 'question': 'What were the results of those exemption petitions?'}, '5728cc17ff5b5019007da6e3': {'truth': 'three big provinces', 'predicted': 'Maharashtra, Bengal and Punjab', 'question': 'What parts of India shaped the demands of the people for nationalism?'}, '5726e8eb708984140094d58b': {'truth': 'Upon being harmed (e.g., stung) by their prey, the appearance in such an organism will be remembered as something to avoid', 'predicted': 'the antithesis of camouflage', 'question': 'How does aposematism help a species population?'}, '5726e8eb708984140094d58c': {'truth': 'bright, easily recognizable and unique colors and patterns', 'predicted': 'brightly colored', 'question': 'What visual cues are characteristic of aposematism?'}, '572f5d5eb2c2fd1400568085': {'truth': 'prescribed targets are not hit', 'predicted': 'inability to damage industries sufficiently', 'question': 'Why did Hitler feel the Luftwaffe was unsuccessful in bombing raids?'}, '570e25b30dc6ce1900204dfb': {'truth': 'its colonial history', 'predicted': 'Ethiopian cooking, including more pasta and greater use of curry powders and cumin.The Italian Eritrean cuisine started to be practiced during the colonial times of the Kingdom of Italy', 'question': 'Where does the Italian influences on Eritrean cuisine come from?'}, '5727ff0c2ca10214002d9ae7': {'truth': 'Mughal Empire', 'predicted': 'Mughals', 'question': 'What empire covered most of India in the 16th century?'}, '56d375b859d6e41400146469': {'truth': 'terminal', 'predicted': 'very seriously ill', 'question': \"What was the diagnosis of Chopin's health condition at this time?\"}, '5727ff7c2ca10214002d9b03': {'truth': '1935', 'predicted': '30 November 1934', 'question': 'When did he take that position?'}, '5726909f5951b619008f76c3': {'truth': '$173 billion', 'predicted': '€130 billion', 'question': 'in 2012, what was the amount of the bailout?'}, '57305b0b069b5314008320a9': {'truth': 'grammatical structure', 'predicted': 'word order', 'question': 'What is it sometimes necessary to reinterpret when translating?'}, '570fc65b80d9841400ab366d': {'truth': 'going up', 'predicted': 'going down', 'question': \"What was happening to Dell's average sale to individuals?\"}, '57268ac8dd62a815002e88da': {'truth': 'eighth', 'predicted': '30th', 'question': 'What ranking in terms of GDP is Mexico City globally?'}, '5706a52352bb891400689b13': {'truth': 'National Gallery of Modern Art', 'predicted': 'Indira Gandhi Memorial Museum', 'question': 'What is the name of the modern art museum located in new Delhi?'}, '57329efbcc179a14009dab80': {'truth': '19', 'predicted': '100 times', 'question': 'For how many months was Australia attacked from the air by Japan?'}, '56e1223ecd28a01900c67633': {'truth': 'literary life', 'predicted': 'rarefied literary', 'question': 'Boston was admired for what kind of life?'}, '56de24b24396321400ee25fd': {'truth': 'John Locke', 'predicted': 'Thomas Hobbes, strongly opposed it. Montesquieu', 'question': 'Which Enlightenment thinker supported the idea of separation of powers?'}, '56de33fc4396321400ee2694': {'truth': 'John Locke', 'predicted': 'Thomas Hobbes, strongly opposed it. Montesquieu', 'question': 'Who was an advocate of separation of powers?'}, '572a7a17be1ee31400cb8029': {'truth': 'Miami International', 'predicted': 'MIA', 'question': \"What is Florida's busiest airport?\"}, '572fe936b2c2fd14005685c2': {'truth': 'perpendicular', 'predicted': 'aligned with the electrical field', 'question': 'How would you place the conductor in relation to the signal you wished to obtain?'}, '572ec21cdfa6aa1500f8d350': {'truth': '\"People\\'s Courts\" were founded to try various monarchist politicians and journalists, and though many were imprisoned, none were executed.', 'predicted': 'purged monarchists and members of Idris\\' Senussi clan from Libya\\'s political world and armed forces; Gaddafi believed this elite were opposed to the will of the Libyan people and had to be expunged. \"People\\'s Courts\" were founded to try various monarchist politicians and journalists, and though many were imprisoned', 'question': 'What happened to the monarchists and and journalists?'}, '57317177a5e9cc1400cdbf71': {'truth': 'monarchists', 'predicted': \"Idris' Senussi clan\", 'question': 'Along with the Senussi, who was purged from the military?'}, '56dd37fe66d3e219004dac78': {'truth': 'Right Honourable', 'predicted': 'Excellency', 'question': 'What honorific title can be given to prime ministers in commonwealth nations?'}, '56fdee67761e401900d28c57': {'truth': 'basic instruction can be given a short name that is indicative of its function', 'predicted': 'mnemonics', 'question': \"A computer's assembly language is known as what?\"}, '5727fb4eff5b5019007d99e5': {'truth': 'two', 'predicted': 'four ministerial portfolios', 'question': \"How many posts did the Muslim Brotherhood get in Naguib's cabinet?\"}, '5727fb4eff5b5019007d99e6': {'truth': 'opposed', 'predicted': 'most of the RCC insisted', 'question': \"What was Nasser's position on executing the rioter's leaders?\"}, '572ea6d5cb0c0d14000f13f2': {'truth': 'California constitutional convention of 1849 had eight Californio participants; the resulting state constitution was produced in English and Spanish,', 'predicted': 'never developed bilingualism', 'question': 'Was California a bilingual state?'}, '56e4cfd839bdeb14003479e0': {'truth': 'smart growth, architectural tradition and classical design', 'predicted': 'sustainable approach towards construction', 'question': 'What are three things the new movements try to achieve?'}, '573428b44776f419006619ca': {'truth': 'Marana', 'predicted': 'the Northwest', 'question': 'Where is the Continental Ranch planned community?'}, '573428b44776f419006619cb': {'truth': 'Marana', 'predicted': 'the Northwest', 'question': 'Where is the Dove Mountain planned community?'}, '573428b44776f419006619cc': {'truth': 'Oro Valley', 'predicted': 'the Northwest', 'question': 'Where is the Rancho Vistoso planned community?'}, '56e0b2127aa994140058e6b0': {'truth': 'phosphorus', 'predicted': 'protium', 'question': 'What does the symbol P represent?'}, '56e0b2127aa994140058e6b1': {'truth': '2H and 3H', 'predicted': 'D and T', 'question': 'What are the preferred symbols for deuterium and tritium?'}, '56dfc0ae231d4119001abd95': {'truth': 'upstream ISPs', 'predicted': 'their customers', 'question': 'Who does an ISP pay for internet access?'}, '56dfc0ae231d4119001abd99': {'truth': 'upstream ISPs', 'predicted': 'contracting ISP', 'question': 'what usually has a larger network, the ISP of the customer or the upstream ISP?'}, '572ed3f503f9891900756a67': {'truth': 'Moses', 'predicted': 'Jesus', 'question': 'Which Biblical character is the most often mentioned person in the Quran?'}, '56d632371c85041400946fe4': {'truth': 'infections.', 'predicted': 'serious infections', 'question': 'A dog scratch can lead to what medical condition?'}, '56d9e7e4dc89441400fdb905': {'truth': 'infections.', 'predicted': 'serious infections', 'question': 'According to the text, dog scratches can cause what?'}, '56d0007f234ae51400d9c247': {'truth': 'single-slope', 'predicted': 'saline or brackish water potable. The first recorded instance of this was by 16th-century Arab alchemists. A large-scale solar distillation project was first constructed in 1872 in the Chilean mining town of Las Salinas', 'question': 'What is an example of a solar distillation design?'}, '57303bb004bcaa1900d773f4': {'truth': 'Government of Ireland', 'predicted': 'North/South Ministerial Council', 'question': 'The Northern Ireland Executive meets with what other government to develop policies for the island of Ireland?'}, '57278133dd62a815002e9ee9': {'truth': 'the brain of one of the lower animals', 'predicted': 'tip of the radicle', 'question': 'To what did Darwin compare the top of the plant radical?'}, '57278133dd62a815002e9eea': {'truth': 'movements of plant shoots', 'predicted': 'the tip of the radicle . . acts like the brain of one of the lower animals', 'question': 'Why did Darwin feel plants had something comparable to a brain?'}, '57278133dd62a815002e9eeb': {'truth': 'promotes cell growth', 'predicted': 'to grow', 'question': 'What do auxins do?'}, '572a7b02111d821400f38b52': {'truth': 'Detroit', 'predicted': 'Miami', 'question': 'As of 2004, what city was the poorest in the United States?'}, '57096fa9ed30961900e84122': {'truth': 'anthroposemiotics', 'predicted': 'zoo semiotics', 'question': 'What is the study of human communication called?'}, '56fa5493f34c681400b0c085': {'truth': 'furniture', 'predicted': 'beds. It is also used for tool handles and cutlery, such as chopsticks, toothpicks, and other utensils, like the wooden spoon', 'question': 'What category of products usually made from wood includes chairs?'}, '56fa5493f34c681400b0c087': {'truth': 'handles', 'predicted': 'wooden spoon', 'question': 'Which parts of tools are sometimes made out of wood?'}, '56fa5493f34c681400b0c089': {'truth': 'chopsticks', 'predicted': 'wooden spoon', 'question': 'What special wooden utensils do many people use to eat Chinese takeout?'}, '573035c8b2c2fd1400568a7d': {'truth': 'give direct access to Tijuana International Airport', 'predicted': 'direct access to Tijuana International Airport, with passengers walking across the U.S.–Mexico border on a footbridge to catch their flight on the Mexican side', 'question': 'What is the purpose of the Tijuana Cross-border Terminal?'}, '572efd66cb0c0d14000f16cc': {'truth': 'social protocols', 'predicted': 'personal hygiene', 'question': 'The invention of elevators brought with it questions of social etiquette and formalities, generally referred to as what?'}, '5730c888aca1c71400fe5ab9': {'truth': 'calculators', 'predicted': 'incandescent and neon indicator lamps', 'question': 'What was one use of early LED light in products?'}, '5730c888aca1c71400fe5abb': {'truth': '1970s', 'predicted': '1968', 'question': 'In what decade were production costs greatly reduced for LEDs to enable successful commercial uses?'}, '57282ec23acd2414000df67b': {'truth': '300', 'predicted': '480', 'question': 'How many miles long was the human chain?'}, '572a3b486aef0514001553b6': {'truth': \"His father's\", 'predicted': 'university professor', 'question': \"Who's occupation inspired Hayek when he was older?\"}, '572a3b486aef0514001553b7': {'truth': 'Franz von Juraschek', 'predicted': 'Gustav Edler von Hayek', 'question': \"Eugen Bohm was friends with which of Hayek's grandfathers?\"}, '5725be0738643c19005acc41': {'truth': 'Vlaams', 'predicted': 'Standard Dutch', 'question': 'What would someone in Belgium call the variation of Dutch spoken in Flanders?'}, '5730878f2461fd1900a9ce92': {'truth': 'Poliane', 'predicted': 'Finnic Chud tribe', 'question': 'Which tribe resided in the south?'}, '572759cb5951b619008f888d': {'truth': 'rayon', 'predicted': 'Nylon', 'question': 'What was the first manufactured fiber?'}, '56d8dc9cdc89441400fdb351': {'truth': 'Taksim Square', 'predicted': 'Sultanahmet Square', 'question': 'Where did the torch end up in Istanbul?'}, '571aeaf69499d21900609ba9': {'truth': 'different theological views', 'predicted': 'the Arian Party was not monolithic', 'question': 'Did all Arians believe the same things?'}, '571aeaf69499d21900609bad': {'truth': 'not monolithic', 'predicted': 'drastically different theological views that spanned the early Christian theological spectrum', 'question': 'Did Arians have one set of beliefs?'}, '5726da10dd62a815002e929c': {'truth': 'Polish nobles, teachers, social workers, priests, judges and political activists', 'predicted': 'Polish elites', 'question': 'Who were the “intelligentia?” '}, '56ce0a3762d2951400fa69d6': {'truth': '1810', 'predicted': '1849', 'question': 'What year was Chopin born?'}, '56d1ca30e7d4791d009021a7': {'truth': '1810', 'predicted': '1849', 'question': 'In what year was Chopin born?'}, '570a5e0e6d058f1900182dbb': {'truth': '2003', 'predicted': '2007', 'question': 'In which year were these allegations raised?'}, '56df631296943c1400a5d4ab': {'truth': '39', 'predicted': '980', 'question': 'About how many inches of rain fall on Plymouth every year?'}, '572e7f0adfa6aa1500f8d04b': {'truth': 'King Cinyras, Teucer and Pygmalion', 'predicted': 'Aphrodite and Adonis', 'question': 'Cyprus is home to which Greek mythological figures?'}, '5705fcd775f01819005e783c': {'truth': 'Thomson', 'predicted': 'Rupert Murdoch', 'question': 'Who did a media magnate in the 1980s buy The Times from?'}, '56e0cfce7aa994140058e735': {'truth': 'Google', 'predicted': 'Mozilla', 'question': 'Which company pays Firefox to make their search engine the default on their browser?'}, '5728d8e3ff5b5019007da810': {'truth': 'Downtown', 'predicted': 'Northwest Detroit', 'question': \"In which district is Detroit Mercy's Law School located?\"}, '5706969952bb891400689ab5': {'truth': 'Indian ragas performed in a disco style', 'predicted': \"electronic instrumentation and minimal arrangement of Charanjit Singh's Synthesizing: Ten Ragas to a Disco Beat (1982), an album of Indian ragas\", 'question': \"What did Singh's album contain?\"}, '5706969952bb891400689ab7': {'truth': 'minimal arrangement', 'predicted': 'electronic instrumentation and minimal', 'question': 'What sort of arrangement did Charanjit Singh use on his 1982 album?'}, '572649865951b619008f6f20': {'truth': '1 January 1981', 'predicted': '1980', 'question': 'Greece joined what later became the European Union when?'}, '5728b8862ca10214002da65b': {'truth': 'feudal', 'predicted': 'Permanent Settlement', 'question': 'What type of land taxation system did the East India Company instigate in Bengal?'}, '570d410afed7b91900d45db4': {'truth': 'for anti-aircraft fire', 'predicted': 'it quickly became clear that guidance would be required for precision', 'question': 'Why was Britain mainly interested in solid fuel rockets?'}, '57303e5ea23a5019007fcfff': {'truth': 'access previous versions of shared files stored on a Windows Server computer', 'predicted': 'once saved previous versions of changed files', 'question': 'What does :76 Shadow Copy do?'}, '572846473acd2414000df84d': {'truth': 'felt that, if freedom and civilization were to survive, it would have to be because the US would triumph over totalitarianism', 'predicted': 'because he felt that, if freedom and civilization were to survive', 'question': 'Why did von Neumann join government work?'}, '56dfb89e7aa994140058e073': {'truth': 'horses', 'predicted': \"the traveller's horse(s)\", 'question': \"Aside from human beings, what creature's needs were traditionally seen to at inns?\"}, '5731eaa7e17f3d1400422548': {'truth': 'evolution of the Greek economy', 'predicted': 'the gradual development of industry and further development of shipping in a predominantly agricultural economy', 'question': 'What does recent research from 2006 examine?'}, '572786b5dd62a815002e9f89': {'truth': 'Darwin always finished one book before starting another', 'predicted': 'always finished one book before starting another. While he was researching, he told many people about his interest in transmutation without causing outrage. He firmly intended to publish, but it was not until September 1854 that he could work on it full-time. His estimate that writing his \"big book\" would take five years', 'question': \"What was Darwin's process on writing his books?\"}, '57266970f1498d1400e8dede': {'truth': 'turn', 'predicted': 'altering their face/heel alignment. This may be an abrupt, surprising event, or it may slowly build up over time. It almost always is accomplished with a markable change in behavior', 'question': 'What might a character do with their persona?'}, '57266970f1498d1400e8dedf': {'truth': 'heel', 'predicted': 'top face', 'question': 'What did Hulk Hogan become? '}, '5726e48b708984140094d500': {'truth': 'as the dielectric', 'predicted': 'insulators', 'question': 'For what use were non conductive materials used in the first capacitors?'}, '5726e48b708984140094d503': {'truth': 'as decoupling capacitors', 'predicted': 'telephony', 'question': 'What other use did paper capacitors serve in the telecommunications industry?'}, '57304c5e8ab72b1400f9c3fe': {'truth': 'seven', 'predicted': '12,000', 'question': 'How many houses were spared damage in Glasgow?'}, '57317c50a5e9cc1400cdbfbf': {'truth': 'by a mosaic replica of this last painting', 'predicted': 'a mosaic replica of this last painting, the Transfiguration', 'question': \"How is Raphael portrayed in St. Peter's?\"}, '572a3a0b6aef0514001553a5': {'truth': 'each other', 'predicted': 'simpliciter', 'question': 'The A-Series orders events according to their being in the past, present or future and in comparison to what else?'}, '572a3a0b6aef0514001553a6': {'truth': 'The B-series', 'predicted': 'A-series', 'question': \"What is McTaggart's second series called?\"}, '5730ef4205b4da19006bcc62': {'truth': 'How can there be absence of sin where there is concupiscence (libido)?', 'predicted': 'sinless', 'question': 'What did the query starter believe to be the ultimate difficulty in accepting  the a virgin conception of Mary ?'}, '5730ef4205b4da19006bcc64': {'truth': 'instituting such a festival without the permission of the Holy See', 'predicted': 'without the permission of the Holy See. In doing so, he takes occasion to repudiate altogether the view that the conception of Mary was sinless', 'question': \"Did the query starter believe that the festival for Mary's conception had authorization to be held ?\"}, '571a10584faf5e1900b8a880': {'truth': 'around 100', 'predicted': '28', 'question': 'How many theater companies does Seattle have in residence?'}, '56fdc60e19033b140034cd68': {'truth': '(clay spheres, cones, etc.)', 'predicted': 'clay spheres, cones, etc.) which represented counts of items, probably livestock or grains', 'question': 'Calculi during the Fertile Crescent refers to what?'}, '56f8def59e9bad19000a0642': {'truth': 'Southampton Water', 'predicted': 'other side', 'question': 'What body of water does the Hythe Ferry cross to reach Hythe from Southampton?'}, '57291a3e1d04691400779039': {'truth': '1952', 'predicted': 'nine', 'question': 'How many states was Germany reduced to in 1952'}, '5728f761af94a219006a9e85': {'truth': 'akin to a marriage', 'predicted': 'their backgrounds were checked by higher-ranked samurai', 'question': 'How did the samurai treat concubines?'}, '5730ed3ea5e9cc1400cdbaf2': {'truth': '11 years', 'predicted': '10 years', 'question': 'How long are females required to go to school?'}, '572a518ab8ce0319002e2a96': {'truth': 'developing commercial centres and routes', 'predicted': 'the state performed basic economic functions', 'question': 'The expansion of international trade through the Empire was the result of what?'}, '5726e07a5951b619008f8107': {'truth': 'the Fourth Phase Offensive', 'predicted': 'Gettysburg', 'question': \"What is considered to be the the Korean War's equivalent to Gettysburg?\"}, '57266c865951b619008f7252': {'truth': 'Out of the 22.5%, the largest groups were 6.5% (1,213,438) Cuban', 'predicted': 'Latino', 'question': 'What Origin makes up most of the Hispanics in Florida '}, '57266c865951b619008f7254': {'truth': 'the second largest Puerto Rican population after New York, as well as the fastest-growing in the nation', 'predicted': '4.5%', 'question': 'What is percentage of Puerto Ricans in Florida '}, '5733b195d058e614000b6084': {'truth': '28 Tajik soldiers', 'predicted': '25', 'question': 'How many solider were killed in September when Islamic militants escaped?'}, '57278cb9f1498d1400e8fbb4': {'truth': 'cases involving members of congress, senators', 'predicted': 'senators, ministers of state, members of the high courts and the President and Vice-President of the Republic', 'question': 'Which legislative bodies does this court sit in cases over?'}, '5731a21de17f3d1400422298': {'truth': 'private benefactors', 'predicted': 'local governments', 'question': 'What type of entity created competition with government created universities?'}, '572827843acd2414000df5b0': {'truth': 'bristles', 'predicted': 'hairs', 'question': 'What are setae?'}, '57325124e17f3d140042285c': {'truth': 'tolls were resented', 'predicted': 'abandonment of tolls altogether.', 'question': 'Why did farmers build a bridge over the Harlem River?'}, '5727a1eeff5b5019007d9162': {'truth': 'Imbabura', 'predicted': 'the northern part of the Sierra', 'question': 'Where has a celebration recently gained acclaim?'}, '57276683dd62a815002e9c36': {'truth': 'paid by the piece', 'predicted': 'they had to work productively', 'question': 'Were the boys in glass making industry paid by the hour?'}, '57276683dd62a815002e9c37': {'truth': 'factory owners preferred boys under 16 years of age', 'predicted': 'Many factory owners', 'question': 'Did the glass industry have a preference for older working boys?'}, '57310f4ae6313a140071cbcb': {'truth': 'against the ratification of the Treaty of Versailles, thus preventing American participation in the League', 'predicted': 'because the United States, meant to be the fifth permanent member, left because the US Senate voted on 19 March 1920 against the ratification of the Treaty of Versailles', 'question': 'Why was United Stated excluded from League of Nations Council?'}, '56de7b394396321400ee2959': {'truth': 'guano', 'predicted': 'maize', 'question': 'What animal byproduct was imported to Plymouth in the 19th century?'}, '572bc28a111d821400f38f78': {'truth': 'requires knowledgeable managers and engineers', 'predicted': 'knowledgeable managers and engineers who are able to operate new machines or production practices', 'question': 'What is greatly needed with technology transfer when it relates to education?'}, '5723fc250dadf01500fa1fe4': {'truth': '17', 'predicted': '14', 'question': 'How old was Princess Victoria when she was married?'}, '5723fc250dadf01500fa1fe5': {'truth': 'Germany', 'predicted': 'London', 'question': 'Where did princess Victoria move to after she was married?'}, '57266b70708984140094c568': {'truth': '17', 'predicted': '14', 'question': \"How old was Victoria's oldest daughter when she was amrried?\"}, '571a2b2410f8ca1400304f2d': {'truth': 'Like most', 'predicted': \"Washington's 7th congressional district\", 'question': \"To which states' election laws are Seattle's law and ballots similar?\"}, '56e7b1d437bdd419002c437a': {'truth': 'Friday or Saturday', 'predicted': 'Sunday', 'question': 'What days were AFL games traditionally played on before the TV deal?'}, '573271ece17f3d1400422981': {'truth': 'National Security Council', 'predicted': 'Strategic Air Command', 'question': 'Along with the Joint Chiefs and SAC, what body was involved with formulating plans for nuclear war with China?'}, '5706b5fa0eeca41400aa0d6f': {'truth': 'Los Angeles', 'predicted': \"Jewel's Catch One\", 'question': 'where did DJs marques wyatt and billy long become successful?'}, '5706b5fa0eeca41400aa0d72': {'truth': 'Haunted House', 'predicted': 'Deep Love', 'question': 'one voice records released a remix of what dada nada song in 1989?'}, '56f8ba089e9bad19000a03cb': {'truth': 'the Dolphin', 'predicted': 'the Brook, The Talking Heads, The Soul Cellar, The Joiners and Turner Sims, as well as smaller \"club circuit\" venues like Hampton\\'s and Lennon\\'s', 'question': 'What small music venue in Southampton is named after an aquatic mammal?'}, '57294fa6af94a219006aa285': {'truth': 'United States Guantánamo Bay detention camp', 'predicted': 'Bermuda', 'question': 'Where were the Uyghurs transferred from?'}, '5726b8addd62a815002e8e26': {'truth': 'a dialogue', 'predicted': 'seek contact', 'question': 'What did Paul VI want to keep open with the modern world and people from all walks of life?'}, '5726b8be708984140094cf17': {'truth': 'Catholic', 'predicted': 'Protestant', 'question': \"What religion was Burke's mother?\"}, '56f8d9db9b226e1400dd10e1': {'truth': 'Guinea', 'predicted': 'Senegal', 'question': 'What country is on the south border of Guinea-Bissau?'}, '56f8d9db9b226e1400dd10e4': {'truth': '13° and 17°W', 'predicted': '11° and 13°N', 'question': 'What longitudes does Guinea-Bissau mostly lie between?'}, '56e6df336fe0821900b8ec11': {'truth': 'boybands', 'predicted': 'power pops', 'question': 'What type of band are Backstreet Boys and Westlife?'}, '57301c4fb2c2fd1400568891': {'truth': \"majority of Tolbert's cabinet and other Americo-Liberian government officials and True Whig Party members.\", 'predicted': 'President William R. Tolbert, Jr.', 'question': \"Who was also executed on the day of William R. Tolbert's death?\"}, '57301c4fb2c2fd1400568892': {'truth': \"the People's Redemption Council\", 'predicted': \"People's Redemption Council (PRC)\", 'question': 'The coup leaders later became known as?'}, '56beb67d3aeaaa14008c929b': {'truth': 'Cater 2 U', 'predicted': 'Independent Women\" and \"Survivor\"', 'question': 'An example of a song aimed towards a male audience is what?'}, '56bfb8dca10cfb140055127b': {'truth': 'man-tending anthems', 'predicted': 'female-empowerment', 'question': 'With Jay Z what were her new themes?'}, '56bfb8dca10cfb140055127c': {'truth': 'co-producing credits', 'predicted': 'co-writing', 'question': 'What does she get credits for in her music?'}, '572a982b34ae481900deaba5': {'truth': 'Naval Reserve', 'predicted': 'Vietnam Veterans Against the War', 'question': 'What branch of the military did Kerry join?'}, '56ce5d70aab44d1400b886f7': {'truth': 'Active', 'predicted': 'demand side technologies', 'question': 'Are supply side solar technologies generally active or passive?'}, '56ce5d70aab44d1400b886f8': {'truth': 'Passive', 'predicted': 'demand side', 'question': 'Are demand side solar technologies generally active or passive?'}, '56cfdf65234ae51400d9bfce': {'truth': 'designing spaces that naturally circulate air', 'predicted': 'photovoltaics, concentrated solar power, solar thermal collectors, pumps, and fans', 'question': 'What is an active solar technique used to generate energy?'}, '56cfdf65234ae51400d9bfcf': {'truth': 'increase the supply of energy', 'predicted': 'convert sunlight into useful outputs', 'question': 'What does an active solar technique do?'}, '56cfdf65234ae51400d9bfd0': {'truth': 'reduce the need for alternate resources', 'predicted': 'convert sunlight into useful outputs', 'question': 'What does a passive solar technique do?'}, '572956496aef051400154d15': {'truth': 'Cenotaph in front of the Cabinet Building', 'predicted': 'The Cenotaph', 'question': \"What is the site for Bermuda's Remembrance Day?\"}, '56cfc4d6234ae51400d9bf4b': {'truth': 'iTunes', 'predicted': 'software that has been specifically designed to transfer media files to iPods', 'question': 'Rather than copying media files directly to it, what software must be used for this purpose so that they are accessible?'}, '570b26c7ec8fbc190045b88a': {'truth': 'Blades', 'predicted': 'Xbox 360 Dashboard', 'question': 'The tabs on the user interface were called what?'}, '571ae53e9499d21900609b99': {'truth': 'after the death of Emperor Constantius', 'predicted': '361', 'question': 'When did Athanasius return to his position as Patriarch?'}, '572fffb8a23a5019007fcc2c': {'truth': 'Octavian', 'predicted': 'Imperator Caesar', 'question': 'Who has been designated as the first Emperor of Rome?'}, '56e75e1200c9c71400d77018': {'truth': 'Laidlines', 'predicted': 'laidlines', 'question': 'What type of lines does wove paper not exhibit?'}, '572e6bacc246551400ce422c': {'truth': 'punk rock', 'predicted': 'new wave', 'question': 'What previous movement is post-punk often identified as coming after?'}, '57271739f1498d1400e8f386': {'truth': 'obesity', 'predicted': 'type 2 diabetes', 'question': 'Insulin resistance has been strongly linked to which health issue?'}, '572e9d6d03f9891900756839': {'truth': '4,414 km (2,743 mi)', 'predicted': '980', 'question': 'How many miles of roads are unpaved on Cyprus?'}, '5727b808ff5b5019007d935c': {'truth': 'law', 'predicted': 'nation', 'question': 'According to the rule of law, what should hold the determination for rules in a land? '}, '57293faa6aef051400154be6': {'truth': 'The first blacks to arrive in Bermuda in any numbers were free blacks from Spanish-speaking areas of the West Indies', 'predicted': 'different', 'question': 'Why is the black population in Bermuda different from that in the British West Indies and the United States?'}, '572f9c99a23a5019007fc7d5': {'truth': 'a 2, 3 or 4-digit sequential number with no significance as to device properties', 'predicted': 'a three-terminal device', 'question': 'What follows the 2N in a JEDEC EIA370?'}, '5732321ce17f3d1400422718': {'truth': 'officially', 'predicted': 'along with traditional religions and from his new Eastern capital, Constantine could be seen to embody both Christian and Hellenic religious interests', 'question': 'How did Constantine accept Christianity?'}, '5732321ce17f3d1400422719': {'truth': 'traditional religions', 'predicted': 'Hellenic', 'question': 'Besides the acceptance of Christianity, what other religious cults were tolerated?'}, '5732321ce17f3d140042271b': {'truth': 'Christian, Imperial, and \"divus\"', 'predicted': 'a Christian, Imperial, and \"divus', 'question': 'As what was Constantine honored when he died?'}, '571aeca132177014007e9fef': {'truth': 'Zyprexa, and the other involved Bextra', 'predicted': \"Eli Lilly's antipsychotic Zyprexa\", 'question': 'What drugs were involved in cases of the largest criminal fines?'}, '571aeca132177014007e9ff1': {'truth': 'Bristol-Myers Squibb, Eli Lilly, Pfizer, AstraZeneca and Johnson & Johnson', 'predicted': \"Eli Lilly's antipsychotic Zyprexa, and the other involved Bextra\", 'question': 'What companies have been involved with health care fraud cases?'}, '57300a06b2c2fd140056878c': {'truth': 'Dorchester', 'predicted': 'Charleston-North Charleston Urban Area', 'question': 'Charleston and Berkeley is combined with what other county to form a metropolitan statistical area?'}, '57313831497a881900248c72': {'truth': 'sea level rise', 'predicted': 'tropical cyclones', 'question': \"To what climate change condition does Tuvalu's low elevation make it susceptible?\"}, '572a470efed8de19000d5b65': {'truth': 'The Beinecke Rare Book and Manuscript', 'predicted': 'Beinecke Rare Book and Manuscript Library', 'question': 'Which museum feature the original copy of the Gutenberg Bible?'}, '572a470efed8de19000d5b67': {'truth': 'New Haven Museum and Historical Society', 'predicted': 'Eli Whitney Museum', 'question': \"There is a museum on Whitney Avenue that contain a variety of historical treasure, what is its' name?\"}, '572b749abe1ee31400cb83ad': {'truth': 'Absolute', 'predicted': 'absolute\"', 'question': 'What sort of idealist did Hegel define himself as?'}, '56d315d159d6e41400146224': {'truth': 'two', 'predicted': 'three weeks after completing his studies at the Warsaw Conservatory, he made his debut in Vienna', 'question': 'How many public performances did Chopin do where he made his debut after completing his education?'}, '56d0772c234ae51400d9c2f8': {'truth': 'Nidānakathā of the Jataka tales of the Theravada', 'predicted': 'Buddhaghoṣa', 'question': 'The Nidānakathā of the Jataka tales of the Theravada is attributed to who?'}, '56d0772c234ae51400d9c2fa': {'truth': 'Most accept that he lived, taught and founded a monastic order', 'predicted': 'historical facts', 'question': 'What do scholars recognize about the life of the Buddha?'}, '570f887880d9841400ab35a5': {'truth': 'world history', 'predicted': \"Britain's longest-lived. In 2015, she surpassed the reign of her great-great-grandmother, Queen Victoria, to become the longest-reigning British head of state and the longest-reigning queen regnant\", 'question': 'In the history of what is Elizabeth the longest reigning queen?'}, '572804792ca10214002d9b9f': {'truth': 'both are elements of a systematic mental framework', 'predicted': 'substances', 'question': 'What did Kant portray space and time to be?'}, '570d5aabfed7b91900d45f07': {'truth': 'North Africa', 'predicted': 'Kingdom of Aragon', 'question': 'Where did the Moriscos go when they were forced out of Spain?'}, '56dd1e8366d3e219004dabdd': {'truth': 'foreign workers', 'predicted': 'Islam', 'question': 'Who form the majority of Islamic residents of the Congo?'}, '572fc623947a6a140053cc97': {'truth': 'Colonel General Yuri Khatchaturov', 'predicted': 'President of Armenia, Serzh Sargsyan', 'question': 'Who is in charge of the the Armenian military?'}, '56fb2e3bf34c681400b0c1f9': {'truth': 'heresy', 'predicted': 'interstate conflict, civil strife, and peasant revolts', 'question': 'Along with controversy and schism, what upset the peace of the Church during the Late Middle Ages?'}, '57325b9fe99e3014001e670c': {'truth': 'disparaging individual decision-making', 'predicted': 'abrogate all responsibility and rights over their personal lives', 'question': \"How do the leaders of the Jehovah's Witnesses cultivate a system of unquestioning obedience?\"}, '5733f165d058e614000b663b': {'truth': 'former colonies and territories', 'predicted': 'those countries', 'question': 'Portuguese law continues to be a major influence for what?'}, '5733f165d058e614000b663c': {'truth': 'a civilian police force who work in urban areas', 'predicted': 'Polícia de Segurança Pública', 'question': 'What is the Policia de Seguranca Publica - PSP (Public Security Police)?'}, '5733f165d058e614000b663e': {'truth': 'the Public Ministry.', 'predicted': 'Polícia Judiciária – PJ (Judicial Police), a highly specialized criminal investigation police that is overseen by the Public Ministry', 'question': 'Which entity oversees the Judicial Police?'}, '56f7c779aef2371900625c0b': {'truth': 'możny', 'predicted': 'magnates', 'question': 'What is another name referring polish nobles?'}, '57257e8fcc50291900b28537': {'truth': 'typhoid fever', 'predicted': \"worry over the Prince of Wales's philandering\", 'question': \"What caused Prince Albert's death?\"}, '57257e8fcc50291900b28538': {'truth': 'their son, the Prince of Wales', 'predicted': 'Conroy and Lehzen', 'question': \"Who did Victoria blame for Prince Albert's death?\"}, '572a268f6aef051400155314': {'truth': 'The Polish Partition Sejm', 'predicted': 'the cession', 'question': 'What was ratified in 1773 in Prussia?'}, '57280c3f3acd2414000df310': {'truth': 'The London Fire Brigade', 'predicted': 'London Fire and Emergency Planning Authority', 'question': 'What agency provides fire fighting and rescue service in London?'}, '572913111d0469140077901d': {'truth': 'Freistaaten', 'predicted': 'Free States', 'question': 'What does Bavaria refer to itself as?'}, '57307352069b5314008320ed': {'truth': 'World Council of Hellenes Abroad put the figure at around 7 million worldwide', 'predicted': '3 million', 'question': 'How many Greeks do they believe would be an accurate number for census numbers ?'}, '572cb837750c471900ed4cf4': {'truth': '103.5 million', 'predicted': '56.3 million traffic cases, 20.4 million criminal cases, 19.0 million civil cases, 5.9 million domestic relations cases, and 1.9 million juvenile cases. In 2010, state appellate courts received 272,795', 'question': 'How many new cases were filed in 2010?'}, '56dc544814d3a41400c267c1': {'truth': 'RNA', 'predicted': 'DNA', 'question': 'What constitutes the viral genome?'}, '57283f892ca10214002da184': {'truth': '$5.00 per two-sided disc', 'predicted': '$1.00', 'question': 'How much did LaserDiscs cost to produce by the end of the 1980s?'}, '5728ea364b864d1900165087': {'truth': '905–914', 'predicted': '10th century', 'question': \"When is the first known use of 'samurai'?\"}, '5729355b1d0469140077916e': {'truth': 'regional ancestry', 'predicted': 'Race\" is still sometimes used within forensic anthropology (when analyzing skeletal remains), biomedical research, and race-based medicine. Brace has criticized this, the practice of forensic anthropologists for using the controversial concept \"race', 'question': 'What term would Brace prefer forensic anthropologists use?'}, '5728cbea3acd2414000dfeac': {'truth': 'Detroit techno', 'predicted': 'science fiction', 'question': 'What genre of music featured robotic themes?'}, '5728cbea3acd2414000dfeae': {'truth': 'Memorial Day Weekend', 'predicted': 'late May', 'question': 'When does \"Movement\" occur?'}, '56e9644c0b45c0140094cdef': {'truth': '9,045', 'predicted': '1,185', 'question': 'How many pupils lived in another municipality?'}, '572a8990f75d5e190021fb56': {'truth': 'Deobandis', 'predicted': 'Barelvis', 'question': 'What group makes up a larger percentage of people in India?'}, '572a8990f75d5e190021fb58': {'truth': 'Mathematics, Computers and science', 'predicted': 'Quranic education', 'question': 'What disciplines does India want to introduce to madaris?'}, '5732549b0fdd8d15006c69c8': {'truth': 'biographer', 'predicted': '\"the political education of General Eisenhower\"', 'question': 'What was Blanche Wiesen Cook in relation to Eisenhower?'}, '5727972a708984140094e1a7': {'truth': 'International Program on the Elimination of Child Labour (IPEC)', 'predicted': 'setting the international law', 'question': 'What did the United Nations take charge of with regards to child labour?'}, '5727972a708984140094e1a8': {'truth': 'progressively eliminate child labour', 'predicted': 'strengthening national capacities', 'question': 'What is the aim of this?'}, '56dcfc7b66d3e219004dab7b': {'truth': 'Bantus', 'predicted': 'Congolese Human Rights Observatory', 'question': 'Who is considered to own members of the Pygmies?'}, '56dcfc7b66d3e219004dab7c': {'truth': 'pets', 'predicted': 'property', 'question': 'The treatment of Pygmies has been compared to the treatment of what?'}, '5726872c5951b619008f75cd': {'truth': \"Aung San Suu Kyi's party won a majority in both houses, ending military rule.\", 'predicted': 'For most of its independent years', 'question': 'Has the country been able to overcome the problems of government with the previous regime?'}, '57279c1aff5b5019007d90eb': {'truth': 'ecclesiastical', 'predicted': 'monastic settlement', 'question': 'What kind of settlement was Cork?'}, '5723df4df6b826140030fcd0': {'truth': 'Masonic communication', 'predicted': 'inter-visitation', 'question': 'What, besides Recognition, must happen between two Grand Lodges in order for them to be considered in amity? '}, '56de5ba04396321400ee284f': {'truth': '1960s', 'predicted': '1956', 'question': 'In what decade did colleges of technology gain the University designation?'}, '570a6f996d058f1900182e5c': {'truth': 'stoic', 'predicted': 'Aristotle', 'question': 'What school of thought saw emotion as an impediment to virtue?'}, '572f6c2cb2c2fd14005680f8': {'truth': 'Khilji dynasty', 'predicted': 'Malik Kafur', 'question': 'Which entity subsumed the Kakatiya dynasty?'}, '5726fc5d5951b619008f8412': {'truth': '32', 'predicted': '26', 'question': 'How old was Joséphine de Beauharnais when she was married to Napoleon?'}, '572f6eacb2c2fd1400568109': {'truth': 'on a swing arm', 'predicted': 'bottom of the polycarbonate layer', 'question': 'Where is the semiconductor laser found in a CD player?'}, '572f6eacb2c2fd140056810a': {'truth': '780 nm', 'predicted': 'semiconductor laser', 'question': 'What wavelenght is used to pull data from a CD?'}, '572f6eacb2c2fd140056810d': {'truth': 'change in height between pits and lands', 'predicted': 'semiconductor laser', 'question': 'What createds the change in light reflected off of a CD?'}, '5735b062dc94161900571f25': {'truth': 'three-star', 'predicted': 'The Garden Hotel', 'question': 'What type of hotel is Aloha Inn?'}, '5735b062dc94161900571f26': {'truth': 'five-star', 'predicted': 'Hyatt Regency', 'question': \"De L'Annapurna is an example of what sort of hotel?\"}, '573236d2e17f3d1400422737': {'truth': 'non-Christian practices', 'predicted': 'an Augustan form of principate', 'question': 'What did Julian try to restore to the empire?'}, '572fb2fb947a6a140053cbad': {'truth': 'Yerevan', 'predicted': 'Tsitsernakaberd hill', 'question': 'Where is the memorial for the Armenian Genocide?'}, '570d7be1b3d812140066d9df': {'truth': '500 people', 'predicted': 'c.\\u2009500', 'question': 'How many were killed by the Communards?'}, '570629ba52bb891400689918': {'truth': 'Digital Audio Tape (DAT) SP', 'predicted': 'Compact Disc', 'question': 'Other than CD parameters, what else can be used as parameter references?'}, '5730b108069b531400832269': {'truth': 'nine islands', 'predicted': 'Eight', 'question': 'How many islands are in the Tuvalu group?'}, '572805f5ff5b5019007d9b1c': {'truth': 'the large restriction on possible patterns', 'predicted': 'without relying on the BOM', 'question': 'Why is it possible to distinguish UTF-8 from other protocols?'}, '5726ec2bdd62a815002e955a': {'truth': 'Chinese troops', 'predicted': 'far greater', 'question': 'Did the UN troops or Chinese troops experience more war casualties?'}, '5726ec2bdd62a815002e955b': {'truth': 'called a conference in Shenyang', 'predicted': 'accelerate the construction of railways and airfields in the area', 'question': 'What did Zhou Enlai do as a result of the significant amount of Chinese casualties?'}, '5726ec2bdd62a815002e955c': {'truth': \"discuss the PVA's logistical problems\", 'predicted': 'accelerate the construction of railways and airfields in the area', 'question': 'What was the purpose of the Shengyang meeting?'}, '5726ec2bdd62a815002e955d': {'truth': 'These commitments did little to directly address the problems', 'predicted': 'little to directly address the problems confronting PVA troops.', 'question': 'Did the actions of the Chinese fix their problems?'}, '56e7860900c9c71400d77235': {'truth': 'historical documents', 'predicted': 'Nanjing was the international hub of East Asia', 'question': 'Where did the information on registered households during that period originate?'}, '56fb7c108ddada1400cd6459': {'truth': 'Marco Polo', 'predicted': 'one of the traders', 'question': 'Who wrote The Travels of Marco Polo?'}, '56fb7c108ddada1400cd645b': {'truth': 'Italy', 'predicted': 'Europe', 'question': 'In what region was gold coinage first reintroduced?'}, '57313a17497a881900248c8f': {'truth': 'Outer Mongolia', 'predicted': 'Albazin', 'question': 'Where did Kangxi lead an army?'}, '57313a17497a881900248c90': {'truth': 'Dzungars', 'predicted': 'Albazin, the far eastern outpost of the Tsardom of Russia', 'question': 'Who did Kangxi fight?'}, '572a8395111d821400f38b91': {'truth': '24.4', 'predicted': '39.3', 'question': 'How many miles long is Metrorail?'}, '572a8395111d821400f38b93': {'truth': 'three', 'predicted': '21', 'question': 'How many lines does Metromover have?'}, '56dfb0e97aa994140058dfe7': {'truth': 'metal bed frame', 'predicted': 'disturbed the instrument', 'question': 'What did Bell think was wrong with the bed, which prevented his machine from finding the bullet?'}, '56e141e2e3433e1400422d07': {'truth': '8.3%', 'predicted': '15.8%', 'question': 'What percentage of the citys population is italian?'}, '5727d11d3acd2414000ded1a': {'truth': '17 °F', 'predicted': '83 °F (28 °C)', 'question': \"What is Oklahoma's record low temperature for Nov 11?\"}, '56e06d44231d4119001ac105': {'truth': 'Muddy consonants', 'predicted': 'a series of muddy consonants', 'question': 'What is /b/ representative of, in addition to aspirated and unaspirated consonants?'}, '56fa2008f34c681400b0bfcb': {'truth': 'diffuse-porous', 'predicted': 'the pores are evenly sized', 'question': 'Is maple wood diffuse-porous or ring-porous?'}, '570b3f0fec8fbc190045b913': {'truth': '1,200', 'predicted': '2,000', 'question': 'How many troops did the US initially send to the Philippines?'}, '56e78bb537bdd419002c4109': {'truth': '(Boni 渤泥)', 'predicted': 'Borneo', 'question': 'What visiting king died in China in 1408?'}, '56fb2c94f34c681400b0c1ec': {'truth': 'the Medieval Warm Period', 'predicted': 'Medieval Warm Period climate change', 'question': 'What event led to larger crop yields in the High Middle Ages?'}, '5726ee62dd62a815002e9584': {'truth': 'the 18th century', 'predicted': '18th century through late 20th century', 'question': 'What time period did the history of science begin to take a progressive narrative?'}, '5726a1d5708984140094cc67': {'truth': 'Philip Magnus', 'predicted': 'Robert Murray', 'question': 'Who wrote a biography of Burke?'}, '57320ba7e99e3014001e6478': {'truth': 'plebeian', 'predicted': 'artisan class', 'question': 'At the end of the regal period, what class was kept out of the state political and priesthood arenas?'}, '56f71740711bf01900a44935': {'truth': 'it does not', 'predicted': 'not establish any rights and obligations', 'question': 'Does the bilateral treaty between Switzerland and the European Union establish rights or obligations amongst the EU and its member states?'}, '56e02a437aa994140058e2e1': {'truth': 'print market', 'predicted': 'shrinking print', 'question': 'Comics continue to thrive regardless of the decrease in what market?'}, '57326cefe99e3014001e67a8': {'truth': 'Annabella Sciorra and Ron Eldard', 'predicted': 'Nancy Savoca', 'question': \"Who starred in 'True Love'?\"}, '56e166ffcd28a01900c67877': {'truth': 'Hal B. Wallis', 'predicted': 'Maxwell Anderson', 'question': 'Who produced Anne of the Thousand Days?'}, '572837e7ff5b5019007d9f46': {'truth': 'changed towards a generally autonomous model', 'predicted': 'It was liberalized', 'question': \"What happened to Russia's subdivision of government? \"}, '57267d5c5951b619008f7487': {'truth': 'burned', 'predicted': 'transcribed and edited', 'question': \"What did Beatrice do with the origional volumes of her mother's diaries?\"}, '572a4e22fed8de19000d5b89': {'truth': 'an increase in the active secretion, or there is an inhibition of absorption', 'predicted': 'there is an increase in the active secretion', 'question': 'What is secretory diarrhea?'}, '56d11b8d17492d1400aab997': {'truth': 'Her father', 'predicted': 'lawyer', 'question': 'Lee modeled the character Atticus after what laywer?'}, '572814c34b864d190016441e': {'truth': 'Ratchet & Clank Future: Tools of Destruction', 'predicted': \"Future: Tools of Destruction, Warhawk and Uncharted: Drake's Fortune\", 'question': 'Which Ratchet & Clank title debuted at E3 2007?'}, '572814c34b864d1900164422': {'truth': 'Devil May Cry 4', 'predicted': 'Metal Gear Solid 4: Guns of the Patriots', 'question': 'Which much anticipated third-party game with the name of a month of the year in it did Sony show at E3 2007?'}, '57303815947a6a140053d2c7': {'truth': 'the Golrizan Festival', 'predicted': 'Tehran World Festival', 'question': 'What Iranian film festival in 1954 was the progenitor of future film festivals in 1969 and 1973?'}, '57303815947a6a140053d2c9': {'truth': '65', 'predicted': '25', 'question': 'How many commercial films were produced yearly on average by the end of the 1960s in Iran?'}, '570e08690dc6ce1900204d9b': {'truth': 'important food', 'predicted': 'keystone species of the ecosystem', 'question': 'Why is the krill so important to the Antarctic area?'}, '572771b85951b619008f8a08': {'truth': 'Tetraploid', 'predicted': 'cultivated cotton', 'question': 'What type of cotton has two separate genomes within its nucleus? '}, '572771b85951b619008f8a0b': {'truth': 'diploid counterparts', 'predicted': 'two separate genomes within its nucleus, referred to as the A and D genomes', 'question': 'In order to understand the tetraploid forms, what must be used as a comparison in cotton gene sequencing?'}, '573036c4947a6a140053d2b3': {'truth': 'one direction', 'predicted': 'multiple wavelengths long', 'question': \"How long are the wire antenna's that the voltage and current waves travel in the same direction?\"}, '5728d7c4ff5b5019007da7f4': {'truth': 'about $3,600', 'predicted': '$6,600', 'question': 'How much was the average cost of hospital stays for asthma-related issues for children??'}, '5728d7c4ff5b5019007da7f7': {'truth': 'the lowest income communities', 'predicted': 'children', 'question': 'Who was more likely to seek hospital help in the US for asthma reasons?'}, '57265bdaf1498d1400e8dd1e': {'truth': 'Spanish were never able to effectively police the border region and the backwoods settlers', 'predicted': 'the Spanish were never able to effectively police the border region', 'question': 'Were the Spanish able to police the backwoods settlements '}, '5728283a2ca10214002d9f73': {'truth': 'unjointed paired extensions of the body wall', 'predicted': 'limbs', 'question': 'What are parapodia?'}, '56f71a5e3d8e2e1400e3735d': {'truth': 'Tito', 'predicted': 'Milan Gorkić', 'question': 'Who became Secretary-General of the CPY after the prior one was murdered?'}, '570cff2cb3d812140066d395': {'truth': '1:23', 'predicted': 'Isaiah 7:14', 'question': \"Which verse in Matthew is believed to refer to Isaiah's prohecy of the Virgin Mary?\"}, '570cff2cb3d812140066d396': {'truth': 'Luke', 'predicted': 'John', 'question': \"Which Gospel writer provided a version of the virgin birth that was different than Matthew's?\"}, '57264be9dd62a815002e80bd': {'truth': 'The lights can be switched on for 24-hrs/day, or a range of step-wise light regimens to encourage the birds to feed often and therefore grow rapidly', 'predicted': 'controlled', 'question': 'What type of conditions are used to increase the weight and profitability of commercial turkeys?'}, '57283e8bff5b5019007d9fe3': {'truth': 'Alamogordo Army Airfield,', 'predicted': 'at the bombing range near Alamogordo Army Airfield', 'question': 'Where did the first atomic blast test take place?'}, '57270821708984140094d8d5': {'truth': 'childhood obesity', 'predicted': 'nutrition literacy', 'question': 'What does the initiative specifically target?'}, '5726c7305951b619008f7dd5': {'truth': 'for a financial bailout from the federal government', 'predicted': 'a financial bailout from the federal government to cover significant debts', 'question': \"What reason did David Buffett give for Norfolk Island surrendering its' tax-free status?\"}, '5728f9a14b864d190016515d': {'truth': 'Yangon Stock Exchange Joint Venture Co. Ltd', 'predicted': 'First Myanmar Investment Co., Ltd.', 'question': 'What is the name of the business that first rang a bell to begin in the winter of 2014 in Myanmar ?'}, '5728f9a14b864d190016515e': {'truth': 'Myanma Economic Bank sharing 51 percent', 'predicted': 'Yangon Stock Exchange Joint Venture Co. Ltd', 'question': 'W is set to to be the major stock holder of the business that first rang a bell to begin in the winter of 2014 in Myanmar ?'}, '5728f9a14b864d190016515f': {'truth': \"Japan's Daiwa Institute of Research Ltd 30.25 percent and Japan Exchange Group 18.75 percent.\", 'predicted': 'Myanmar', 'question': 'Did other countries actively participate in business that first rang a bell to begin in the winter of 2014 in Myanmar ?'}, '56dfb914231d4119001abd05': {'truth': 'lodging', 'predicted': 'meals', 'question': 'What is the main service of an inn, now also attainable in motels, hotels and lodges?'}, '5726dcbf708984140094d3fc': {'truth': '22,495,187', 'predicted': '12,214,853', 'question': 'How many votes did Goodluck get in 2011?'}, '57255c8a69ff041400e58c38': {'truth': 'boosted after the establishment of the Third French Republic', 'predicted': \"In 1870, republican sentiment in Britain, fed by the Queen's seclusion\", 'question': 'How was the republican sentiment in Britain changed?'}, '57267f57f1498d1400e8e1ce': {'truth': \"the tenth anniversary of her husband's death\", 'predicted': 'typhoid fever', 'question': \"After what event did the Prince of Wales' health begin to improve?\"}, '56e83bdf37bdd419002c44be': {'truth': 'sere', 'predicted': 'haber\" and has done away with \"ser', 'question': 'When forming compound tenses in Spanish, what auxiliary is no longer used?'}}}\n"
          ]
        }
      ],
      "source": [
        "import sklearn\n",
        "result, text= model.eval_model(randomtest, acc=sklearn.metrics.accuracy_score, verbose= True)\n",
        "print(result)#contains evaluation result\n",
        "print(text)#a dict containing the the correct_text, similar_text and incorrect text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "526aWiKBEcbG"
      },
      "source": [
        "Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "IS3WVykoJ1sv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102,
          "referenced_widgets": [
            "6bced45727b04b6f8cdf59c62c102f18",
            "58abe491fa774df4ab88b35eb8221f9d",
            "66af14aa77024510b8951a5031a71528",
            "101afed5cbce4403a31b07a4083296f3",
            "88c6581dab4a495088f961bebdc43b6a",
            "35bc34aa611d464db5f37a0943cc7757",
            "70c156b41478458ea6f18cdda0617520",
            "9d9b0cd16c0d494fa5f2cdc2874d9ae8",
            "a71ad8c5c63f47a687a31a77aa0dd294",
            "3b1cb077478245fb8ea2d520002fd861",
            "4043b93889d44b408fc24d412e94b6e9"
          ]
        },
        "outputId": "596d34c2-13f1-47a3-c74a-860e604d67be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "convert squad examples to features: 100%|██████████| 11873/11873 [01:26<00:00, 137.55it/s]\n",
            "add example index and unique id: 100%|██████████| 11873/11873 [00:00<00:00, 498657.92it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running Prediction:   0%|          | 0/1540 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6bced45727b04b6f8cdf59c62c102f18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '56ddde6b9a695914005b9628', 'answer': ['France']}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "import sklearn\n",
        "dev = [item for topic in dev['data'] for item in topic['paragraphs'] ]\n",
        "predtn1, raw_outputs = model.predict(dev, n_best_size=2)#return a list of dict+ containg each question mapped to its answer and a list of dicts of question id mapped to probability score of the answer\n",
        "\n",
        "predtn1[0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "BAVk2q0qd0tJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "848fdade-aa60-4e24-dc58-b58c5750140c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '56ddde6b9a695914005b9628', 'probability': [0.9999629470302284]}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "raw_outputs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "dJxWE2orAlQI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "e69132cd2c144b3abaf3375e97bd718e",
            "beb81b9cabc045edb90607884d606dff",
            "1283c6afa9a643549a608f5403b64cd6",
            "65760d34e15248df8776b5e68a139730",
            "bc5ca9d6082a47c9ad7e2cd1ef5cb081",
            "bec2c38f61074697933fb0ca5d60cbe4",
            "cd3ed8f3539547efb0679f6cf34eedcc",
            "d53c11f608e8464fb4084c2b056039bc",
            "3b3c7a9692b94f8d92b00a056c046086",
            "9721dc4fcb344111ade964823eea817b",
            "202fd5aa3bfe4dd1a49794ac614d3d08"
          ]
        },
        "outputId": "81f51198-ae3a-4929-ee0a-8c2be69a6760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 6636.56it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 9058.97it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e69132cd2c144b3abaf3375e97bd718e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "context_text = \"Linear regression is used for predicting quantitative values, such as an individual’s salary. In order to predict qualitative values, such as whether a patient survives or dies, or whether the stock market increases or decreases, Fisher proposed linear discriminant analysis in 1936.\"\n",
        "predtn2, raw_outputs = model.predict(\n",
        "    [\n",
        "        {\n",
        "            \"context\": context_text,\n",
        "            \"qas\": [\n",
        "                {\n",
        "                    \"question\": \"Who proposed linear discriminant analysis?\",\n",
        "                    \"id\": \"0\",\n",
        "                }\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "n_best_size=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "umrczR-m9s0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87b3975d-a268-4f45-b4a2-c1d650d0436a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': '0', 'answer': ['Fisher']}]\n",
            "[{'id': '0', 'probability': [0.9063219973254493]}]\n"
          ]
        }
      ],
      "source": [
        "print(predtn2)#return a list of dict containg each question mapped to its answer \n",
        "\n",
        "print(raw_outputs)#a list of dicts of question id mapped to probability score of the answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "wVNLO9fd0OTL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "8e6af3d7709f4cf7abd6ce3c3ba34e87",
            "c37422984b5b4755b870f6d28cb6252f",
            "e7f839bc6f5a48d981a19d1137ca7003",
            "9317179a13974bb49f9f336b18f763b8",
            "bd361453dc4845b0846182ab714c433a",
            "1c81f85dec884da092cc769e7d150e51",
            "77c887ba837d488a8a296d1b06ae3d0e",
            "8eee2597e0c343a0b62ada89627b3a38",
            "16b57c42fc8843409289ccb84f4d24b5",
            "eb4e8004a24c45ccb6f5d1854145c4dc",
            "640487b39db640268968ba41c19a837b"
          ]
        },
        "outputId": "97b2d7e4-1e4b-430d-b95d-805e64fe7606"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 5817.34it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 10180.35it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e6af3d7709f4cf7abd6ce3c3ba34e87"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "context_text = \"Thomas Alva Edison was an American inventor and businessman who has been described as America's greatest inventor.One of his inventions, is the phonograph\"\n",
        "predtn3, raw_outputs = model.predict(\n",
        "    [\n",
        "        {\n",
        "            \"context\": context_text,\n",
        "            \"qas\": [\n",
        "                {\n",
        "                    \"question\": \"Who invented phonograph?\",\n",
        "                    \"id\": \"0\",\n",
        "                }\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "n_best_size=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "dGl8u9Sq0Osh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4248285c-c7de-461a-b324-408bfb7b7456"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': '0', 'answer': ['Thomas Alva Edison']}]\n",
            "[{'id': '0', 'probability': [0.9999271013245725]}]\n"
          ]
        }
      ],
      "source": [
        "print(predtn3)#return a list of dict containg each question mapped to its answer \n",
        "\n",
        "print(raw_outputs)#a list of dicts of question id mapped to probability score of the answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "s2nRIDir0ciP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "fd2e3ce1427f4ba2850382681ee0b9c6",
            "7e8ad34bfd75431aa50c151de2959744",
            "1eaabfb73ba448b1886d3ef1288daf42",
            "dcd82f7fa2cb41088c43b76acf0d36df",
            "d138030b492b4f6d8484c0d21855966e",
            "72a2a65311ec4a898ca5ea4904121a4a",
            "56de35c0653348c8b57f714a4130b4f0",
            "e9766acea27a4e948f3f1e6b7da6b779",
            "d45bbdbd8cad46ac8df9d8c41a3f5b6c",
            "789bdd40b10e45198cb1dafcc0ac680d",
            "a0e9f456631c437c80e1fb283c1bdf64"
          ]
        },
        "outputId": "6039669b-ccd9-4e72-a0a1-7dc134144c4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 56.94it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 10280.16it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd2e3ce1427f4ba2850382681ee0b9c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': '0', 'answer': ['3', '3 hours to work in the morning and 5']}]\n",
            "[{'id': '0', 'probability': [0.8082874890362919, 0.1912366274631815]}]\n"
          ]
        }
      ],
      "source": [
        "context_text = \"Mary drove for 3 hours to work in the morning and 5 hours in the evening to her house\"\n",
        "predtn4, raw_outputs = model.predict(\n",
        "    [\n",
        "        {\n",
        "            \"context\": context_text,\n",
        "            \"qas\": [\n",
        "                {\n",
        "                    \"question\": \"How many hours did Mary drive for today?\",\n",
        "                    \"id\": \"0\",\n",
        "                }\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "n_best_size=2)\n",
        "print(predtn4)\n",
        "print(raw_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "jca_0BLt0c7D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c25ccd03-768a-4b34-d3a1-21a90b91cc4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': '0', 'answer': ['3', '3 hours to work in the morning and 5']}]\n",
            "[{'id': '0', 'probability': [0.8082874890362919, 0.1912366274631815]}]\n"
          ]
        }
      ],
      "source": [
        "print(predtn4)#return a list of dict containg each question mapped to its answer \n",
        "\n",
        "print(raw_outputs)#a list of dicts of question id mapped to probability score of the answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "7FDAWS_i4G3x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "6a140ae435984892a990ab5689cc3bed",
            "e0fe947eed354451b50a82774d8200ba",
            "968a561ecb1f468aa58133855f5aaf0c",
            "56055eb72e714ec988ac46fcf4a7cdea",
            "1766ac60f97b4643af7062ad334f49df",
            "c0277559c46f4249a0356952e9160736",
            "2cd141fdd3114531a3f83aef8216d3bb",
            "4c09acb9ee8d45cebf8c644963616cae",
            "8afc03c986fe4ea2a3b2076c6e8fd43e",
            "025193eef48741e49cc6dece60b9d1a1",
            "8d29f3978f634a899d1f9463b15dff3e"
          ]
        },
        "outputId": "fe7bb302-c905-48dd-c556-ebec8c3e04a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 10.47it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 9383.23it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a140ae435984892a990ab5689cc3bed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['its inhibition of the production of phosphodiesterase', 'hibition of the production of phosphodiesterase']\n",
            "[0.7526491206225663, 0.24626585011519841]\n",
            "\n",
            "Most likely answer is:  its inhibition of the production of phosphodiesterase\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        " \n",
        "paragraph = \"Caffeine, the stimulant in coffee, has been called \\u201Cthe most widely used psychoactive substance on Earth.\\u201D Snyder, Daly, and Bruns have recently proposed that caffeine affects behavior by countering the activity in the human brain of a naturally occurring chemical called adenosine. Adenosine normally depresses neuron firing in many areas of the brain. It apparently does this by inhibiting the release of neurotransmitters, chemicals that carry nerve impulses from one neuron to the next.  Like many other agents that affect neuron firing, adenosine must first bind to specific receptors on neuronal membranes. There are at least two classes of these receptors, which have been designated A1 and A2. Snyder et al. propose that caffeine, which is structurally similar to adenosine, is able to bind to both types of receptors, which prevents adenosine from attaching there and allows the neurons to fire more readily than they otherwise would.  For many years, caffeine\\u2019s effects have been attributed to its inhibition of the production of phosphodiesterase, an enzyme that breaks down the chemical called cyclic AMP. A number of neurotransmitters exert their effects by first increasing cyclic AMP concentrations in target neurons. Therefore, prolonged periods at the elevated concentrations, as might be brought about by a phosphodiesterase inhibitor, could lead to a greater amount of neuron firing and, consequently, to behavioral stimulation. But Snyder et al. point out that the caffeine concentrations needed to inhibit the production of phosphodiesterase in the brain are much higher than those that produce stimulation. Moreover, other compounds that block phosphodiesterase\\u2019s activity are not stimulants.  To buttress their case that caffeine acts instead by preventing adenosine binding, Snyder et al. compared the stimulatory effects of a series of caffeine derivatives with their ability to dislodge adenosine from its receptors in the brains of mice. \\u201CIn general,\\u201D they reported, \\u201Cthe ability of the compounds to compete at the receptors correlates with their ability to stimulate locomotion in the mouse; i.e., the higher their capacity to bind at the receptors, the higher their ability to stimulate locomotion.\\u201D Theophylline, a close structural relative of caffeine and the major stimulant in tea, was one of the most effective compounds in both regards.  There were some apparent exceptions to the general correlation observed between adenosine receptor binding and stimulation. One of these was a compound called 3-isobutyl-1-methylxanthine (IBMX), which bound very well but actually depressed mouse locomotion. Snyder et al. suggest that this is not a major stumbling block to their hypothesis. The problem is that the compound has mixed effects in the brain, a not unusual occurrence with psychoactive drugs. Even caffeine, which is generally known only for its stimulatory effects, displays this property, depressing mouse locomotion at very low concentrations and stimulating it at higher ones.\" #@param {type:\"string\"}\n",
        "question = \"what is caffeine's effects attributed to?\" #@param {type:\"string\"}\n",
        " \n",
        "algorithm = 'BERT' #@param{type:\"string\"}\n",
        "\n",
        "model = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/Squad/model.pkl', 'rb'))\n",
        "\n",
        "predtn5, raw_outputs = model.predict(\n",
        "    [\n",
        "        {\n",
        "            \"context\": paragraph,\n",
        "            \"qas\": [\n",
        "                {\n",
        "                    \"question\": question,\n",
        "                    \"id\": \"0\",\n",
        "                }\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "n_best_size=2)\n",
        "\n",
        "print(predtn5[0]['answer'])\n",
        "print(raw_outputs[0]['probability'])\n",
        "\n",
        "print(\"\\nMost likely answer is: \",predtn5[0]['answer'][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gs_zhv9NS9Z3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e0bb6bf817394d7a8a5bb789ed8238d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3be670194ed49949a11a9eaf1a49c9e",
              "IPY_MODEL_5ad0208f33904ed18d087e6a3752b5d6",
              "IPY_MODEL_21abe1facd614c27a18110fe18e772b7"
            ],
            "layout": "IPY_MODEL_9087d7f5df354912bd6abbdd06daace1"
          }
        },
        "b3be670194ed49949a11a9eaf1a49c9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3df73aca057045308693f99093d5b288",
            "placeholder": "​",
            "style": "IPY_MODEL_8a440f05040c43b98480638fc2701cd4",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "5ad0208f33904ed18d087e6a3752b5d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f5851cfd20c4437b8d7c4e755195b67",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a00f9a9b83a49e29cc411b10be9a9ef",
            "value": 570
          }
        },
        "21abe1facd614c27a18110fe18e772b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddba09d5cad04f2e9b783ca5ae17156f",
            "placeholder": "​",
            "style": "IPY_MODEL_43a1112936604087baf7f00b8cc96007",
            "value": " 570/570 [00:00&lt;00:00, 15.7kB/s]"
          }
        },
        "9087d7f5df354912bd6abbdd06daace1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3df73aca057045308693f99093d5b288": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a440f05040c43b98480638fc2701cd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f5851cfd20c4437b8d7c4e755195b67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a00f9a9b83a49e29cc411b10be9a9ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ddba09d5cad04f2e9b783ca5ae17156f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43a1112936604087baf7f00b8cc96007": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7748e936d10446fd8c949559270dcc30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4671acf233342d99565528bce958d36",
              "IPY_MODEL_0195a29e7cd549eca1a37636a779f722",
              "IPY_MODEL_635b4a176f57458ba86171073529045c"
            ],
            "layout": "IPY_MODEL_917c9a0e69314039abb81df163ea1ad6"
          }
        },
        "f4671acf233342d99565528bce958d36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_282766e0b4364d79bb04f1bd09d98d82",
            "placeholder": "​",
            "style": "IPY_MODEL_a348b98d62cc482fb1c42220af423958",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "0195a29e7cd549eca1a37636a779f722": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_852e8d4f9a4940d7999a6c85812f2615",
            "max": 435779157,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_587d0d7788194da4ae7346de594afa17",
            "value": 435779157
          }
        },
        "635b4a176f57458ba86171073529045c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8de2548d5f90445e8cf571dc2f1ac5dd",
            "placeholder": "​",
            "style": "IPY_MODEL_5a608247c5c44f61ad70e99c75810914",
            "value": " 436M/436M [00:01&lt;00:00, 240MB/s]"
          }
        },
        "917c9a0e69314039abb81df163ea1ad6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "282766e0b4364d79bb04f1bd09d98d82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a348b98d62cc482fb1c42220af423958": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "852e8d4f9a4940d7999a6c85812f2615": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "587d0d7788194da4ae7346de594afa17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8de2548d5f90445e8cf571dc2f1ac5dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a608247c5c44f61ad70e99c75810914": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02260e2c73bb4940b7f4f2363138f9ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9e73a36174b46f4875e2118fc8282ac",
              "IPY_MODEL_3e83c058b27943dd8c4a16b62d4059d3",
              "IPY_MODEL_ed4a9180f159442ca0b2b9d005439b7b"
            ],
            "layout": "IPY_MODEL_2897359a25b34d98ae953c3b31a14edf"
          }
        },
        "f9e73a36174b46f4875e2118fc8282ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fc33e971cc64a66a7a08a61e1e49dc3",
            "placeholder": "​",
            "style": "IPY_MODEL_af8ce5b495fb493b9f81d83a378c672d",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "3e83c058b27943dd8c4a16b62d4059d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42fff9765bea4ababac7d3af2af83e2b",
            "max": 213450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83f10ef5f55d48f9b7cfd742790fc99f",
            "value": 213450
          }
        },
        "ed4a9180f159442ca0b2b9d005439b7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6aadc785a9ed4561b9019071fdf5e071",
            "placeholder": "​",
            "style": "IPY_MODEL_2acd9dc35c71468798ef36501d017337",
            "value": " 213k/213k [00:00&lt;00:00, 1.15MB/s]"
          }
        },
        "2897359a25b34d98ae953c3b31a14edf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fc33e971cc64a66a7a08a61e1e49dc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af8ce5b495fb493b9f81d83a378c672d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42fff9765bea4ababac7d3af2af83e2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83f10ef5f55d48f9b7cfd742790fc99f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6aadc785a9ed4561b9019071fdf5e071": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2acd9dc35c71468798ef36501d017337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d292ffd50c444ad687d651a065795c09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_021eb80e4a814c2b8a7eda3daa7a1efe",
              "IPY_MODEL_8df2c1ec2ecb422f99be7b783e770437",
              "IPY_MODEL_df908dd56bfc465581b54d0c20e48d2d"
            ],
            "layout": "IPY_MODEL_53ed3f6483e34e2f80bcf3fb4922880c"
          }
        },
        "021eb80e4a814c2b8a7eda3daa7a1efe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62f57cdd09f245a295b22659a2817664",
            "placeholder": "​",
            "style": "IPY_MODEL_1a15958fa202446bb0cd5220b344e580",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "8df2c1ec2ecb422f99be7b783e770437": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_093dce8ad0a345f49c94e0211b0c64f5",
            "max": 29,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fdcac9c7c13e41cba39d2fe6ce4372ea",
            "value": 29
          }
        },
        "df908dd56bfc465581b54d0c20e48d2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_801933e2696d4cfdae0a48de176201dd",
            "placeholder": "​",
            "style": "IPY_MODEL_30293d4b4fc34fe1bf708b1f91ea896f",
            "value": " 29.0/29.0 [00:00&lt;00:00, 1.08kB/s]"
          }
        },
        "53ed3f6483e34e2f80bcf3fb4922880c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62f57cdd09f245a295b22659a2817664": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a15958fa202446bb0cd5220b344e580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "093dce8ad0a345f49c94e0211b0c64f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdcac9c7c13e41cba39d2fe6ce4372ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "801933e2696d4cfdae0a48de176201dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30293d4b4fc34fe1bf708b1f91ea896f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f66e41bd35584a70914b5ad72ec03e30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7c7fda5a5fa6477e93684a51f6e6c1b4",
              "IPY_MODEL_9c8a5f4a384c4c9ca8616b96ba81e1d5",
              "IPY_MODEL_1209338024d7450c97ffb91dd87bb1ce"
            ],
            "layout": "IPY_MODEL_b959c58ab43f48debe5c899313c47bc8"
          }
        },
        "7c7fda5a5fa6477e93684a51f6e6c1b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21a3155086c3454fb01497cf8d1cca51",
            "placeholder": "​",
            "style": "IPY_MODEL_97fc1475c69a49f28d2ac269b2b9fcc8",
            "value": "Epoch 2 of 2: 100%"
          }
        },
        "9c8a5f4a384c4c9ca8616b96ba81e1d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c616a40287d4df19ac0ad0bfdfd2416",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d5b1d418ec345fda161038c61f24ea2",
            "value": 2
          }
        },
        "1209338024d7450c97ffb91dd87bb1ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c212a83a4a34c38993046642b1d523a",
            "placeholder": "​",
            "style": "IPY_MODEL_95714e5fdb0f40b899bfe4b997b8300f",
            "value": " 2/2 [43:59&lt;00:00, 1326.72s/it]"
          }
        },
        "b959c58ab43f48debe5c899313c47bc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21a3155086c3454fb01497cf8d1cca51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97fc1475c69a49f28d2ac269b2b9fcc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c616a40287d4df19ac0ad0bfdfd2416": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d5b1d418ec345fda161038c61f24ea2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c212a83a4a34c38993046642b1d523a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95714e5fdb0f40b899bfe4b997b8300f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe4e8342b7bc4b22858ad25381f69d88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_122994e4fe90493e9b3520e60d250acf",
              "IPY_MODEL_6aacab780d0c48809ccccc0ef13ed2a0",
              "IPY_MODEL_7acad35807c7492d82edca70c387c1e1"
            ],
            "layout": "IPY_MODEL_f1a511ce016845fd88b8e229b0f697e2"
          }
        },
        "122994e4fe90493e9b3520e60d250acf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31b5096936084c64b644b5e1ca584cbd",
            "placeholder": "​",
            "style": "IPY_MODEL_a63f6b60b27342ca88dfab89868b8e09",
            "value": "Epochs 0/2. Running Loss:    1.0961: 100%"
          }
        },
        "6aacab780d0c48809ccccc0ef13ed2a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63f073ebad8e4003b3de52371a8c4201",
            "max": 17470,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_869f35e2aad6414893f86dba710c23eb",
            "value": 17470
          }
        },
        "7acad35807c7492d82edca70c387c1e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4d4f3eda1ac4e579d10de772ad982f9",
            "placeholder": "​",
            "style": "IPY_MODEL_20b8ce32c84f478bae5753b923fe092d",
            "value": " 17470/17470 [21:14&lt;00:00, 15.83it/s]"
          }
        },
        "f1a511ce016845fd88b8e229b0f697e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31b5096936084c64b644b5e1ca584cbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a63f6b60b27342ca88dfab89868b8e09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63f073ebad8e4003b3de52371a8c4201": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "869f35e2aad6414893f86dba710c23eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4d4f3eda1ac4e579d10de772ad982f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20b8ce32c84f478bae5753b923fe092d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "816a05098cc5427f8e011ae0b40a018a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f07bb5e2fbc44eeeb7671f421fa5bdab",
              "IPY_MODEL_9f7821f3fff046d9bdb19e0672f57f85",
              "IPY_MODEL_f749c126f5f84fe8b316448f5d3dd29e"
            ],
            "layout": "IPY_MODEL_b7a2fcab45e34aa3a4c99c879873ed67"
          }
        },
        "f07bb5e2fbc44eeeb7671f421fa5bdab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0b3e64d278246a8802eadb3295e9aa5",
            "placeholder": "​",
            "style": "IPY_MODEL_3d6f3f1d8e7a4e5d81e1b5cfbaa07967",
            "value": "Epochs 1/2. Running Loss:    2.2607: 100%"
          }
        },
        "9f7821f3fff046d9bdb19e0672f57f85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbd8c432e5f24f28b0063fc3f7e27609",
            "max": 17470,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d007641e9a94d97ab6a21c39ce5c3f5",
            "value": 17470
          }
        },
        "f749c126f5f84fe8b316448f5d3dd29e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfc9667963a74f06b39f12f81d582295",
            "placeholder": "​",
            "style": "IPY_MODEL_0e6795e29d73493caa0fcb8da399e85e",
            "value": " 17470/17470 [22:34&lt;00:00, 14.93it/s]"
          }
        },
        "b7a2fcab45e34aa3a4c99c879873ed67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0b3e64d278246a8802eadb3295e9aa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d6f3f1d8e7a4e5d81e1b5cfbaa07967": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbd8c432e5f24f28b0063fc3f7e27609": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d007641e9a94d97ab6a21c39ce5c3f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cfc9667963a74f06b39f12f81d582295": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e6795e29d73493caa0fcb8da399e85e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6af222981aad4fcc922c4ed19a143af4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_797efa893ae941d9a5c25dcd14005a05",
              "IPY_MODEL_4c943fa930cc4bfc942cbe49be696e9e",
              "IPY_MODEL_7e2d0d2d4d944790858407b9e416b361"
            ],
            "layout": "IPY_MODEL_7458e9c47545461e93c9f885561ea521"
          }
        },
        "797efa893ae941d9a5c25dcd14005a05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_526776e3471d472aba940f24e76a658e",
            "placeholder": "​",
            "style": "IPY_MODEL_b887db09095d42b0b5c78d9a0c11973f",
            "value": "Running Evaluation: 100%"
          }
        },
        "4c943fa930cc4bfc942cbe49be696e9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_745b437fc6cd481f9a917c87ff666f4d",
            "max": 864,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ad75817597f46d09945391b10570d52",
            "value": 864
          }
        },
        "7e2d0d2d4d944790858407b9e416b361": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af742c8180bb4ecda5a607a5b9909ba1",
            "placeholder": "​",
            "style": "IPY_MODEL_5f12786dd82e46679fc57f47d8583b5e",
            "value": " 864/864 [00:57&lt;00:00, 15.11it/s]"
          }
        },
        "7458e9c47545461e93c9f885561ea521": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "526776e3471d472aba940f24e76a658e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b887db09095d42b0b5c78d9a0c11973f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "745b437fc6cd481f9a917c87ff666f4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ad75817597f46d09945391b10570d52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af742c8180bb4ecda5a607a5b9909ba1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f12786dd82e46679fc57f47d8583b5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bced45727b04b6f8cdf59c62c102f18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_58abe491fa774df4ab88b35eb8221f9d",
              "IPY_MODEL_66af14aa77024510b8951a5031a71528",
              "IPY_MODEL_101afed5cbce4403a31b07a4083296f3"
            ],
            "layout": "IPY_MODEL_88c6581dab4a495088f961bebdc43b6a"
          }
        },
        "58abe491fa774df4ab88b35eb8221f9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35bc34aa611d464db5f37a0943cc7757",
            "placeholder": "​",
            "style": "IPY_MODEL_70c156b41478458ea6f18cdda0617520",
            "value": "Running Prediction: 100%"
          }
        },
        "66af14aa77024510b8951a5031a71528": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d9b0cd16c0d494fa5f2cdc2874d9ae8",
            "max": 1540,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a71ad8c5c63f47a687a31a77aa0dd294",
            "value": 1540
          }
        },
        "101afed5cbce4403a31b07a4083296f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b1cb077478245fb8ea2d520002fd861",
            "placeholder": "​",
            "style": "IPY_MODEL_4043b93889d44b408fc24d412e94b6e9",
            "value": " 1540/1540 [01:41&lt;00:00, 15.30it/s]"
          }
        },
        "88c6581dab4a495088f961bebdc43b6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35bc34aa611d464db5f37a0943cc7757": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70c156b41478458ea6f18cdda0617520": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d9b0cd16c0d494fa5f2cdc2874d9ae8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a71ad8c5c63f47a687a31a77aa0dd294": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b1cb077478245fb8ea2d520002fd861": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4043b93889d44b408fc24d412e94b6e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e69132cd2c144b3abaf3375e97bd718e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_beb81b9cabc045edb90607884d606dff",
              "IPY_MODEL_1283c6afa9a643549a608f5403b64cd6",
              "IPY_MODEL_65760d34e15248df8776b5e68a139730"
            ],
            "layout": "IPY_MODEL_bc5ca9d6082a47c9ad7e2cd1ef5cb081"
          }
        },
        "beb81b9cabc045edb90607884d606dff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bec2c38f61074697933fb0ca5d60cbe4",
            "placeholder": "​",
            "style": "IPY_MODEL_cd3ed8f3539547efb0679f6cf34eedcc",
            "value": "Running Prediction: 100%"
          }
        },
        "1283c6afa9a643549a608f5403b64cd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d53c11f608e8464fb4084c2b056039bc",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b3c7a9692b94f8d92b00a056c046086",
            "value": 1
          }
        },
        "65760d34e15248df8776b5e68a139730": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9721dc4fcb344111ade964823eea817b",
            "placeholder": "​",
            "style": "IPY_MODEL_202fd5aa3bfe4dd1a49794ac614d3d08",
            "value": " 1/1 [00:00&lt;00:00, 22.52it/s]"
          }
        },
        "bc5ca9d6082a47c9ad7e2cd1ef5cb081": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bec2c38f61074697933fb0ca5d60cbe4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd3ed8f3539547efb0679f6cf34eedcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d53c11f608e8464fb4084c2b056039bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b3c7a9692b94f8d92b00a056c046086": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9721dc4fcb344111ade964823eea817b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "202fd5aa3bfe4dd1a49794ac614d3d08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e6af3d7709f4cf7abd6ce3c3ba34e87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c37422984b5b4755b870f6d28cb6252f",
              "IPY_MODEL_e7f839bc6f5a48d981a19d1137ca7003",
              "IPY_MODEL_9317179a13974bb49f9f336b18f763b8"
            ],
            "layout": "IPY_MODEL_bd361453dc4845b0846182ab714c433a"
          }
        },
        "c37422984b5b4755b870f6d28cb6252f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c81f85dec884da092cc769e7d150e51",
            "placeholder": "​",
            "style": "IPY_MODEL_77c887ba837d488a8a296d1b06ae3d0e",
            "value": "Running Prediction: 100%"
          }
        },
        "e7f839bc6f5a48d981a19d1137ca7003": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8eee2597e0c343a0b62ada89627b3a38",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16b57c42fc8843409289ccb84f4d24b5",
            "value": 1
          }
        },
        "9317179a13974bb49f9f336b18f763b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb4e8004a24c45ccb6f5d1854145c4dc",
            "placeholder": "​",
            "style": "IPY_MODEL_640487b39db640268968ba41c19a837b",
            "value": " 1/1 [00:00&lt;00:00, 17.87it/s]"
          }
        },
        "bd361453dc4845b0846182ab714c433a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c81f85dec884da092cc769e7d150e51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77c887ba837d488a8a296d1b06ae3d0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8eee2597e0c343a0b62ada89627b3a38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16b57c42fc8843409289ccb84f4d24b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eb4e8004a24c45ccb6f5d1854145c4dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "640487b39db640268968ba41c19a837b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd2e3ce1427f4ba2850382681ee0b9c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7e8ad34bfd75431aa50c151de2959744",
              "IPY_MODEL_1eaabfb73ba448b1886d3ef1288daf42",
              "IPY_MODEL_dcd82f7fa2cb41088c43b76acf0d36df"
            ],
            "layout": "IPY_MODEL_d138030b492b4f6d8484c0d21855966e"
          }
        },
        "7e8ad34bfd75431aa50c151de2959744": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72a2a65311ec4a898ca5ea4904121a4a",
            "placeholder": "​",
            "style": "IPY_MODEL_56de35c0653348c8b57f714a4130b4f0",
            "value": "Running Prediction: 100%"
          }
        },
        "1eaabfb73ba448b1886d3ef1288daf42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9766acea27a4e948f3f1e6b7da6b779",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d45bbdbd8cad46ac8df9d8c41a3f5b6c",
            "value": 1
          }
        },
        "dcd82f7fa2cb41088c43b76acf0d36df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_789bdd40b10e45198cb1dafcc0ac680d",
            "placeholder": "​",
            "style": "IPY_MODEL_a0e9f456631c437c80e1fb283c1bdf64",
            "value": " 1/1 [00:00&lt;00:00, 10.25it/s]"
          }
        },
        "d138030b492b4f6d8484c0d21855966e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72a2a65311ec4a898ca5ea4904121a4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56de35c0653348c8b57f714a4130b4f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9766acea27a4e948f3f1e6b7da6b779": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d45bbdbd8cad46ac8df9d8c41a3f5b6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "789bdd40b10e45198cb1dafcc0ac680d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0e9f456631c437c80e1fb283c1bdf64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a140ae435984892a990ab5689cc3bed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0fe947eed354451b50a82774d8200ba",
              "IPY_MODEL_968a561ecb1f468aa58133855f5aaf0c",
              "IPY_MODEL_56055eb72e714ec988ac46fcf4a7cdea"
            ],
            "layout": "IPY_MODEL_1766ac60f97b4643af7062ad334f49df"
          }
        },
        "e0fe947eed354451b50a82774d8200ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0277559c46f4249a0356952e9160736",
            "placeholder": "​",
            "style": "IPY_MODEL_2cd141fdd3114531a3f83aef8216d3bb",
            "value": "Running Prediction: 100%"
          }
        },
        "968a561ecb1f468aa58133855f5aaf0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c09acb9ee8d45cebf8c644963616cae",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8afc03c986fe4ea2a3b2076c6e8fd43e",
            "value": 1
          }
        },
        "56055eb72e714ec988ac46fcf4a7cdea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_025193eef48741e49cc6dece60b9d1a1",
            "placeholder": "​",
            "style": "IPY_MODEL_8d29f3978f634a899d1f9463b15dff3e",
            "value": " 1/1 [00:00&lt;00:00, 10.84it/s]"
          }
        },
        "1766ac60f97b4643af7062ad334f49df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0277559c46f4249a0356952e9160736": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cd141fdd3114531a3f83aef8216d3bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c09acb9ee8d45cebf8c644963616cae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8afc03c986fe4ea2a3b2076c6e8fd43e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "025193eef48741e49cc6dece60b9d1a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d29f3978f634a899d1f9463b15dff3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}